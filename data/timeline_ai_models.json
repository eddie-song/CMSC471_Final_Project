{
    "title": {
        "text": {
            "headline": "AI Model Timeline",
            "text": "<p>A timeline of notable AI models and their release dates.</p>"
        }
    },
    "events": [
        {
            "start_date": {
                "year": 2025,
                "month": 4,
                "day": 10
            },
            "text": {
                "headline": "Pangu Ultra",
                "text": "<p>We present Pangu Ultra, a Large Language Model (LLM) with 135 billion parameters and dense Transformer modules trained on Ascend Neural Processing Units (NPUs). Although the field of LLM has been witnessing unprecedented advances in pushing the scale and capability of LLM in recent years, training such a large-scale model still involves significant optimization and system challenges. To stabilize the training process, we propose depth-scaled sandwich normalization, which effectively eliminates loss spikes during the training process of deep models. We pre-train our model on 13.2 trillion diverse and high-quality tokens and further enhance its reasoning capabilities during post-training. To perform such large-scale training efficiently, we utilize 8,192 Ascend NPUs with a series of system optimizations. Evaluations on multiple diverse benchmarks indicate that Pangu Ultra significantly advances the state-of-the-art capabilities of dense LLMs such as Llama 405B and Mistral Large 2, and even achieves competitive results with DeepSeek-R1, whose sparse model structure contains much more parameters. Our exploration demonstrates that Ascend NPUs are capable of efficiently and effectively training dense models with more than 100 billion parameters. Our model and system will be available for our commercial customers.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2504.07866",
                "caption": "Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend NPUs\n",
                "credit": "Yichun Yin, Wenyong Huang, Kaikai Song, Yehui Tang, Xueyu Wu, Wei Guo, Peng Guo, Yaoyuan Wang, Xiaojun Meng, Yasheng Wang, Dong Li, Can Chen, Dandan Tu, Yin Li, Fisher Yu, Ruiming Tang, Yunhe Wang, Baojun Wang, Bin Wang, Bo Wang, Boxiao Liu, Changzheng Zhang, Duyu Tang, Fei Mi, Hui Jin, Jiansheng Wei, Jiarui Qin, Jinpeng Li, Jun Zhao, Liqun Deng, Lin Li, Minghui Xu, Naifu Zhang, Nianzu Zheng, Qiang Li, Rongju Ruan, Shengjun Cheng, Tianyu Guo, Wei He, Wei Li, Weiwen Liu, Wulong Liu, Xinyi Dai, Yonghan Dong, Yu Pan, Yue Li, Yufei Wang, Yujun Li, Yunsheng Ni, Zhe Liu, Zhenhe Zhang, Zhicheng Liu"
            }
        },
        {
            "start_date": {
                "year": 2025,
                "month": 4,
                "day": 5
            },
            "text": {
                "headline": "Llama 4 Behemoth",
                "text": "<p>We’re sharing the first models in the Llama 4 herd, which will enable people to build more personalized multimodal experiences.\n...\nLlama 4 Behemoth, a 288 billion active parameter model with 16 experts that is our most powerful yet and among the world’s smartest LLMs. Llama 4 Behemoth outperforms GPT-4.5, Claude Sonnet 3.7, and Gemini 2.0 Pro on several STEM benchmarks. Llama 4 Behemoth is still training, and we’re excited to share more details about it even while it’s still in flight.\nDownload the Llama 4 Scout and Llama 4 Maverick models today on llama.com and Hugging Face. Try Meta AI built with Llama 4 in WhatsApp, Messenger, Instagram Direct, and on the web.</p>"
            },
            "media": {
                "url": "https://ai.meta.com/blog/llama-4-multimodal-intelligence/",
                "caption": "The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation",
                "credit": ""
            }
        },
        {
            "start_date": {
                "year": 2025,
                "month": 3,
                "day": 16
            },
            "text": {
                "headline": "EXAONE Deep 32B",
                "text": "<p>We present EXAONE Deep series, which exhibits superior capabilities in various reasoning tasks, including math and coding benchmarks. We train our models mainly on the reasoning-specialized dataset that incorporates long streams of thought processes. Evaluation results show that our smaller models, EXAONE Deep 2.4B and 7.8B, outperform other models of comparable size, while the largest model, EXAONE Deep 32B, demonstrates competitive performance against leading open-weight models. All EXAONE Deep models are openly available for research purposes and can be downloaded from this https URL</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2503.12524",
                "caption": "EXAONE Deep: LLMs with Enhanced Reasoning Performance",
                "credit": "LG AI Research, Kyunghoon Bae, Eunbi Choi, Kibong Choi, Stanley Jungkyu Choi, Yemuk Choi, Seokhee Hong, Junwon Hwang, Hyojin Jeon, Kijeong Jeon, Gerrard Jeongwon Jo, Hyunjik Jo, Jiyeon Jung, Hyosang Kim, Joonkee Kim, Seonghwan Kim, Soyeon Kim, Sunkyoung Kim, Yireun Kim, Yongil Kim, Youchul Kim, Edward Hwayoung Lee, Haeju Lee, Honglak Lee, Jinsik Lee, Kyungmin Lee, Sangha Park, Yongmin Park, Sihoon Yang, Heuiyeen Yeen, Sihyuk Yi, Hyeongu Yun"
            }
        },
        {
            "start_date": {
                "year": 2025,
                "month": 3,
                "day": 16
            },
            "text": {
                "headline": "ERNIE 4.5 (文心大模型4.5)",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://www.datacamp.com/blog/ernie-4-5-x1",
                "caption": "Baidu's ERNIE 4.5 & X1: Features, Access, DeepSeek Comparison\n",
                "credit": ""
            }
        },
        {
            "start_date": {
                "year": 2025,
                "month": 3,
                "day": 11
            },
            "text": {
                "headline": "Hunyuan-TurboS",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://web.archive.org/web/20250408105622/https://www.dapingtime.com/article/2171.html\n\nhttps://medium.com/data-science-in-your-pocket/tencent-hunyuan-turbo-s-the-fastest-reasoning-llm-d64a02bed5c8",
                "caption": "",
                "credit": ""
            }
        },
        {
            "start_date": {
                "year": 2025,
                "month": 3,
                "day": 6
            },
            "text": {
                "headline": "QwQ-32B",
                "text": "<p>QwQ is the reasoning model of the Qwen series. Compared with conventional instruction-tuned models, QwQ, which is capable of thinking and reasoning, can achieve significantly enhanced performance in downstream tasks, especially hard problems. QwQ-32B is the medium-sized reasoning model, which is capable of achieving competitive performance against state-of-the-art reasoning models, e.g., DeepSeek-R1, o1-mini.</p>"
            },
            "media": {
                "url": "https://qwenlm.github.io/blog/qwq-32b/",
                "caption": "QwQ-32B: Embracing the Power of Reinforcement Learning",
                "credit": "Qwen Team"
            }
        },
        {
            "start_date": {
                "year": 2025,
                "month": 3,
                "day": 6
            },
            "text": {
                "headline": "Mistral OCR",
                "text": "<p>Mistral OCR is an Optical Character Recognition API that sets a new standard in document understanding. Unlike other models, Mistral OCR comprehends each element of documents—media, text, tables, equations—with unprecedented accuracy and cognition. It takes images and PDFs as input and extracts content in an ordered interleaved text and images.</p>"
            },
            "media": {
                "url": "https://mistral.ai/news/mistral-ocr",
                "caption": "Mistral OCR\n",
                "credit": ""
            }
        },
        {
            "start_date": {
                "year": 2025,
                "month": 2,
                "day": 27
            },
            "text": {
                "headline": "Mercury",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://machine-learning-made-simple.medium.com/is-the-mercury-llm-the-first-of-a-new-generation-of-llms-b64de1d36029\nhttps://www.inceptionlabs.ai/news\n",
                "caption": "Is the Mercury LLM the first of a new Generation of LLMs?",
                "credit": ""
            }
        },
        {
            "start_date": {
                "year": 2025,
                "month": 2,
                "day": 27
            },
            "text": {
                "headline": "GPT-4.5",
                "text": "<p>We advance AI capabilities by scaling two complementary paradigms: unsupervised learning and reasoning. These represent two axes of intelligence.\n\nUnsupervised learning increases world model accuracy and intuition. Models like GPT‑3.5, GPT‑4, and GPT‑4.5 advance this paradigm.\nScaling reasoning⁠, on the other hand, teaches models to think and produce a chain of thought before they respond, allowing them to tackle complex STEM or logic problems. Models like OpenAI o1 and OpenAI o3‑mini advance this paradigm.\nGPT‑4.5 is an example of scaling unsupervised learning by scaling up compute and data, along with architecture and optimization innovations. GPT‑4.5 was trained on Microsoft Azure AI supercomputers. The result is a model that has broader knowledge and a deeper understanding of the world, leading to reduced hallucinations and more reliability across a wide range of topics.</p>"
            },
            "media": {
                "url": "https://openai.com/index/introducing-gpt-4-5/",
                "caption": "Introducing GPT-4.5",
                "credit": "Foundational contributors\nAlex Paino, Ali Kamali, Amin Tootoonchian, Andrew Tulloch, Ben Sokolowsky, Clemens Winter, Colin Wei, Daniel Kappler, Daniel Levy, Felipe Petroski Such, Geoff Salmon, Ian O’Connell, Jason Teplitz, Kai Chen, Nik Tezak, Prafulla Dhariwal, Rapha Gontijo Lopes, Sam Schoenholz, Youlong Cheng, Yujia Jin, Yunxing Dai\n\nResearch\nCore contributors\n\nAiden Low, Alec Radford, Alex Carney, Alex Nichol, Alexis Conneau, Ananya Kumar, Ben Wang, Charlotte Cole , Elizabeth Yang, Gabriel Goh, Hadi Salman, Haitang Hu, Heewoo Jun, Ian Sohl, Ishaan Gulrajani, Jacob Coxon, James Betker, Jamie Kiros, Jessica Landon, Kyle Luther, Lia Guy, Lukas Kondraciuk, Lyric Doshi, Mikhail Pavlov, Qiming Yuan, Reimar Leike, Rowan Zellers, Sean Metzger, Shengjia Zhao, Spencer Papay, Tao Wang\n\nContributors\n\nAdam Lerer, Aidan McLaughlin, Alexander Prokofiev, Alexandra Barr, Allan Jabri, Ananya Kumar, Andrew Gibiansky, Andrew Schmidt, Casey Chu, Chak Li, Chelsea Voss, Chris Hallacy, Chris Koch, Christine McLeavey, David Mely, Dimitris Tsipras, Eric Sigler, Erin Kavanaugh, Farzad Khorasani, Huiwen Chang, Ilya Kostrikov, Ishaan Singal, Ji Lin, Jiahui Yu, Jing Yu Zhang, John Rizzo, Jong Wook Kim, Joyce Lee, Juntang Zhuang, Leo Liu, Li Jing, Long Ouyang, Louis Feuvrier, Mo Bavarian, Nick Stathas, Nitish Keskar, Oleg Murk, Preston Bowman, Scottie Yan, SQ Mah, Tao Xu, Taylor Gordon, Valerie Qi, Wenda Zhou, Yu Zhang\n\nScaling\nCore contributors\n\nAdam Goucher, Alex Chow, Alex Renzin, Aleksandra Spyra, Avi Nayak, Ben Leimberger, Christopher Hesse, Duc Phong Nguyen, Dinghua Li, Eric Peterson, Francis Zhang, Gene Oden, Kai Fricke, Kai Hayashi, Larry Lv, Leqi Zou, Lin Yang, Madeleine Thompson, Michael Petrov, Miguel Castro, Natalia Gimelshein, Phil Tillet, Reza Zamani, Ryan Cheu Stanley Hsieh, Steve Lee, Stewart Hall, Thomas Raoux, Tianhao Zheng, Vishal Kuo, Yongjik Kim, Yuchen Zhang, Zhuoran Liu\n\nContributors\n\nAlvin Wan, Andrew Cann, Antoine Pelisse, Anuj Kalia, Aaron Hurst, Avital Oliver, Brad Barnes, Brian Hsu, Chen Ding, Chen Shen, Cheng Chang, Christian Gibson, Duncan Findlay, Fan Wang, Fangyuan Li, Gianluca Borello, Heather Schmidt, Henrique Ponde de Oliveira Pinto, Ikai Lan, Jiayi Weng, James Crooks, Jos Kraaijeveld, Junru Shao, Kenny Hsu, Kenny Nguyen, Kevin King, Leah Burkhardt, Leo Chen, Linden Li, Lu Zhang, Mahmoud Eariby, Marat Dukhan, Mateusz Litwin, Miki Habryn, Natan LaFontaine, Pavel Belov, Peng Su, Prasad Chakka, Rachel Lim, Rajkumar Samuel, Renaud Gaubert, Rory Carmichael, Sarah Dong, Shantanu Jain, Stephen Logsdon, Todd Underwood, Weixing Zhang, Will Sheu, Weiyi Zheng, Yinghai Lu, Yunqiao Zhang\n\nSafety Systems\nAndrea Vallone, Andy Applebaum, Cameron Raymond, Chong Zhang, Dan Mossing, Elizabeth Proehl, Eric Wallace, Evan Mays, Grace Zhao, Ian Kivlichan, Irina Kofman, Joel Parish, Kevin Liu, Keren Gu-Lemberg, Kristen Ying, Lama Ahmad, Lilian Weng , Leon Maksin, Leyton Ho, Meghan Shah, Michael Lampe, Michele Wang, Miles Wang, Olivia Watkins, Phillip Guo, Samuel Miserendino, Sam Toizer, Sandhini Agarwal, Tejal Patwardhan, Tom Dupré la Tour, Tong Mu, Tyna Eloundou, Yunyun Wang\n\nDeployment\nAdam Brandon, Adam Perelman, Adele Li, Akshay Nathan, Alan Hayes, Alfred Xue, Alison Ben, Alec Gorge, Alex Guziel, Alex Iftimie, Ally Bennett, Andrew Chen, Andy Wang, Andy Wood, Angad Singh, Anoop Kotha, Antonia Woodford, Anuj Saharan, Ashley Tyra, Atty Eleti, Ben Schneider, Bessie Ji, Beth Hoover, Bill Chen, Blake Samic, Britney Smith, Brian Yu, Caleb Wang, Cary Bassin, Cary Hudson, Charlie Jatt, Chengdu Huang, Chris Beaumont, Christina Huang, Cristina Scheau, Dana Palmie, Daniel Levine, Daryl Neubieser, Dave Cummings, David Sasaki, Dibya Bhattacharjee, Dylan Hunn, Edwin Arbus, Elaine Ya Le, Enis Sert, Eric Kramer, Fred von Lohmann, Gaby Janatpour, Garrett McGrath, Garrett Ollinger, Gary Yang, Hao Sheng, Harold Hotelling, Janardhanan Vembunarayanan, Jeff Harris, Jeffrey Sabin Matsumoto, Jennifer Robinson, Jessica Liang, Jessica Shieh, Jiacheng Yang, Joel Morris, Joseph Florencio, Josh Kaplan, Kan Wu, Karan Sharma, Karen Li, Katie Pypes, Kendal Simon, Kendra Rimbach, Kevin Park, Kevin Rao, Laurance Fauconnet, Lauren Workman, Leher Pathak, Liang Wu, Liang Xiong, Lien Mamitsuka, Lindsay McCallum, Lukas Gross, Manoli Liodakis, Matt Nichols, Michelle Fradin, Minal Khan, Mingxuan Wang, Nacho Soto, Natalie Staudacher, Nikunj Handa, Niko Felix, Ning Liu, Olivier Godement, Oona Gleeson, Philip Pronin, Raymond Li, Reah Miyara, Rohan Nuttall, R.J. Marsan, Sara Culver, Scott Ethersmith, Sean Fitzgerald, Shamez Hemani, Sherwin Wu, Shiao Lee, Shuyang Cheng, Siyuan Fu, Spug Golden, Steve Coffey, Steven Heidel, Sundeep Tirumalareddy, Tabarak Khan, Thomas Degry, Thomas Dimson, Tom Stasi, Tomo Hiratsuka, Trevor Creech, Uzair Navid Iftikhar, Victoria Chernova, Victoria Spiegel, Wanning Jiang, Wenlei Xie, Yaming Lin, Yara Khakbaz, Yilei Qian, Yilong Qin, Yo Shavit, Zhi Bie\n\nExecutive Leadership\nBob McGrew, Greg Brockman, Hannah Wong, Jakub Pachocki, Johannes Heidecke, Joanne Jang, Kate Rouch, Kevin Weil, Lauren Itow, Liam Fedus, Mark Chen, Mia Glaese, Mira Murati, Nick Ryder, Sam Altman, Srinivas Narayanan, Tal Broda"
            }
        },
        {
            "start_date": {
                "year": 2025,
                "month": 2,
                "day": 25
            },
            "text": {
                "headline": "Wan 2.1",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://huggingface.co/Wan-AI\nhttps://huggingface.co/blog/LLMhacker/wanai-wan21\n",
                "caption": "Wan 2.1 by Wan AI :best cost efficient video generation model Now Available",
                "credit": ""
            }
        },
        {
            "start_date": {
                "year": 2025,
                "month": 2,
                "day": 24
            },
            "text": {
                "headline": "Claude 3.7 Sonnet",
                "text": "<p>Today, we’re announcing Claude 3.7 Sonnet1, our most intelligent model to date and the first hybrid reasoning model on the market. Claude 3.7 Sonnet can produce near-instant responses or extended, step-by-step thinking that is made visible to the user. API users also have fine-grained control over how long the model can think for.\n\nClaude 3.7 Sonnet shows particularly strong improvements in coding and front-end web development. </p>"
            },
            "media": {
                "url": "https://www.anthropic.com/news/claude-3-7-sonnet",
                "caption": "Claude 3.7 Sonnet",
                "credit": ""
            }
        },
        {
            "start_date": {
                "year": 2025,
                "month": 2,
                "day": 17
            },
            "text": {
                "headline": "Grok-3",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://x.ai/blog/grok-3",
                "caption": "Grok 3 Beta — The Age of Reasoning Agents",
                "credit": ""
            }
        },
        {
            "start_date": {
                "year": 2025,
                "month": 2,
                "day": 3
            },
            "text": {
                "headline": "Eurus-2-7B-PRIME",
                "text": "<p>Dense process rewards have proven a more effective alternative to the sparse outcome-level rewards in the inference-time scaling of large language models (LLMs), particularly in tasks requiring complex multi-step reasoning. While dense rewards also offer an appealing choice for the reinforcement learning (RL) of LLMs since their fine-grained rewards have the potential to address some inherent issues of outcome rewards, such as training efficiency and credit assignment, this potential remains largely unrealized. This can be primarily attributed to the challenges of training process reward models (PRMs) online, where collecting high-quality process labels is prohibitively expensive, making them particularly vulnerable to reward hacking. To address these challenges, we propose PRIME (Process Reinforcement through IMplicit rEwards), which enables online PRM updates using only policy rollouts and outcome labels through implict process rewards. PRIME combines well with various advantage functions and forgoes the dedicated reward model training phase that existing approaches require, substantially reducing the development overhead. We demonstrate PRIME’s effectiveness on competitional math and coding. Starting from Qwen2.5-Math-7B-Base, PRIME achieves a 15.1% average improvement across several key reasoning benchmarks over the SFT model. Notably, our resulting model, Eurus-2-7B-PRIME, surpasses Qwen2.5-Math-7B-Instruct on seven reasoning benchmarks with 10% of its training data.1</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2502.01456",
                "caption": "Process Reinforcement through Implicit Rewards",
                "credit": "Ganqu Cui, Lifan Yuan, Zefan Wang, Hanbin Wang, Wendi Li, Bingxiang He, Yuchen Fan, Tianyu Yu, Qixin Xu, Weize Chen, Jiarui Yuan, Huayu Chen, Kaiyan Zhang, Xingtai Lv, Shuo Wang, Yuan Yao, Xu Han, Hao Peng, Yu Cheng, Zhiyuan Liu, Maosong Sun, Bowen Zhou, Ning Ding"
            }
        },
        {
            "start_date": {
                "year": 2025,
                "month": 1,
                "day": 31
            },
            "text": {
                "headline": "o3-mini",
                "text": "<p>We’re releasing OpenAI o3-mini, the newest, most cost-efficient model in our reasoning series, available in both ChatGPT and the API today. Previewed in December 2024⁠, this powerful and fast model advances the boundaries of what small models can achieve, delivering exceptional STEM capabilities—with particular strength in science, math, and coding—all while maintaining the low cost and reduced latency of OpenAI o1-mini.\n\nOpenAI o3-mini is our first small reasoning model that supports highly requested developer features including function calling⁠(opens in a new window), Structured Outputs⁠(opens in a new window), and developer messages⁠(opens in a new window), making it production-ready out of the gate. Like OpenAI o1-mini and OpenAI o1-preview, o3-mini will support streaming⁠(opens in a new window). Also, developers can choose between three reasoning effort⁠(opens in a new window) options—low, medium, and high—to optimize for their specific use cases. This flexibility allows o3-mini to “think harder” when tackling complex challenges or prioritize speed when latency is a concern. o3-mini does not support vision capabilities, so developers should continue using OpenAI o1 for visual reasoning tasks. o3-mini is rolling out in the Chat Completions API, Assistants API, and Batch API starting today to select developers in API usage tiers 3-5⁠</p>"
            },
            "media": {
                "url": "https://openai.com/index/openai-o3-mini/",
                "caption": "Pushing the frontier of cost-effective reasoning.",
                "credit": "Training\nBrian Zhang, Eric Mitchell, Hongyu Ren, Kevin Lu, Max Schwarzer, Michelle Pokrass, Shengjia Zhao, Ted Sanders\n\nEval\nAdam Kalai, Alex Tachard Passos, Ben Sokolowsky, Elaine Ya Le, Erik Ritter, Hao Sheng, Hanson Wang, Ilya Kostrikov, James Lee, Johannes Ferstad, Michael Lampe, Prashanth Radhakrishnan, Sean Fitzgerald, Sebastien Bubeck, Yann Dubois, Yu Bai\n\nFrontier Evals & Preparedness\nAndy Applebaum, Elizabeth Proehl, Evan Mays, Joel Parish, Kevin Liu, Leon Maksin, Leyton Ho, Miles Wang, Michele Wang, Olivia Watkins, Patrick Chao, Samuel Miserendino, Tejal Patwardhan\n\nEngineering\nAdam Walker, Akshay Nathan, Alyssa Huang, Andy Wang, Ankit Gohel, Ben Eggers, Brian Yu, Bryan Ashley, Chengdu Huang, Christian Hoareau, Davin Bogan, Emily Sokolova, Eric Horacek, Eric Jiang, Felipe Petroski Such, Jonah Cohen, Josh Gross, Justin Becker, Kan Wu, Kevin Whinnery, Larry Lv, Lee Byron, Manoli Liodakis, Max Johnson, Mike Trpcic, Murat Yesildal, Rasmus Rygaard, RJ Marsan, Rohit Ramchandani, Rohan Kshirsagar, Roman Huet, Sara Conlon, Shuaiqi (Tony) Xia, Siyuan Fu, Srinivas Narayanan, Sulman Choudhry, Tomer Kaftan, Trevor Creech\n\nSearch\nAdam Fry, Adam Perelman, Brandon Wang, Cristina Scheau, Philip Pronin, Sundeep Tirumalareddy, Will Ellsworth, Zewei Chu\n\nProduct\nAntonia Woodford, Beth Hoover, Jake Brill, Kelly Stirman, Minnia Feng, Neel Ajjarapu, Nick Turley, Nikunj Handa, Olivier Godement\n\nSafety\nAndrea Vallone, Andrew Duberstein, Enis Sert, Eric Wallace, Grace Zhao, Irina Kofman, Jieqi Yu, Joaquin Quinonero Candela, Madelaine Boyd, Mehmet Yatbaz, Mike McClay, Mingxuan Wang, Saachi Jain, Sandhini Agarwal, Sam Toizer, Santiago Hernández, Steve Mostovoy, Young Cha, Tao Li, Yunyun Wang\n\nExternal Redteaming\nLama Ahmad, Troy Peterson\n\n\nResearch Program Managers\nCarpus Chang, Kristen Ying\n\nLeadership\nAidan Clark, Dane Stuckey, Jerry Tworek, Jakub Pachocki, Johannes Heidecke, Kevin Weil, Liam Fedus, Mark Chen, Sam Altman, Wojciech Zaremba"
            }
        },
        {
            "start_date": {
                "year": 2025,
                "month": 1,
                "day": 23
            },
            "text": {
                "headline": "Computer-Using Agent (CUA)",
                "text": "<p>Today we introduced a research preview of Operator⁠(opens in a new window), an agent that can go to the web to perform tasks for you. Powering Operator is Computer-Using Agent (CUA), a model that combines GPT‑4o's vision capabilities with advanced reasoning through reinforcement learning. CUA is trained to interact with graphical user interfaces (GUIs)—the buttons, menus, and text fields people see on a screen—just as humans do. This gives it the flexibility to perform digital tasks without using OS-or web-specific APIs. \n\nCUA builds off of years of foundational research at the intersection of multimodal understanding and reasoning. By combining advanced GUI perception with structured problem-solving, it can break tasks into multi-step plans and adaptively self-correct when challenges arise. This capability marks the next step in AI development, allowing models to use the same tools humans rely on daily and opening the door to a vast range of new applications.\n\nWhile CUA is still early and has limitations, it sets new state-of-the-art benchmark results, achieving a 38.1% success rate on OSWorld for full computer use tasks, and 58.1% on WebArena and 87% on WebVoyager for web-based tasks. These results highlight CUA’s ability to navigate and operate across diverse environments using a single general action space. </p>"
            },
            "media": {
                "url": "https://openai.com/index/computer-using-agent/",
                "caption": "Powering Operator with Computer-Using Agent, a universal interface for AI to interact with the digital world.",
                "credit": ""
            }
        },
        {
            "start_date": {
                "year": 2025,
                "month": 1,
                "day": 22
            },
            "text": {
                "headline": "Kimi k1.5",
                "text": "<p>Language model pretraining with next token prediction has proved effective for scaling compute but is limited to the amount of available training data. Scaling reinforcement learning (RL) unlocks a new axis for the continued improvement of artificial intelligence, with the promise that large language models (LLMs) can scale their training data by learning to explore with rewards. However, prior published work has not produced competitive results. In light of this, we report on the training practice of Kimi k1.5, our latest multi-modal LLM trained with RL, including its RL training techniques, multi-modal data recipes, and infrastructure optimization. Long context scaling and improved policy optimization methods are key ingredients of our approach, which establishes a simplistic, effective RL framework without relying on more complex techniques such as Monte Carlo tree search, value functions, and process reward models. Notably, our system achieves state-of-the-art reasoning performance across multiple benchmarks and modalities—e.g., 77.5 on AIME, 96.2 on MATH 500, 94-th percentile on Codeforces, 74.9 on MathVista—matching OpenAI’s o1. Moreover, we present effective long2short methods that use long-CoT techniques to improve short-CoT models, yielding state-of-the-art short-CoT reasoning results—e.g., 60.8 on AIME, 94.6 on MATH500, 47.3 on LiveCodeBench—outperforming existing short-CoT models such as GPT-4o and Claude Sonnet 3.5 by a large margin (up to +550%).</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2501.12599v1",
                "caption": "Kimi k1.5: Scaling Reinforcement Learning with LLMs",
                "credit": "Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, Chuning Tang, Congcong Wang, Dehao Zhang, Enming Yuan, Enzhe Lu, Fengxiang Tang, Flood Sung, Guangda Wei, Guokun Lai, Haiqing Guo, Han Zhu, Hao Ding, Hao Hu, Hao Yang, Hao Zhang, Haotian Yao, Haotian Zhao, Haoyu Lu, Haoze Li, Haozhen Yu, Hongcheng Gao, Huabin Zheng, Huan Yuan, Jia Chen, Jianhang Guo, Jianlin Su, Jianzhou Wang, Jie Zhao, Jin Zhang, Jingyuan Liu, Junjie Yan, Junyan Wu, Lidong Shi, Ling Ye, Longhui Yu, Mengnan Dong, Neo Zhang, Ningchen Ma, Qiwei Pan, Qucheng Gong, Shaowei Liu, Shengling Ma, Shupeng Wei, Sihan Cao, Siying Huang, Tao Jiang, Weihao Gao, Weimin Xiong, Weiran He, Weixiao Huang, Wenhao Wu, Wenyang He, Xianghui Wei, Xianqing Jia, Xingzhe Wu, Xinran Xu, Xinxing Zu, Xinyu Zhou, Xuehai Pan, Y. Charles, Yang Li, Yangyang Hu, Yangyang Liu, Yanru Chen, Yejie Wang, Yibo Liu, Yidao Qin, Yifeng Liu, Ying Yang, Yiping Bao, Yulun Du, Yuxin Wu, Yuzhi Wang, Zaida Zhou, Zhaoji Wang, Zhaowei Li, Zhen Zhu, Zheng Zhang, Zhexu Wang, Zhilin Yang, Zhiqi Huang, Zihao Huang, Ziyao Xu, Zonghan Yang"
            }
        },
        {
            "start_date": {
                "year": 2025,
                "month": 1,
                "day": 22
            },
            "text": {
                "headline": "Doubao-1.5-pro",
                "text": "<p>The model uses the MoE architecture and explores the ultimate balance between model performance and reasoning performance through integrated training-thinking design. Doubao-1.5-pro can use only a smaller activation parameter to exceed the performance of a first-class super-large pre-training model and achieve excellent results on multiple evaluation benchmarks.</p>"
            },
            "media": {
                "url": "https://team.doubao.com/zh/special/doubao_1_5_pro",
                "caption": "Doubao-1.5-pro",
                "credit": ""
            }
        },
        {
            "start_date": {
                "year": 2025,
                "month": 1,
                "day": 20
            },
            "text": {
                "headline": "DeepSeek-R1",
                "text": "<p>We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities.\nThrough RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing reasoning behaviors. However, it encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To support the research community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama.</p>"
            },
            "media": {
                "url": "https://api-docs.deepseek.com/news/news250120",
                "caption": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning",
                "credit": ""
            }
        },
        {
            "start_date": {
                "year": 2025,
                "month": 1,
                "day": 17
            },
            "text": {
                "headline": "INTELLECT-MATH",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://huggingface.co/PrimeIntellect/INTELLECT-MATH\nhttps://www.primeintellect.ai/blog/intellect-math",
                "caption": "INTELLECT-MATH: Frontier Mathematical Reasoning through Better Initializations for Reinforcement Learning\n",
                "credit": ""
            }
        },
        {
            "start_date": {
                "year": 2024,
                "month": 12,
                "day": 24
            },
            "text": {
                "headline": "DeepSeek-V3",
                "text": "<p>We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. To achieve efficient inference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architectures, which were thoroughly validated in DeepSeek-V2. Furthermore, DeepSeek-V3 pioneers an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training objective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities. Comprehensive evaluations reveal that DeepSeek-V3 outperforms other open-source models and achieves performance comparable to leading closed-source models. Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its full training. In addition, its training process is remarkably stable. Throughout the entire training process, we did not experience any irrecoverable loss spikes or perform any rollbacks.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2412.19437",
                "caption": "DeepSeek-V3 Technical Report",
                "credit": "DeepSeek-AI, Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Daya Guo, Dejian Yang, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Haowei Zhang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Li, Hui Qu, J.L. Cai, Jian Liang, Jianzhong Guo, Jiaqi Ni, Jiashi Li, Jiawei Wang, Jin Chen, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, Junxiao Song, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Lei Xu, Leyi Xia, Liang Zhao, Litong Wang, Liyue Zhang, Meng Li, Miaojun Wang, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Mingming Li, Ning Tian, Panpan Huang, Peiyi Wang, Peng Zhang, Qiancheng Wang, Qihao Zhu, Qinyu Chen, Qiushi Du, R.J. Chen, R.L. Jin, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, Runxin Xu, Ruoyu Zhang, Ruyi Chen, S.S. Li, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shaoqing Wu, Shengfeng Ye, Shengfeng Ye, Shirong Ma, Shiyu Wang, Shuang Zhou, Shuiping Yu, Shunfeng Zhou, Shuting Pan, T. Wang, Tao Yun, Tian Pei, Tianyu Sun, W.L. Xiao, Wangding Zeng et al. (100 additional authors not shown)"
            }
        },
        {
            "start_date": {
                "year": 2024,
                "month": 12,
                "day": 20
            },
            "text": {
                "headline": "o3",
                "text": "<p>Today, we’re releasing OpenAI o3 and o4-mini, the latest in our o-series of models trained to think for longer before responding. These are the smartest models we’ve released to date, representing a step change in ChatGPT's capabilities for everyone from curious users to advanced researchers. For the first time, our reasoning models can agentically use and combine every tool within ChatGPT—this includes searching the web, analyzing uploaded files and other data with Python, reasoning deeply about visual inputs, and even generating images. Critically, these models are trained to reason about when and how to use tools to produce detailed and thoughtful answers in the right output formats, typically in under a minute, to solve more complex problems. This allows them to tackle multi-faceted questions more effectively, a step toward a more agentic ChatGPT that can independently execute tasks on your behalf. The combined power of state-of-the-art reasoning with full tool access translates into significantly stronger performance across academic benchmarks and real-world tasks, setting a new standard in both intelligence and usefulness.\n<..>\nOpenAI o3 is our most powerful reasoning model that pushes the frontier across coding, math, science, visual perception, and more. It sets a new SOTA on benchmarks including Codeforces, SWE-bench (without building a custom model-specific scaffold), and MMMU. It’s ideal for complex queries requiring multi-faceted analysis and whose answers may not be immediately obvious. It performs especially strongly at visual tasks like analyzing images, charts, and graphics. In evaluations by external experts, o3 makes 20 percent fewer major errors than OpenAI o1 on difficult, real-world tasks—especially excelling in areas like programming, business/consulting, and creative ideation. Early testers highlighted its analytical rigor as a thought partner and emphasized its ability to generate and critically evaluate novel hypotheses—particularly within biology, math, and engineering contexts.\n\n________\nmodel was announced 2024/12/20 \nfrom ARS technica: \"On Friday, during Day 12 of its \"12 days of OpenAI,\" OpenAI CEO Sam Altman announced its latest AI \"reasoning\" models, o3 and o3-mini, which build upon the o1 models launched earlier this year. The company is not releasing them yet but will make these models available for public safety testing and research access today.\"\n\nhttps://arstechnica.com/information-technology/2024/12/openai-announces-o3-and-o3-mini-its-next-simulated-reasoning-models/\n\nmodel was released 2025/04/16</p>"
            },
            "media": {
                "url": "https://openai.com/index/introducing-o3-and-o4-mini/",
                "caption": "Our most powerful reasoning model with leading performance on coding, math, science, and vision",
                "credit": ""
            }
        },
        {
            "start_date": {
                "year": 2024,
                "month": 12,
                "day": 16
            },
            "text": {
                "headline": "Veo 2",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://deepmind.google/technologies/veo/veo-2/",
                "caption": "Our state-of-the-art video generation model",
                "credit": ""
            }
        },
        {
            "start_date": {
                "year": 2024,
                "month": 12,
                "day": 11
            },
            "text": {
                "headline": "Gemini 2.0 Pro",
                "text": "<p>Today, we’re releasing an experimental version of Gemini 2.0 Pro that responds to that feedback. It has the strongest coding performance and ability to handle complex prompts, with better understanding and reasoning of world knowledge, than any model we’ve released so far. It comes with our largest context window at 2 million tokens, which enables it to comprehensively analyze and understand vast amounts of information, as well as the ability to call tools like Google Search and code execution.</p>"
            },
            "media": {
                "url": "https://deepmind.google/technologies/gemini/pro/",
                "caption": "Our best model yet for coding performance and complex prompts",
                "credit": "Gemini Team"
            }
        },
        {
            "start_date": {
                "year": 2024,
                "month": 12,
                "day": 9
            },
            "text": {
                "headline": "EXAONE 3.5 32B",
                "text": "<p>This technical report introduces the EXAONE 3.5 instruction-tuned language models, developed and released by LG AI Research. The EXAONE 3.5 language models are offered in three configurations: 32B, 7.8B, and 2.4B. These models feature several standout capabilities: 1) exceptional instruction following capabilities in real-world scenarios, achieving the highest scores across seven benchmarks, 2) outstanding long-context comprehension, attaining the top performance in four benchmarks, and 3) competitive results compared to state-of-the-art open models of similar sizes across nine general benchmarks. The EXAONE 3.5 language models are open to anyone for research purposes and can be downloaded from this https URL. For commercial use, please reach out to the official contact point of LG AI Research: contact_us@lgresearch.ai.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2412.04862",
                "caption": "EXAONE 3.5: Series of Large Language Models for Real-world Use Cases",
                "credit": "Soyoung An, Kyunghoon Bae, Eunbi Choi, Kibong Choi, Stanley Jungkyu Choi, Seokhee Hong, Junwon Hwang, Hyojin Jeon, Gerrard Jeongwon Jo, Hyunjik Jo, Jiyeon Jung, Yountae Jung, Hyosang Kim, Joonkee Kim, Seonghwan Kim, Soyeon Kim, Sunkyoung Kim, Yireun Kim, Yongil Kim, Youchul Kim, Edward Hwayoung Lee, Haeju Lee, Honglak Lee, Jinsik Lee, Kyungmin Lee, Woohyung Lim, Sangha Park, Sooyoun Park, Yongmin Park, Sihoon Yang, Heuiyeen Yeen, Hyeongu Yun"
            }
        },
        {
            "start_date": {
                "year": 2024,
                "month": 12,
                "day": 6
            },
            "text": {
                "headline": "Llama 3.3 70B",
                "text": "<p>The Meta Llama 3.3 multilingual large language model (LLM) is a pretrained and instruction tuned generative model in 70B (text in/text out). The Llama 3.3 instruction tuned text only model is optimized for multilingual dialogue use cases and outperforms many of the available open source and closed chat models on common industry benchmarks.\n\nModel developer: Meta\n\nModel Architecture: Llama 3.3 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety.</p>"
            },
            "media": {
                "url": "https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_3/",
                "caption": "Meta Llama 3.3 multilingual large language model (LLM) ",
                "credit": ""
            }
        },
        {
            "start_date": {
                "year": 2024,
                "month": 12,
                "day": 5
            },
            "text": {
                "headline": "o1",
                "text": "<p>We've developed a new series of AI models designed to spend more time thinking before they respond. They can reason through complex tasks and solve harder problems than previous models in science, coding, and math.\n\nToday, we are releasing the first of this series in ChatGPT and our API. This is a preview and we expect regular updates and improvements. Alongside this release, we’re also including evaluations for the next update, currently in development.</p>"
            },
            "media": {
                "url": "https://openai.com/index/introducing-chatgpt-pro/",
                "caption": "Introducing ChatGPT Pro: Broadening usage of frontier AI.",
                "credit": ""
            }
        },
        {
            "start_date": {
                "year": 2024,
                "month": 12,
                "day": 5
            },
            "text": {
                "headline": "Infinity",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://arxiv.org/html/2412.04431v1",
                "caption": "Infinity∞: Scaling Bitwise AutoRegressive Modeling for High-Resolution Image Synthesis",
                "credit": "Jian Han, Jinlai Liu, Yi Jiang, Bin Yan, Yuqi Zhang, Zehuan Yuan, Bingyue Peng, Xiaobing Liu"
            }
        },
        {
            "start_date": {
                "year": 2024,
                "month": 12,
                "day": 3
            },
            "text": {
                "headline": "Amazon Nova Pro",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://aws.amazon.com/es/blogs/aws/introducing-amazon-nova-frontier-intelligence-and-industry-leading-price-performance/",
                "caption": "Introducing Amazon Nova foundation models: Frontier intelligence and industry leading price performance",
                "credit": ""
            }
        },
        {
            "start_date": {
                "year": 2024,
                "month": 11,
                "day": 25
            },
            "text": {
                "headline": "Fugatto 1",
                "text": "<p>Fugatto is a versatile audio synthesis and transformation model capable of following free-form text instructions with optional audio inputs. While large language models (LLMs) trained with text on a simple next-token prediction objective can learn to infer instructions directly from the data, models trained solely on audio data lack this capacity. This is because audio data does not inherently contain the instructions that were used to generate it. To overcome this challenge, we introduce a specialized dataset generation approach optimized for producing a wide range of audio generation and transformation tasks, ensuring the data reveals meaningful relationships between audio and language. Another challenge lies in achieving compositional abilities – such as combining, interpolating between, or negating instructions – using data alone. To address it, we propose ComposableART, an inference-time technique that extends classifier-free guidance to compositional guidance. It enables the seamless and flexible composition of instructions, leading to highly customizable audio outputs outside the training distribution. Our evaluations across a diverse set of tasks demonstrate that Fugatto performs competitively with specialized models, while ComposableART enhances its sonic palette and control over synthesis. Most notably, we highlight our framework’s ability to synthesize emergent sounds – sonic phenomena that transcend conventional audio generation – unlocking new creative possibilities. Demo Website.</p>"
            },
            "media": {
                "url": "https://research.nvidia.com/publication/2024-11_fugatto-1-foundational-generative-audio-transformer-opus-1",
                "caption": "Fugatto 1 - Foundational Generative Audio Transformer Opus 1\n",
                "credit": "Rafael Valle, Rohan Badlani, Zhifeng Kong, Sang-gil Lee, Arushi Goel, Sungwon Kim,\nJoao Felipe Santos, Shuqi Dai, Siddharth Gururani, Aya AlJa'fari, Alex Liu, Kevin Shih, Wei Ping, Bryan Catanzaro\n"
            }
        },
        {
            "start_date": {
                "year": 2024,
                "month": 11,
                "day": 19
            },
            "text": {
                "headline": "Suno v4",
                "text": "<p>Today, we’re excited to introduce v4—the next step toward enabling you to make music at the speed of your ideas. When we launched v3 earlier this year, it opened up new possibilities for music creation. Post v3 launch, we’ve refined what worked and added more where it mattered most. The result is v4—a major update that takes music creation to the next level. v4 delivers cleaner audio, sharper lyrics, and more dynamic song structures.</p>"
            },
            "media": {
                "url": "https://suno.com/blog/v4",
                "caption": "Introducing v4",
                "credit": "Team Suno"
            }
        },
        {
            "start_date": {
                "year": 2024,
                "month": 11,
                "day": 18
            },
            "text": {
                "headline": "Pixtral Large",
                "text": "<p>Today we announce Pixtral Large, a 124B open-weights multimodal model built on top of Mistral Large 2. Pixtral Large is the second model in our multimodal family and demonstrates frontier-level image understanding. Particularly, the model is able to understand documents, charts and natural images, while maintaining the leading text-only understanding of Mistral Large 2.</p>"
            },
            "media": {
                "url": "https://mistral.ai/news/pixtral-large/",
                "caption": "Pixtral Large",
                "credit": "Mistral AI Team"
            }
        },
        {
            "start_date": {
                "year": 2024,
                "month": 11,
                "day": 16
            },
            "text": {
                "headline": "k0-math",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://web.archive.org/web/20250124103542/https://www.globaltimes.cn/page/202411/1323248.shtml",
                "caption": "Chinese AI start-up unveils latest reasoning model comparable to OpenAI o1 series",
                "credit": ""
            }
        },
        {
            "start_date": {
                "year": 2024,
                "month": 11,
                "day": 6
            },
            "text": {
                "headline": "Hunyuan-Large",
                "text": "<p>In this paper, we introduce Hunyuan-Large, which is currently the largest open-source Transformer-based mixture of experts model, with a total of 389 billion parameters and 52 billion activation parameters, capable of handling up to 256K tokens. We conduct a thorough evaluation of Hunyuan-Large's superior performance across various benchmarks including language understanding and generation, logical reasoning, mathematical problem-solving, coding, long-context, and aggregated tasks, where it outperforms LLama3.1-70B and exhibits comparable performance when compared to the significantly larger LLama3.1-405B model. Key practice of Hunyuan-Large include large-scale synthetic data that is orders larger than in previous literature, a mixed expert routing strategy, a key-value cache compression technique, and an expert-specific learning rate strategy. Additionally, we also investigate the scaling laws and learning rate schedule of mixture of experts models, providing valuable insights and guidances for future model development and optimization. The code and checkpoints of Hunyuan-Large are released to facilitate future innovations and applications.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2411.02265",
                "caption": "Hunyuan-Large: An Open-Source MoE Model with 52 Billion Activated Parameters by Tencent",
                "credit": "Xingwu Sun, Yanfeng Chen, Yiqing Huang, Ruobing Xie, Jiaqi Zhu, Kai Zhang, Shuaipeng Li, Zhen Yang, Jonny Han, Xiaobo Shu, Jiahao Bu, Zhongzhi Chen, Xuemeng Huang, Fengzong Lian,\nSaiyong Yang, Jianfeng Yan, Yuyuan Zeng, Xiaoqin Ren, Chao Yu, Lulu Wu, Yue Mao, Jun Xia, Tao Yang, Suncong Zheng, Kan Wu, Dian Jiao, Jinbao Xue, Xipeng Zhang, Decheng Wu, Kai Liu, Dengpeng Wu, Guanghui Xu, Shaohua Chen, Shuang Chen, Xiao Feng, Yigeng Hong, Junqiang Zheng, Chengcheng Xu, Zongwei Li, Xiong Kuang, Jianglu Hu, Yiqi Chen, Yuchi Deng, Guiyang Li, Ao Liu, Chenchen Zhang, Shihui Hu, Zilong Zhao, Zifan Wu, Yao Ding, Weichao Wang, Han Liu, Roberts Wang, Hao Fei, Peijie Yu, Ze Zhao, Xun Cao, Hai Wang, Fusheng Xiang, Mengyuan Huang, Zhiyuan Xiong, Bin Hu, Xuebin Hou, Lei Jiang, Jianqiang Ma, Jiajia Wu, Yaping Deng, Yi Shen, Qian Wang, Weijie Liu, Jie Liu, Meng Chen, Liang Dong, Weiwen Jia, Hu Chen, Feifei Liu, Rui Yuan, Huilin Xu, Zhenxiang Yan, Tengfei Cao, Zhichao Hu, Xinhua Feng, Dong Du, Tinghao Yu, Yangyu Tao, Feng Zhang, Jianchen Zhu, Chengzhong Xu, Xirui Li, Chong Zha, Wen Ouyang, Yinben Xia, Xiang Li, Zekun He, Rongpeng Chen, Jiawei Song, Ruibin Chen, Fan Jiang, Chongqing Zhao, Bo Wang, Hao Gong, Rong Gan, Winston Hu, Zhanhui Kang, Yong Yang, Yuhong Liu, Di Wang, and Jie Jiang.\n"
            }
        },
        {
            "start_date": {
                "year": 2024,
                "month": 10,
                "day": 28
            },
            "text": {
                "headline": "Doubao-pro",
                "text": "<p>A professional-grade, self-developed LLM supporting up to 128k tokens, enabling fine-tuning across the entire series. </p>"
            },
            "media": {
                "url": "https://www.volcengine.com/docs/6360/1264663",
                "caption": "Doubao General Model Pro (Doubao-pro)",
                "credit": ""
            }
        },
        {
            "start_date": {
                "year": 2024,
                "month": 10,
                "day": 22
            },
            "text": {
                "headline": "NVLM-X 72B",
                "text": "<p>We introduce NVLM 1.0, a family of frontier-class multimodal large language models (LLMs) that achieve state-of-the-art results on vision-language tasks, rivaling the leading proprietary models (e.g., GPT-4o) and open-access models (e.g., Llama 3-V 405B and InternVL 2). Remarkably, NVLM 1.0 shows improved text-only performance over its LLM backbone after multimodal training. In terms of model design, we perform a comprehensive comparison between decoder-only multimodal LLMs (e.g., LLaVA) and cross-attention-based models (e.g., Flamingo). Based on the strengths and weaknesses of both approaches, we propose a novel architecture that enhances both training efficiency and multimodal reasoning capabilities. Furthermore, we introduce a 1-D tile-tagging design for tile-based dynamic high-resolution images, which significantly boosts performance on multimodal reasoning and OCR-related tasks. Regarding training data, we meticulously curate and provide detailed information on our multimodal pretraining and supervised fine-tuning datasets. Our findings indicate that dataset quality and task diversity are more important than scale, even during the pretraining phase, across all architectures. Notably, we develop production-grade multimodality for the NVLM-1.0 models, enabling them to excel in vision-language tasks while maintaining and even improving text-only performance compared to their LLM backbones. To achieve this, we craft and integrate a high-quality text-only dataset into multimodal training, alongside a substantial amount of multimodal math and reasoning data, leading to enhanced math and coding capabilities across modalities. To advance research in the field, we release the model weights at this https URL and will open-source the training code for the community soon.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2409.11402",
                "caption": "NVLM: Open Frontier-Class Multimodal LLMs",
                "credit": "Wenliang Dai, Nayeon Lee, Boxin Wang, Zhuolin Yang, Zihan Liu, Jon Barker, Tuomas Rintamaki, Mohammad Shoeybi, Bryan Catanzaro, Wei Ping"
            }
        },
        {
            "start_date": {
                "year": 2024,
                "month": 10,
                "day": 22
            },
            "text": {
                "headline": "NVLM-H 72B",
                "text": "<p>We introduce NVLM 1.0, a family of frontier-class multimodal large language models (LLMs) that achieve state-of-the-art results on vision-language tasks, rivaling the leading proprietary models (e.g., GPT-4o) and open-access models (e.g., Llama 3-V 405B and InternVL 2). Remarkably, NVLM 1.0 shows improved text-only performance over its LLM backbone after multimodal training. In terms of model design, we perform a comprehensive comparison between decoder-only multimodal LLMs (e.g., LLaVA) and cross-attention-based models (e.g., Flamingo). Based on the strengths and weaknesses of both approaches, we propose a novel architecture that enhances both training efficiency and multimodal reasoning capabilities. Furthermore, we introduce a 1-D tile-tagging design for tile-based dynamic high-resolution images, which significantly boosts performance on multimodal reasoning and OCR-related tasks. Regarding training data, we meticulously curate and provide detailed information on our multimodal pretraining and supervised fine-tuning datasets. Our findings indicate that dataset quality and task diversity are more important than scale, even during the pretraining phase, across all architectures. Notably, we develop production-grade multimodality for the NVLM-1.0 models, enabling them to excel in vision-language tasks while maintaining and even improving text-only performance compared to their LLM backbones. To achieve this, we craft and integrate a high-quality text-only dataset into multimodal training, alongside a substantial amount of multimodal math and reasoning data, leading to enhanced math and coding capabilities across modalities. To advance research in the field, we release the model weights at this https URL and will open-source the training code for the community soon.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2409.11402",
                "caption": "NVLM: Open Frontier-Class Multimodal LLMs",
                "credit": "Wenliang Dai, Nayeon Lee, Boxin Wang, Zhuolin Yang, Zihan Liu, Jon Barker, Tuomas Rintamaki, Mohammad Shoeybi, Bryan Catanzaro, Wei Ping"
            }
        },
        {
            "start_date": {
                "year": 2024,
                "month": 10,
                "day": 22
            },
            "text": {
                "headline": "NVLM-D 72B",
                "text": "<p>We introduce NVLM 1.0, a family of frontier-class multimodal large language models (LLMs) that achieve state-of-the-art results on vision-language tasks, rivaling the leading proprietary models (e.g., GPT-4o) and open-access models (e.g., Llama 3-V 405B and InternVL 2). Remarkably, NVLM 1.0 shows improved text-only performance over its LLM backbone after multimodal training. In terms of model design, we perform a comprehensive comparison between decoder-only multimodal LLMs (e.g., LLaVA) and cross-attention-based models (e.g., Flamingo). Based on the strengths and weaknesses of both approaches, we propose a novel architecture that enhances both training efficiency and multimodal reasoning capabilities. Furthermore, we introduce a 1-D tile-tagging design for tile-based dynamic high-resolution images, which significantly boosts performance on multimodal reasoning and OCR-related tasks. Regarding training data, we meticulously curate and provide detailed information on our multimodal pretraining and supervised fine-tuning datasets. Our findings indicate that dataset quality and task diversity are more important than scale, even during the pretraining phase, across all architectures. Notably, we develop production-grade multimodality for the NVLM-1.0 models, enabling them to excel in vision-language tasks while maintaining and even improving text-only performance compared to their LLM backbones. To achieve this, we craft and integrate a high-quality text-only dataset into multimodal training, alongside a substantial amount of multimodal math and reasoning data, leading to enhanced math and coding capabilities across modalities. To advance research in the field, we release the model weights at this https URL and will open-source the training code for the community soon.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2409.11402",
                "caption": "NVLM: Open Frontier-Class Multimodal LLMs",
                "credit": "Wenliang Dai, Nayeon Lee, Boxin Wang, Zhuolin Yang, Zihan Liu, Jon Barker, Tuomas Rintamaki, Mohammad Shoeybi, Bryan Catanzaro, Wei Ping"
            }
        },
        {
            "start_date": {
                "year": 2024,
                "month": 10,
                "day": 18
            },
            "text": {
                "headline": "Yi-Lightning",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://www.lingyiwanwu.com/en https://platform.lingyiwanwu.com/",
                "caption": "Yi-Lightning",
                "credit": ""
            }
        },
        {
            "start_date": {
                "year": 2024,
                "month": 10,
                "day": 15
            },
            "text": {
                "headline": "CHAI-1",
                "text": "<p>We introduce Chai-1, a multi-modal foundation model for molecular structure prediction that performs at the state-of-the-art across a variety of tasks relevant to drug discovery. Chai-1 can optionally be prompted with experimental restraints (e.g. derived from wet-lab data) which boosts\nperformance by double-digit percentage points. Chai-1 can also be run in single-sequence mode without MSAs while preserving most of its performance. We release Chai-1 model weights and inference code as a Python package for non-commercial use and via a web interface where it can be used for free including for commercial drug discovery purposes.</p>"
            },
            "media": {
                "url": "https://www.chaidiscovery.com/blog/introducing-chai-1\nhttps://www.biorxiv.org/content/10.1101/2024.10.10.615955v2",
                "caption": "Introducing Chai-1: Decoding the molecular interactions of life",
                "credit": "Jacques Boitreaud, Jack Dent, Matthew McPartlon, Joshua Meier, Vinicius Reis, Alex Rogozhnikov, Kevin Wu"
            }
        },
        {
            "start_date": {
                "year": 2024,
                "month": 10,
                "day": 9
            },
            "text": {
                "headline": "Palmyra X 004",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://writer.com/engineering/actions-with-palmyra-x-004/",
                "caption": "Introducing actions with Palmyra X 004",
                "credit": "Sam Julien / Writer"
            }
        },
        {
            "start_date": {
                "year": 2024,
                "month": 10,
                "day": 8
            },
            "text": {
                "headline": "GR-2",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://arxiv.org/html/2410.06158v1",
                "caption": "GR-2: A Generative Video-Language-Action Model with Web-Scale Knowledge for Robot Manipulation",
                "credit": "Hang Li, Yifeng Li, Yuxiao Liu, Hongtao Wu, Jiafeng Xu, Yichu Yang, Hanbo Zhang, Minzhao Zhu"
            }
        },
        {
            "start_date": {
                "year": 2024,
                "month": 10,
                "day": 4
            },
            "text": {
                "headline": "Movie Gen Video",
                "text": "<p>We present Movie Gen, a cast of foundation models that generates high-quality, 1080p HD videos\nwith different aspect ratios and synchronized audio. We also show additional capabilities such as\nprecise instruction-based video editing and generation of personalized videos based on a user’s image.\nOur models set a new state-of-the-art on multiple tasks: text-to-video synthesis, video personalization,\nvideo editing, video-to-audio generation, and text-to-audio generation. Our largest video generation\nmodel is a 30B parameter transformer trained with a maximum context length of 73K video tokens,\ncorresponding to a generated video of 16 seconds at 16 frames-per-second. We show multiple technical\ninnovations and simplifications on the architecture, latent spaces, training objectives and recipes, data\ncuration, evaluation protocols, parallelization techniques, and inference optimizations that allow us to\nreap the benefits of scaling pre-training data, model size, and training compute for training large scale\nmedia generation models. We hope this paper helps the research community to accelerate progress\nand innovation in media generation models.\nAll videos from this paper are available at https://go.fb.me/MovieGenResearchVideos.</p>"
            },
            "media": {
                "url": "https://ai.meta.com/static-resource/movie-gen-research-paper",
                "caption": "Movie Gen: A Cast of Media Foundation Models",
                "credit": "Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen\nShi, Chih-Yao Ma, Ching-Yao Chuang, David Yan, Dhruv Choudhary, Dingkang Wang, Geet Sethi, Guan\nPang, Haoyu Ma, Ishan Misra, Ji Hou, Jialiang Wang, Kiran Jagadeesh, Kunpeng Li, Luxin Zhang, Mannat\nSingh, Mary Williamson, Matt Le, Mitesh Kumar Singh, Peizhao Zhang, Peter Vajda, Quentin Duval, Rohit\nGirdhar, Roshan Sumbaly, Sai Saketh Rambhatla, Sam Tsai, Samaneh Azadi, Samyak Datta, Sanyuan Chen,\nSean Bell, Sharadh Ramaswamy, Shelly Sheynin, Siddharth Bhattacharya, Tao Xu, Tingbo Hou, Wei-Ning\nHsu, Xi Yin, Xiaoliang Dai, Yaniv Taigman, Yaqiao Luo, Yen-Cheng Liu, Yi-Chiao Wu, Yue Zhao, Yuval\nKirstain, Zecheng He, Zijian He"
            }
        },
        {
            "start_date": {
                "year": 2024,
                "month": 9,
                "day": 24
            },
            "text": {
                "headline": "Llama 3.2 11B",
                "text": "<p>Today, we’re releasing Llama 3.2, which includes small and medium-sized vision LLMs (11B and 90B), and lightweight, text-only models (1B and 3B) that fit onto edge and mobile devices, including pre-trained and instruction-tuned versions.\nSupported by a broad ecosystem, the Llama 3.2 11B and 90B vision models are drop-in replacements for their corresponding text model equivalents, while exceeding on image understanding tasks compared to closed models, such as Claude 3 Haiku. Unlike other open multimodal models, both pre-trained and aligned models are available to be fine-tuned for custom applications using torchtune and deployed locally using torchchat. They’re also available to try using our smart assistant, Meta AI.\nWe’re sharing the first official Llama Stack distributions, which will greatly simplify the way developers work with Llama models in different environments, including single-node, on-prem, cloud, and on-device, enabling turnkey deployment of retrieval-augmented generation (RAG) and tooling-enabled applications with integrated safety.\nWe’ve been working closely with partners like AWS, Databricks, Dell Technologies, Fireworks, Infosys, and Together AI to build Llama Stack distributions for their downstream enterprise clients. On-device distribution is via PyTorch ExecuTorch, and single-node distribution is via Ollama.\nWe continue to share our work because we believe openness drives innovation and is good for developers, Meta, and the world. Llama is already leading the way on openness, modifiability, and cost efficiency—enabling more people to have creative, useful, and life-changing breakthroughs using generative AI.\nWe’re making Llama 3.2 models available for download on llama.com and Hugging Face, as well as available for immediate development on our broad ecosystem of partner platforms, including AMD, AWS, Databricks, Dell, Google Cloud, Groq, IBM, Intel, Microsoft Azure, NVIDIA, Oracle Cloud, Snowflake, and more.</p>"
            },
            "media": {
                "url": "https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/",
                "caption": "Llama 3.2: Revolutionizing edge AI and vision with open, customizable models",
                "credit": "Meta AI"
            }
        },
        {
            "start_date": {
                "year": 2024,
                "month": 9,
                "day": 19
            },
            "text": {
                "headline": "Qwen2.5-72B",
                "text": "<p>In the past three months since Qwen2’s release, numerous developers have built new models on the Qwen2 language models, providing us with valuable feedback. During this period, we have focused on creating smarter and more knowledgeable language models. Today, we are excited to introduce the latest addition to the Qwen family: Qwen2.5. We are announcing what might be the largest opensource release in history! Let’s get the party started!</p>"
            },
            "media": {
                "url": "https://qwenlm.github.io/blog/qwen2.5/",
                "caption": "Qwen2.5: A Party of Foundation Models!",
                "credit": "Qwen Team"
            }
        },
        {
            "start_date": {
                "year": 2024,
                "month": 9,
                "day": 19
            },
            "text": {
                "headline": "Qwen2.5 Instruct (72B)",
                "text": "<p>Qwen2.5 is the latest series of Qwen large language models. For Qwen2.5, we release a number of base language models and instruction-tuned language models ranging from 0.5 to 72 billion parameters. Qwen2.5 brings the following improvements upon Qwen2:\n\nSignificantly more knowledge and has greatly improved capabilities in coding and mathematics, thanks to our specialized expert models in these domains.\nSignificant improvements in instruction following, generating long texts (over 8K tokens), understanding structured data (e.g, tables), and generating structured outputs especially JSON. More resilient to the diversity of system prompts, enhancing role-play implementation and condition-setting for chatbots.\nLong-context Support up to 128K tokens and can generate up to 8K tokens.\nMultilingual support for over 29 languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more.</p>"
            },
            "media": {
                "url": "https://qwenlm.github.io/blog/qwen2.5/",
                "caption": "Qwen2.5: A Party of Foundation Models!",
                "credit": "Qwen Team"
            }
        },
        {
            "start_date": {
                "year": 2024,
                "month": 9,
                "day": 19
            },
            "text": {
                "headline": "Oryx 34B",
                "text": "<p>Visual data comes in various forms, ranging from small icons of just a few pixels to long videos spanning hours. Existing multi-modal LLMs usually standardize these diverse visual inputs to a fixed resolution for visual encoders and yield similar numbers of tokens for LLMs. This approach is non-optimal for multimodal understanding and inefficient for processing inputs with long and short visual contents. To solve the problem, we propose Oryx, a unified multimodal architecture for the spatial-temporal understanding of images, videos, and multi-view 3D scenes. Oryx offers an on-demand solution to seamlessly and efficiently process visual inputs with arbitrary spatial sizes and temporal lengths through two core innovations: 1) a pre-trained OryxViT model that can encode images at any resolution into LLM-friendly visual representations; 2) a dynamic compressor module that supports 1x to 16x compression on visual tokens by request. These design features enable Oryx to accommodate extremely long visual contexts, such as videos, with lower resolution and high compression while maintaining high recognition precision for tasks like document understanding with native resolution and no compression. Beyond the architectural improvements, enhanced data curation and specialized training on long-context retrieval and spatial-aware data help Oryx achieve strong capabilities in image, video, and 3D multimodal understanding simultaneously</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2409.12961v1",
                "caption": "Oryx MLLM: On-Demand Spatial-Temporal Understanding at Arbitrary Resolution\n",
                "credit": "Zuyan Liu, Yuhao Dong, Ziwei Liu, Winston Hu, Jiwen Lu, Yongming Rao"
            }
        },
        {
            "start_date": {
                "year": 2024,
                "month": 9,
                "day": 17
            },
            "text": {
                "headline": "Qwen2.5-32B",
                "text": "<p>In the past three months since Qwen2’s release, numerous developers have built new models on the Qwen2 language models, providing us with valuable feedback. During this period, we have focused on creating smarter and more knowledgeable language models. Today, we are excited to introduce the latest addition to the Qwen family: Qwen2.5. We are announcing what might be the largest opensource release in history! Let’s get the party started!\n\nThe Qwen2.5-7B model surpasses its predecessors and counterparts in numerous benchmarks, despite having fewer non-embedding parameters. It demonstrates significant improvements across various tasks, achieving 74.2 on general benchmarks like MMLU, 49.8 on math challenges such as MATH, and 57.9 on coding tasks like HumanEval.</p>"
            },
            "media": {
                "url": "https://qwenlm.github.io/blog/qwen2.5/ ",
                "caption": "Qwen2.5: A Party of Foundation Models!",
                "credit": "Qwen Team"
            }
        },
        {
            "start_date": {
                "year": 2024,
                "month": 9,
                "day": 12
            },
            "text": {
                "headline": "o1-preview",
                "text": "<p>We've developed a new series of AI models designed to spend more time thinking before they respond. They can reason through complex tasks and solve harder problems than previous models in science, coding, and math.\n\nToday, we are releasing the first of this series in ChatGPT and our API. This is a preview and we expect regular updates and improvements. Alongside this release, we’re also including evaluations for the next update, currently in development.</p>"
            },
            "media": {
                "url": "https://openai.com/index/introducing-openai-o1-preview/",
                "caption": "A new series of reasoning models for solving hard problems.",
                "credit": ""
            }
        },
        {
            "start_date": {
                "year": 2024,
                "month": 9,
                "day": 12
            },
            "text": {
                "headline": "o1-mini",
                "text": "<p>We've developed a new series of AI models designed to spend more time thinking before they respond. They can reason through complex tasks and solve harder problems than previous models in science, coding, and math.\n\n...\n\nWe’re also releasing OpenAI o1-mini, a faster, cheaper reasoning model that is particularly effective at coding. As a smaller model, o1-mini is 80% cheaper than o1-preview, making it a powerful, cost-effective model for applications that require reasoning but not broad world knowledge.</p>"
            },
            "media": {
                "url": "https://openai.com/index/learning-to-reason-with-llms/",
                "caption": "Learning to reason with LLMs",
                "credit": ""
            }
        },
        {
            "start_date": {
                "year": 2024,
                "month": 9,
                "day": 6
            },
            "text": {
                "headline": "DeepSeek-V2.5",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://huggingface.co/deepseek-ai/DeepSeek-V2.5",
                "caption": "DeepSeek-V2.5",
                "credit": "DeepSeek-AI, Aixin Liu, Bei Feng, Bin Wang, Bingxuan Wang, Bo Liu, Chenggang Zhao, Chengqi Dengr, Chong Ruan, Damai Dai, Daya Guo, Dejian Yang, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Hanwei Xu, Hao Yang, Haowei Zhang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Li, Hui Qu, J.L. Cai, Jian Liang, Jianzhong Guo, Jiaqi Ni, Jiashi Li, Jin Chen, Jingyang Yuan, Junjie Qiu, Junxiao Song, Kai Dong, Kaige Gao, Kang Guan, Lean Wang, Lecong Zhang, Lei Xu, Leyi Xia, Liang Zhao, Liyue Zhang, Meng Li, Miaojun Wang, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Mingming Li, Ning Tian, Panpan Huang, Peiyi Wang, Peng Zhang, Qihao Zhu, Qinyu Chen, Qiushi Du, R.J. Chen, R.L. Jin, Ruiqi Ge, Ruizhe Pan, Runxin Xu, Ruyi Chen, S.S. Li, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shaoqing Wu, Shengfeng Ye, Shirong Ma, Shiyu Wang, Shuang Zhou, Shuiping Yu, Shunfeng Zhou, Size Zheng, T. Wang, Tian Pei, Tian Yuan, Tianyu Sun, W.L. Xiao, Wangding Zeng, Wei An, Wen Liu, Wenfeng Liang, Wenjun Gao, Wentao Zhang, X.Q. Li, Xiangyue Jin, Xianzu Wang, Xiao Bi, Xiaodong Liu, Xiaohan Wang, Xiaojin Shen, Xiaokang Chen, Xiaosha Chen, Xiaotao Nie, Xiaowen Sun"
            }
        },
        {
            "start_date": {
                "year": 2024,
                "month": 9,
                "day": 5
            },
            "text": {
                "headline": "Hunyuan Turbo",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://zhidx.com/p/442250.html",
                "caption": "腾讯版“GPT-4o”来了！混元Turbo首发并上线，效率翻倍价格砍半\n",
                "credit": ""
            }
        },
        {
            "start_date": {
                "year": 2024,
                "month": 9,
                "day": 5
            },
            "text": {
                "headline": "AlphaProteo",
                "text": "<p>Computational design of protein-binding proteins is a fundamental capability with broad utility in biomedical research and biotechnology. Recent methods have made strides against some target proteins, but on-demand creation of high-affinity binders without multiple rounds of experimental testing remains\nan unsolved challenge. This technical report introduces AlphaProteo, a family of machine learning models for protein design, and details its performance on the de novo binder design problem. With AlphaProteo, we achieve 3- to 300-fold better binding affinities and higher experimental success rates than the best existing methods on seven target proteins. Our results suggest that AlphaProteo can generate binders \"ready-to-use\" for many research applications using only one round of medium-throughput screening\nand no further optimization.</p>"
            },
            "media": {
                "url": "https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/alphaproteo-generates-novel-proteins-for-biology-and-health-research/AlphaProteo2024.pdf",
                "caption": "De novo design of high-affinity protein binders with AlphaProteo",
                "credit": "Vinicius Zambaldi, David La, Alexander E. Chu, Harshnira Patani, Amy E. Danson, Tristan O. C. Kwan, Thomas Frerix, Rosalia G. Schneider, David Saxton, Ashok Thillaisundaram, Zachary Wu, Isabel Moraes, Oskar Lange, Eliseo Papa, Gabriella Stanton, Victor Martin, Sukhdeep Singh, Lai H. Wong, Russ Bates, Simon A. Kohl, Josh Abramson, Andrew W. Senior, Yilmaz Alguel, Mary Y. Wu,\nIrene M. Aspalter, Katie Bentley, David L.V. Bauer, Peter Cherepanov, Demis Hassabis, Pushmeet Kohli, Rob Fergus, Jue Wang"
            }
        },
        {
            "start_date": {
                "year": 2024,
                "month": 8,
                "day": 29
            },
            "text": {
                "headline": "Hairuo",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://encloud.inspur.com/hairuo/cp/index.html\n\nhttps://www.eguizhou.gov.cn/guiyang/2024-08/29/c_1016827.htm",
                "caption": "",
                "credit": ""
            }
        },
        {
            "start_date": {
                "year": 2024,
                "month": 8,
                "day": 29
            },
            "text": {
                "headline": "GLM-4-Plus",
                "text": "<p>At the KDD International Conference on Data Mining and Knowledge Discovery, the Zhipu GLM team unveiled the new generation of base large model—GLM-4-Plus. As the latest version of Zhipu’s fully self-developed GLM large model, GLM-4-Plus signifies Zhipu AI’s continuous dedication in the field of general artificial intelligence, advancing the independent and autonomous innovation of large model technology.</p>"
            },
            "media": {
                "url": "https://bigmodel.cn/dev/howuse/glm-4",
                "caption": "GLM-4-Plus",
                "credit": "Zhipu AI"
            }
        },
        {
            "start_date": {
                "year": 2024,
                "month": 8,
                "day": 22
            },
            "text": {
                "headline": "Jamba 1.5-Large",
                "text": "<p>We present Jamba-1.5, new instruction-tuned large language models based on our Jamba architecture. Jamba is a hybrid Transformer-Mamba mixture of experts architecture, providing high throughput and low memory usage across context lengths, while retaining the same or better quality as Transformer models. We release two model sizes: Jamba-1.5-Large, with 94B active parameters, and Jamba-1.5-Mini, with 12B active parameters. Both models are fine-tuned for a variety of conversational and instruction-following capabilties, and have an effective context length of 256K tokens, the largest amongst open-weight models. To support cost-effective inference, we introduce ExpertsInt8, a novel quantization technique that allows fitting Jamba-1.5-Large on a machine with 8 80GB GPUs when processing 256K-token contexts without loss of quality. When evaluated on a battery of academic and chatbot benchmarks, Jamba-1.5 models achieve excellent results while providing high throughput and outperforming other open-weight models on long-context benchmarks. The model weights for both sizes are publicly available under the Jamba Open Model License and we release ExpertsInt8 as open source.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2408.12570\nhttps://www.ai21.com/blog/announcing-jamba-model-family\nhttps://huggingface.co/ai21labs/AI21-Jamba-1.5-Large",
                "caption": "Jamba-1.5: Hybrid Transformer-Mamba Models at Scale",
                "credit": "Barak Lenz, Alan Arazi, Amir Bergman, Avshalom Manevich, Barak Peleg, Ben Aviram, Chen Almagor, Clara Fridman, Dan Padnos, Daniel Gissin, Daniel Jannai, Dor Muhlgay, Dor Zimberg, Edden M Gerber, Elad Dolev, Eran Krakovsky, Erez Safahi, Erez Schwartz, Gal Cohen, Gal Shachaf, Haim Rozenblum, Hofit Bata, Ido Blass, Inbal Magar, Itay Dalmedigos, Jhonathan Osin, Julie Fadlon, Maria Rozman, Matan Danos, Michael Gokhman, Mor Zusman, Naama Gidron, Nir Ratner, Noam Gat, Noam Rozen, Oded Fried, Ohad Leshno, Omer Antverg, Omri Abend, Opher Lieber, Or Dagan, Orit Cohavi, Raz Alon, Ro'i Belson, Roi Cohen, Rom Gilad, Roman Glozman, Shahar Lev, Shaked Meirom, Tal Delbari, Tal Ness, Tomer Asida, Tom Ben Gal, Tom Braude, Uriya Pumerantz, Yehoshua Cohen, Yonatan Belinkov, Yuval Globerson, Yuval Peleg Levy, Yoav Shoham"
            }
        },
        {
            "start_date": {
                "year": 2024,
                "month": 8,
                "day": 13
            },
            "text": {
                "headline": "Grok-2",
                "text": "<p>Grok-2 is our frontier language model with state-of-the-art reasoning capabilities. This release includes two members of the Grok family: Grok-2 and Grok-2 mini. Both models are now being released to Grok users on the 𝕏 platform.</p>"
            },
            "media": {
                "url": "https://x.ai/blog/grok-2",
                "caption": "Grok-2 Beta Release",
                "credit": ""
            }
        },
        {
            "start_date": {
                "year": 2024,
                "month": 8,
                "day": 7
            },
            "text": {
                "headline": "Table Tennis Agent",
                "text": "<p>Achieving human-level speed and performance on real world tasks is a north star for the robotics research community. This work takes a step towards that goal and presents the first learned robot agent that reaches amateur human-level performance in competitive table tennis. Table tennis is a physically demanding sport which requires human players to undergo years of training to achieve an advanced level of proficiency. In this paper, we contribute (1) a hierarchical and modular policy architecture consisting of (i) low level controllers with their detailed skill descriptors which model the agent's capabilities and help to bridge the sim-to-real gap and (ii) a high level controller that chooses the low level skills, (2) techniques for enabling zero-shot sim-to-real including an iterative approach to defining the task distribution that is grounded in the real-world and defines an automatic curriculum, and (3) real time adaptation to unseen opponents. Policy performance was assessed through 29 robot vs. human matches of which the robot won 45% (13/29). All humans were unseen players and their skill level varied from beginner to tournament level. Whilst the robot lost all matches vs. the most advanced players it won 100% matches vs. beginners and 55% matches vs. intermediate players, demonstrating solidly amateur human-level performance. Videos of the matches can be viewed at this https URL</p>"
            },
            "media": {
                "url": "https://deepmind.google/research/publications/107741/",
                "caption": "Achieving Human Level Competitive Robot Table Tennis",
                "credit": "David B. D'Ambrosio, Saminda Abeyruwan, Laura Graesser, Atil Iscen, Heni Ben Amor, Alex Bewley, Barney J. Reed, Krista Reymann, Leila Takayama, Yuval Tassa, Krzysztof Choromanski, Erwin Coumans, Deepali Jain, Navdeep Jaitly, Natasha Jaques, Satoshi Kataoka, Yuheng Kuang, Nevena Lazic, Reza Mahjourian, Sherry Moore, Kenneth Oslund, Anish Shankar, Vikas Sindhwani, Vincent Vanhoucke, Grace Vesom, Peng Xu, and Pannag R. Sanketi"
            }
        },
        {
            "start_date": {
                "year": 2024,
                "month": 8,
                "day": 6
            },
            "text": {
                "headline": "LLaVA-OV-72B",
                "text": "<p>We present LLaVA-OneVision, a family of open large multimodal models (LMMs) developed by consolidating our insights into data, models, and visual representations in the LLaVA-NeXT blog series. Our experimental results demonstrate that LLaVA-OneVision is the first single model that can simultaneously push the performance boundaries of open LMMs in three important computer vision scenarios: single-image, multi-image, and video scenarios. Importantly, the design of LLaVAOneVision allows strong transfer learning across different modalities/scenarios, yielding new emerging capabilities. In particular, strong video understanding and cross-scenario capabilities are demonstrated through task transfer from images to\nvideos.</p>"
            },
            "media": {
                "url": "https://arxiv.org/pdf/2408.03326",
                "caption": "LLaVA-OneVision: Easy Visual Task Transfer\n",
                "credit": "Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, Chunyuan Li"
            }
        },
        {
            "start_date": {
                "year": 2024,
                "month": 7,
                "day": 29
            },
            "text": {
                "headline": "AFM-server",
                "text": "<p>We present foundation language models developed to power Apple Intelligence features, including a ~3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.</p>"
            },
            "media": {
                "url": "https://machinelearning.apple.com/research/apple-intelligence-foundation-language-models",
                "caption": "Apple Intelligence Foundation Language Models",
                "credit": "Andy Narayanan, Aonan Zhang, Bowen Zhang, Chen Chen, Chong Wang, Chung-Cheng Chiu, David Qiu, Deepak Gopinath, Dian Ang Yap, Dong Yin, Feng Nan, Floris Weers, Guoli Yin, Haoshuo Huang, Jianyu Wang, Jiarui Lu, John Peebles, Ke Ye, Mark Lee, Nan Du, Qibin Chen, Quentin Keunebroek, Ruoming Pang, Sam Wiseman, Syd Evans, Tao Lei, Tom Gunter, Vivek Rathod, Xiang Kong, Xianzhi Du, Yanghao Li, Yongqiang Wang, Yuan Gao, Zaid Ahmed, Zhaoyang Xu, Zhiyun Lu, Zirui Wang, Al Rashid, Albin Madappally Jose, Alec Doane, Alfredo Bencomo, Allison Vanderby, Andrew Hansen, Ankur Jain, Anupama Mann Anupama, Areeba\nKamal, Bugu Wu, Carolina Brum, Charlie Maalouf, Chinguun Erdenebileg,\nChris Dulhanty, Dominik Moritz, Doug Kang, Eduardo Jimenez, Evan Ladd,\nFangping Shi, Felix Bai, Frank Chu, Fred Hohman, Hadas Kotek, Hannah\nGillis Coleman, Jane Li, Jeffrey Bigham, Jeffery Cao, Jeff Lai, Jessica Cheung, Jiulong Shan, Joe Zhou, John Li, Jun Qin, Karanjeet Singh, Karla Vega, Ke Ye, Kelvin Zou, Laura Heckman, Lauren Gardiner, Margit Bowler, Mark Lee, Maria Cordell, Meng Cao, Nicole Hay, Nilesh Shahdadpuri, Otto Godwin, Pranay Dighe, Pushyami Rachapudi, Ramsey Tantawi, Roman Frigg, Sam Davarnia, Sanskruti Shah, Saptarshi Guha, Sasha Sirovica, Shen Ma, Shuang Ma, Simon Wang, Sulgi Kim, Suma Jayaram, Vaishaal Shankar, Varsha Paidi, Vivek Kumar, Xiang Kong, Xin Wang, Xin Zheng, Walker Cheng, Yael Shrager, Yang Ye, Yasu Tanaka, Yihao Guo, Yunsong Meng, Zhao Tang Luo, Zhi Ouyang, Zhiyun Lu, Alp Aygar, Alvin Wan, Andrew Walkingshaw, Andy Narayanan, Antonie Lin, Arsalan Farooq, Brent Ramerth, Chong Wang, Colorado Reed, Chris Bartels, Chris Chaney, David Riazati, Eric Liang Yang, Erin Feldman, Gabriel Hochstrasser, Guillaume Seguin, Guoli Yin, Irina Belousova, Jianyu Wang, Joris Pelemans, Karen Yang, Keivan Alizadeh Vahid, Liangliang Cao, Mahyar Najibi , Marco Zuliani, Max Horton, Minsik Cho, Nikhil Bhendawade, Patrick Dong, Piotr Maj, Pulkit Agrawal, Qi Shan, Qibin Chen, Qichen Fu, Regan Poston, Sam Xu, Shuangning Liu, Sushma Rao, Tashweena Heeramun, Thomas Merth, Uday Rayala, Victor Cui, Vivek Rangarajan Sridhar, Vivek Rathod, Wencong Zhang, Wenqi Zhang, Wentao Wu, Xiang Kong, Xingyu Zhou, Xinwen Liu, Yang Zhao, Yin Xia, Zhile Ren, Zhongzheng Ren"
            }
        },
        {
            "start_date": {
                "year": 2024,
                "month": 7,
                "day": 29
            },
            "text": {
                "headline": "AFM-on-device",
                "text": "<p>We present foundation language models developed to power Apple Intelligence features, including a ∼3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute [Apple, 2024b]. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.</p>"
            },
            "media": {
                "url": "https://machinelearning.apple.com/research/apple-intelligence-foundation-language-models",
                "caption": "Apple Intelligence Foundation Language Models",
                "credit": "Andy Narayanan, Aonan Zhang, Bowen Zhang, Chen Chen, Chong Wang, Chung-Cheng Chiu, David Qiu, Deepak Gopinath, Dian Ang Yap, Dong Yin, Feng Nan, Floris Weers, Guoli Yin, Haoshuo Huang, Jianyu Wang, Jiarui Lu, John Peebles, Ke Ye, Mark Lee, Nan Du, Qibin Chen, Quentin Keunebroek, Ruoming Pang, Sam Wiseman, Syd Evans, Tao Lei, Tom Gunter, Vivek Rathod, Xiang Kong, Xianzhi Du, Yanghao Li, Yongqiang Wang, Yuan Gao, Zaid Ahmed, Zhaoyang Xu, Zhiyun Lu, Zirui Wang, Al Rashid, Albin Madappally Jose, Alec Doane, Alfredo Bencomo, Allison Vanderby, Andrew Hansen, Ankur Jain, Anupama Mann Anupama, Areeba\nKamal, Bugu Wu, Carolina Brum, Charlie Maalouf, Chinguun Erdenebileg,\nChris Dulhanty, Dominik Moritz, Doug Kang, Eduardo Jimenez, Evan Ladd,\nFangping Shi, Felix Bai, Frank Chu, Fred Hohman, Hadas Kotek, Hannah\nGillis Coleman, Jane Li, Jeffrey Bigham, Jeffery Cao, Jeff Lai, Jessica Cheung, Jiulong Shan, Joe Zhou, John Li, Jun Qin, Karanjeet Singh, Karla Vega, Ke Ye, Kelvin Zou, Laura Heckman, Lauren Gardiner, Margit Bowler, Mark Lee, Maria Cordell, Meng Cao, Nicole Hay, Nilesh Shahdadpuri, Otto Godwin, Pranay Dighe, Pushyami Rachapudi, Ramsey Tantawi, Roman Frigg, Sam Davarnia, Sanskruti Shah, Saptarshi Guha, Sasha Sirovica, Shen Ma, Shuang Ma, Simon Wang, Sulgi Kim, Suma Jayaram, Vaishaal Shankar, Varsha Paidi, Vivek Kumar, Xiang Kong, Xin Wang, Xin Zheng, Walker Cheng, Yael Shrager, Yang Ye, Yasu Tanaka, Yihao Guo, Yunsong Meng, Zhao Tang Luo, Zhi Ouyang, Zhiyun Lu, Alp Aygar, Alvin Wan, Andrew Walkingshaw, Andy Narayanan, Antonie Lin, Arsalan Farooq, Brent Ramerth, Chong Wang, Colorado Reed, Chris Bartels, Chris Chaney, David Riazati, Eric Liang Yang, Erin Feldman, Gabriel Hochstrasser, Guillaume Seguin, Guoli Yin, Irina Belousova, Jianyu Wang, Joris Pelemans, Karen Yang, Keivan Alizadeh Vahid, Liangliang Cao, Mahyar Najibi , Marco Zuliani, Max Horton, Minsik Cho, Nikhil Bhendawade, Patrick Dong, Piotr Maj, Pulkit Agrawal, Qi Shan, Qibin Chen, Qichen Fu, Regan Poston, Sam Xu, Shuangning Liu, Sushma Rao, Tashweena Heeramun, Thomas Merth, Uday Rayala, Victor Cui, Vivek Rangarajan Sridhar, Vivek Rathod, Wencong Zhang, Wenqi Zhang, Wentao Wu, Xiang Kong, Xingyu Zhou, Xinwen Liu, Yang Zhao, Yin Xia, Zhile Ren, Zhongzheng Ren"
            }
        },
        {
            "start_date": {
                "year": 2024,
                "month": 7,
                "day": 24
            },
            "text": {
                "headline": "Mistral Large 2",
                "text": "<p>Today, we are announcing Mistral Large 2, the new generation of our flagship model. Compared to its predecessor, Mistral Large 2 is significantly more capable in code generation, mathematics, and reasoning. It also provides a much stronger multilingual support, and advanced function calling capabilities.</p>"
            },
            "media": {
                "url": "https://mistral.ai/news/mistral-large-2407/",
                "caption": "Top-tier reasoning for high-complexity tasks, for your most sophisticated needs.",
                "credit": "Albert Jiang, Alexandre Sablayrolles, Alexis Tacnet, Alok Kothari, Antoine Roux, Arthur Mensch, Audrey Herblin-Stoop, Augustin Garreau, Austin Birky, Bam4d, Baptiste Bout, Baudouin de Monicault, Blanche Savary, Carole Rambaud, Caroline Feldman, Devendra Singh Chaplot, Diego de las Casas, Diogo Costa, Eleonore Arcelin, Emma Bou Hanna, Etienne Metzger, Gaspard Blanchet, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Harizo Rajaona, Henri Roussez, Hichem Sattouf, Ian Mack, Jean-Malo Delignon, Jessica Chudnovsky, Justus Murke, Kartik Khandelwal, Lawrence Stewart, Louis Martin, Louis Ternon, Lucile Saulnier, Lélio Renard Lavaud, Margaret Jennings, Marie Pellat, Marie Torelli, Marie-Anne Lachaux, Marjorie Janiewicz, Mickaël Seznec, Nicolas Schuhl, Niklas Muhs, Olivier de Garrigues, Patrick von Platen, Paul Jacob, Pauline Buche, Pavan Kumar Reddy, Perry Savas, Pierre Stock, Romain Sauvestre, Sagar Vaze, Sandeep Subramanian, Saurabh Garg, Sophia Yang, Szymon Antoniak, Teven Le Scao, Thibault Schueller, Thibaut Lavril, Thomas Wang, Théophile Gervet, Timothée Lacroix, Valera Nemychnikova, Wendy Shang, William El Sayed, William Marshall"
            }
        },
        {
            "start_date": {
                "year": 2024,
                "month": 7,
                "day": 23
            },
            "text": {
                "headline": "Llama 3.1-405B",
                "text": "<p>Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.</p>"
            },
            "media": {
                "url": "https://ai.meta.com/research/publications/the-llama-3-herd-of-models/",
                "caption": "The Llama 3 Herd of Models",
                "credit": "Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman,\nAlan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie\nSravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen\nGregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux,\nChaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang\nWu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle\nPintz, Danny Livshits, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino,\nDieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip\nRadenovic, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Graeme Nail, Gregoire\nMialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov,\nImanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan Geffert,\nJana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong,\nJenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe\nSpisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala,\nKartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer,\nKshitiz Malik, Kuenley Chiu, Kunal Bhalla, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence\nChen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat,\nLuke de Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas,\nMathew Oldham, Mathieu Rita, Maya Pavlova, Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar\nSingh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji,\nOlivier Duchenne, Onur Çelebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic, Peter Weng,\nPrajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong,\nRagavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta\nRaileanu, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor,\nRuan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell, Seohyun\nSonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy, Sheng Shen, Shengye Wan,\nShruti Bhosale, Shun Zhang, Simon Vandenhende, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin\nGururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas\nScialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta,\nVignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, Vladan Petrovic, Weiwei\nChu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaoqing Ellen Tan,\nXinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen\nSong, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen, and Zoe\nPapakipos.\n(core contributors)"
            }
        },
        {
            "start_date": {
                "year": 2024,
                "month": 7,
                "day": 18
            },
            "text": {
                "headline": "GPT-4o mini",
                "text": "<p>OpenAI is committed to making intelligence as broadly accessible as possible. Today, we're announcing GPT-4o mini, our most cost-efficient small model. We expect GPT-4o mini will significantly expand the range of applications built with AI by making intelligence much more affordable. GPT-4o mini scores 82% on MMLU and currently outperforms GPT-41 on chat preferences in LMSYS leaderboard(opens in a new window). It is priced at 15 cents per million input tokens and 60 cents per million output tokens, an order of magnitude more affordable than previous frontier models and more than 60% cheaper than GPT-3.5 Turbo.</p>"
            },
            "media": {
                "url": "https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/",
                "caption": "GPT-4o mini: advancing cost-efficient intelligence",
                "credit": "Pre-training leads\nAidan Clark, Alex Paino, Jacob Menick\n\nPost-training leads\nLiam Fedus, Luke Metz\n\nArchitecture leads\nClemens Winter, Lia Guy\n\nOptimization leads\nSam Schoenholz, Daniel Levy\n\nLong-context lead\nNitish Keskar\n\nPre-training Data leads\nAlex Carney, Alex Paino, Ian Sohl, Qiming Yuan\n\nTokenizer lead\nReimar Leike\n\nHuman data leads\nArka Dhar, Brydon Eastman, Mia Glaese\n\nEval lead\nBen Sokolowsky\n\nData flywheel lead\nAndrew Kondrich\n\nInference lead\nFelipe Petroski Such\n\nInference Productionization lead\nHenrique Ponde de Oliveira Pinto\n\nPost-training infrastructure leads\nJiayi Weng, Randall Lin, Youlong Cheng\n\nPre-training organization lead\nNick Ryder\n\nPre-training program lead\nLauren Itow\n\nPost-training organization leads\nBarret Zoph, John Schulman\n\nPost-training program lead\nMianna Chen\n\nCore contributors\nAdam Lerer, Adam P. Goucher, Adam Perelman, Akila Welihinda, Alec Radford, Alex Borzunov, Alex Carney, Alex Chow, Alex Paino, Alex Renzin, Alex Tachard Passos, Alexi Christakis, Ali Kamali, Allison Moyer, Allison Tam, Amadou Crookes, Amin Tootoonchian, Ananya Kumar, Andrej Karpathy, Andrey Mishchenko, Andrew Cann, Andrew Kondrich, Andrew Tulloch, Angela Jiang, Antoine Pelisse, Antonia Woodford, Anuj Gosalia, Avi Nayak, Avital Oliver, Behrooz Ghorbani, Ben Leimberger, Ben Wang, Beth Hoover, Blake Samic, Brian Guarraci, Brydon Eastman, Camillo Lugaresi, Chak Li, Charlotte Barette, Chelsea Voss, Chen Ding, Chong Zhang, Chris Beaumont, Chris Hallacy, Chris Koch, Christian Gibson, Christine Choi, Christopher Hesse, Colin Wei, Daniel Kappler, Daniel Levin, Daniel Levy, David Farhi, David Mely, David Sasaki, Dimitris Tsipras, Doug Li, Duc Phong Nguyen, Duncan Findlay, Edmund Wong, Ehsan Asdar, Elizabeth Proehl, Elizabeth Yang, Eric Peterson, Eric Sigler, Eugene Brevdo, Farzad Khorasani, Francis Zhang, Gene Oden, Geoff Salmon, Hadi Salman, Haiming Bao, Heather Schmidt, Hongyu Ren, Hyung Won Chung, Ian Kivlichan, Ian O'Connell, Ian Osband, Ibrahim Okuyucu, Ilya Kostrikov, Ingmar Kanitscheider, Jacob Coxon, James Crooks, James Lennon, Jane Park, Jason Teplitz, Jason Wei, Jason Wolfe, Jay Chen, Jeff Harris, Jiayi Weng, Jie Tang, Joanne Jang, Jonathan Ward, Jonathan McKay, Jong Wook Kim, Josh Gross, Josh Kaplan, Joy Jiao, Joyce Lee, Juntang Zhuang, Kai Fricke, Kavin Karthik, Kenny Hsu, Kiel Howe, Kyle Luther, Larry Kai, Lauren Itow, Leo Chen, Lia Guy, Lien Mamitsuka, Lilian Weng, Long Ouyang, Louis Feuvrier, Lukas Kondraciuk, Lukasz Kaiser, Lyric Doshi, Mada Aflak, Maddie Simens, Madeleine Thompson, Marat Dukhan, Marvin Zhang, Mateusz Litwin, Matthew Zeng, Max Johnson, Mayank Gupta, Mia Glaese, Michael Janner, Michael Petrov, Michael Wu, Michelle Fradin, Michelle Pokrass, Miguel Oom Temudo de Castro, Mikhail Pavlov, Minal Khan, Mo Bavarian, Murat Yesildal, Natalia Gimelshein, Natalie Staudacher, Nick Stathas, Nik Tezak, Nithanth Kudige, Noel Bundick, Ofir Nachum, Oleg Boiko, Oleg Murk, Olivier Godement, Owen Campbell-Moore, Philip Pronin, Philippe Tillet, Rachel Lim, Rajan Troll, Randall Lin, Rapha gontijo lopes, Raul Puri, Reah Miyara, Reimar Leike, Renaud Gaubert, Reza Zamani, Rob Honsby, Rohit Ramchandani, Rory Carmichael, Ruslan Nigmatullin, Ryan Cheu, Sara Culver, Scott Gray, Sean Grove, Sean Metzger, Shantanu Jain, Shengjia Zhao, Sherwin Wu, Shuaiqi (Tony) Xia, Sonia Phene, Spencer Papay, Steve Coffey, Steve Lee, Steve Lee, Stewart Hall, Suchir Balaji, Tal Broda, Tal Stramer, Tarun Gogineni, Ted Sanders, Thomas Cunninghman, Thomas Dimson, Thomas Raoux, Tianhao Zheng, Christina Kim, Todd Underwood, Tristan Heywood, Valerie Qi, Vinnie Monaco, Vlad Fomenko, Weiyi Zheng, Wenda Zhou, Wojciech Zaremba, Yash Patil, Yilei, Qian, Yongjik Kim, Youlong Cheng, Yuchen He, Yuchen Zhang, Yujia Jin, Yunxing Dai, Yury Malkov"
            }
        },
        {
            "start_date": {
                "year": 2024,
                "month": 7,
                "day": 16
            },
            "text": {
                "headline": "Mathstral",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://huggingface.co/mistralai/Mathstral-7B-v0.1\nhttps://mistral.ai/news/mathstral",
                "caption": "Mathstral-7B-v0.1 ",
                "credit": "Albert Jiang, Alexandre Sablayrolles, Alexis Tacnet, Alok Kothari, Antoine Roux, Arthur Mensch, Audrey Herblin-Stoop, Augustin Garreau, Austin Birky, Bam4d, Baptiste Bout, Baudouin de Monicault, Blanche Savary, Carole Rambaud, Caroline Feldman, Devendra Singh Chaplot, Diego de las Casas, Eleonore Arcelin, Emma Bou Hanna, Etienne Metzger, Gaspard Blanchet, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Harizo Rajaona, Henri Roussez, Hichem Sattouf, Ian Mack, Jean-Malo Delignon, Jessica Chudnovsky, Justus Murke, Kartik Khandelwal, Lawrence Stewart, Louis Martin, Louis Ternon, Lucile Saulnier, Lélio Renard Lavaud, Margaret Jennings, Marie Pellat, Marie Torelli, Marie-Anne Lachaux, Marjorie Janiewicz, Mickaël Seznec, Nicolas Schuhl, Niklas Muhs, Olivier de Garrigues, Patrick von Platen, Paul Jacob, Pauline Buche, Pavan Kumar Reddy, Perry Savas, Pierre Stock, Romain Sauvestre, Sagar Vaze, Sandeep Subramanian, Saurabh Garg, Sophia Yang, Szymon Antoniak, Teven Le Scao, Thibault Schueller, Thibaut Lavril, Thomas Wang, Théophile Gervet, Timothée Lacroix, Valera Nemychnikova, Wendy Shang, William El Sayed, William Marshall"
            }
        },
        {
            "start_date": {
                "year": 2024,
                "month": 7,
                "day": 16
            },
            "text": {
                "headline": "DeepL LLM",
                "text": "<p>Our next-generation (“next-gen”) language model outperforms Google Translate, ChatGPT-4, and Microsoft for translation quality\nThe new LLM's translations require fewer edits, with Google needing 2x and ChatGPT-4 needing 3x more edits to achieve the same quality\nBuilt using our own groundbreaking, specialized LLM technology and proprietary training data, designed specifically for translation\nThe same enterprise-level security you’re used to for Pro customers</p>"
            },
            "media": {
                "url": "https://www.deepl.com/en/blog/next-gen-language-model",
                "caption": "DeepL's next-gen LLM outperforms ChatGPT-4, Google, and Microsoft for translation quality",
                "credit": "DeepL"
            }
        },
        {
            "start_date": {
                "year": 2024,
                "month": 6,
                "day": 25
            },
            "text": {
                "headline": "ESM3 (98B)",
                "text": "<p>More than three billion years of evolution have\nproduced an image of biology encoded into the\nspace of natural proteins. Here we show that language models trained on tokens generated by evolution can act as evolutionary simulators to generate functional proteins that are far away from\nknown proteins. We present ESM3, a frontier\nmultimodal generative language model that reasons over the sequence, structure, and function\nof proteins. ESM3 can follow complex prompts\ncombining its modalities and is highly responsive\nto biological alignment. We have prompted ESM3\nto generate fluorescent proteins with a chain of\nthought. Among the generations that we synthesized, we found a bright fluorescent protein at far\ndistance (58% identity) from known fluorescent\nproteins. Similarly distant natural fluorescent proteins are separated by over five hundred million\nyears of evolution,\n\n(from paper preview: https://evolutionaryscale-public.s3.us-east-2.amazonaws.com/research/esm3.pdf )</p>"
            },
            "media": {
                "url": "https://www.evolutionaryscale.ai/blog/esm3-release ",
                "caption": "ESM3: Simulating 500 million years of evolution with a language model",
                "credit": "Thomas Hayes, Roshan Rao, Halil Akin, Nicholas James Sofroniew, Deniz Oktay, Zeming Lin, Robert Verkuil, Vincent Quy Tran, Jonathan Deaton, Marius Wiggert, Rohil Badkundri, Irhum Shafkat, Jun Gong, Alexander Derry, Raul Santiago Molina, Neil Thomas, Yousuf Khan, Chetan Mishra, Carolyn Kim, Liam J Bartie, Patrick D Hsu, Tom Sercu, Salvatore Candido, Alexander Rives"
            }
        },
        {
            "start_date": {
                "year": 2024,
                "month": 6,
                "day": 24
            },
            "text": {
                "headline": "Cambrian-1-34B",
                "text": "<p>We introduce Cambrian-1, a family of multimodal LLMs (MLLMs) designed with a vision-centric approach. While stronger language models can enhance multimodal capabilities, the design choices for vision components are often insufficiently explored and disconnected from visual representation learning research. This gap hinders accurate sensory grounding in real-world scenarios. Our study uses LLMs and visual instruction tuning as an interface to evaluate various visual representations, offering new insights into different models and architectures -- self-supervised, strongly supervised, or combinations thereof -- based on experiments with over 20 vision encoders. We critically examine existing MLLM benchmarks, address the difficulties involved in consolidating and interpreting results from various tasks, and introduce a new vision-centric benchmark, CV-Bench. To further improve visual grounding, we propose the Spatial Vision Aggregator (SVA), a dynamic and spatially-aware connector that integrates high-resolution vision features with LLMs while reducing the number of tokens. Additionally, we discuss the curation of high-quality visual instruction-tuning data from publicly available sources, emphasizing the importance of data source balancing and distribution ratio. Collectively, Cambrian-1 not only achieves state-of-the-art performance but also serves as a comprehensive, open cookbook for instruction-tuned MLLMs. We provide model weights, code, supporting tools, datasets, and detailed instruction-tuning and evaluation recipes. We hope our release will inspire and accelerate advancements in multimodal systems and visual representation learning.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2406.16860",
                "caption": "Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal LLMs\n",
                "credit": "Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha Akula, Jihan Yang, Shusheng Yang, Adithya Iyer, Xichen Pan, Ziteng Wang, Rob Fergus, Yann LeCun, Saining Xie"
            }
        },
        {
            "start_date": {
                "year": 2024,
                "month": 6,
                "day": 24
            },
            "text": {
                "headline": "Cambrian-1-13B",
                "text": "<p>We introduce Cambrian-1, a family of multimodal LLMs (MLLMs) designed with a vision-centric approach. While stronger language models can enhance multimodal capabilities, the design choices for vision components are often insufficiently explored and disconnected from visual representation learning research. This gap hinders accurate sensory grounding in real-world scenarios. Our study uses LLMs and visual instruction tuning as an interface to evaluate various visual representations, offering new insights into different models and architectures -- self-supervised, strongly supervised, or combinations thereof -- based on experiments with over 20 vision encoders. We critically examine existing MLLM benchmarks, address the difficulties involved in consolidating and interpreting results from various tasks, and introduce a new vision-centric benchmark, CV-Bench. To further improve visual grounding, we propose the Spatial Vision Aggregator (SVA), a dynamic and spatially-aware connector that integrates high-resolution vision features with LLMs while reducing the number of tokens. Additionally, we discuss the curation of high-quality visual instruction-tuning data from publicly available sources, emphasizing the importance of data source balancing and distribution ratio. Collectively, Cambrian-1 not only achieves state-of-the-art performance but also serves as a comprehensive, open cookbook for instruction-tuned MLLMs. We provide model weights, code, supporting tools, datasets, and detailed instruction-tuning and evaluation recipes. We hope our release will inspire and accelerate advancements in multimodal systems and visual representation learning.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2406.16860",
                "caption": "Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal LLMs\n",
                "credit": "Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha Akula, Jihan Yang, Shusheng Yang, Adithya Iyer, Xichen Pan, Ziteng Wang, Rob Fergus, Yann LeCun, Saining Xie"
            }
        },
        {
            "start_date": {
                "year": 2024,
                "month": 6,
                "day": 24
            },
            "text": {
                "headline": "Cambrian-1-8B",
                "text": "<p>We introduce Cambrian-1, a family of multimodal LLMs (MLLMs) designed with a vision-centric approach. While stronger language models can enhance multimodal capabilities, the design choices for vision components are often insufficiently explored and disconnected from visual representation learning research. This gap hinders accurate sensory grounding in real-world scenarios. Our study uses LLMs and visual instruction tuning as an interface to evaluate various visual representations, offering new insights into different models and architectures -- self-supervised, strongly supervised, or combinations thereof -- based on experiments with over 20 vision encoders. We critically examine existing MLLM benchmarks, address the difficulties involved in consolidating and interpreting results from various tasks, and introduce a new vision-centric benchmark, CV-Bench. To further improve visual grounding, we propose the Spatial Vision Aggregator (SVA), a dynamic and spatially-aware connector that integrates high-resolution vision features with LLMs while reducing the number of tokens. Additionally, we discuss the curation of high-quality visual instruction-tuning data from publicly available sources, emphasizing the importance of data source balancing and distribution ratio. Collectively, Cambrian-1 not only achieves state-of-the-art performance but also serves as a comprehensive, open cookbook for instruction-tuned MLLMs. We provide model weights, code, supporting tools, datasets, and detailed instruction-tuning and evaluation recipes. We hope our release will inspire and accelerate advancements in multimodal systems and visual representation learning.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2406.16860",
                "caption": "Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal LLMs\n",
                "credit": "Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha Akula, Jihan Yang, Shusheng Yang, Adithya Iyer, Xichen Pan, Ziteng Wang, Rob Fergus, Yann LeCun, Saining Xie"
            }
        },
        {
            "start_date": {
                "year": 2024,
                "month": 6,
                "day": 20
            },
            "text": {
                "headline": "Claude 3.5 Sonnet",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://www-cdn.anthropic.com/fed9cc193a14b84131812372d8d5857f8f304c52/Model_Card_Claude_3_Addendum.pdf",
                "caption": "Claude 3.5 Sonnet",
                "credit": ""
            }
        },
        {
            "start_date": {
                "year": 2024,
                "month": 6,
                "day": 17
            },
            "text": {
                "headline": "DeepSeek-Coder-V2 236B",
                "text": "<p>We present DeepSeek-Coder-V2, an open-source Mixture-of-Experts (MoE) code language model that achieves performance comparable to GPT4-Turbo in code-specific tasks. Specifically, DeepSeek-Coder-V2 is further pre-trained from an intermediate checkpoint of DeepSeek-V2 with additional 6 trillion tokens. Through this continued pre-training, DeepSeek-Coder-V2 substantially enhances the coding and mathematical reasoning capabilities of DeepSeek-V2, while maintaining comparable performance in general language tasks. Compared to DeepSeekCoder-33B, DeepSeek-Coder-V2 demonstrates significant advancements in various aspects of code-related tasks, as well as reasoning and general capabilities. Additionally, DeepSeek-CoderV2 expands its support for programming languages from 86 to 338, while extending the context length from 16K to 128K. In standard benchmark evaluations, DeepSeek-Coder-V2 achieves superior performance compared to closed-source models such as GPT4-Turbo, Claude 3 Opus, and Gemini 1.5 Pro in coding and math benchmarks.</p>"
            },
            "media": {
                "url": "https://github.com/deepseek-ai/DeepSeek-Coder-V2",
                "caption": "DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence",
                "credit": "Qihao Zhu, Daya Guo, Zhihong Shao, Dejian Yang, Peiyi Wang, Runxin Xu, Y. Wu, Yukun Li, Huazuo Gao, Shirong Ma, Wangding Zeng, Xiao Bi, Zihui Gu, Hanwei Xu, Damai Dai, Kai Dong, Liyue Zhang, Yishi Piao, Zhibin Gou, Zhenda Xie, Zhewen Hao, Bingxuan Wang, Junxiao Song, Deli Chen, Xin Xie, Kang Guan, Yuxiang You, Aixin Liu, Qiushi Du, Wenjun Gao, Xuan Lu, Qinyu Chen, Yaohui Wang, Chengqi Deng, Jiashi Li, Chenggang Zhao, Chong Ruan, Fuli Luo, Wenfeng Liang"
            }
        },
        {
            "start_date": {
                "year": 2024,
                "month": 6,
                "day": 14
            },
            "text": {
                "headline": "Nemotron-4 340B",
                "text": "<p>We release the Nemotron-4 340B model family, including Nemotron-4-340B-Base, Nemotron-4-\n340B-Instruct, and Nemotron-4-340B-Reward. Our models are open access under the NVIDIA Open\nModel License Agreement, a permissive model license that allows distribution, modification, and use of\nthe models and its outputs. These models perform competitively to open access models on a wide range\nof evaluation benchmarks, and were sized to fit on a single DGX H100 with 8 GPUs when deployed in\nFP8 precision. We believe that the community can benefit from these models in various research studies\nand commercial applications, especially for generating synthetic data to train smaller language models.\nNotably, over 98% of data used in our model alignment process is synthetically generated, showcasing\nthe effectiveness of these models in generating synthetic data. To further support open research and\nfacilitate model development, we are also open-sourcing the synthetic data generation pipeline used in\nour model alignment process.\n\n(from technical report: https://d1qx31qr3h6wln.cloudfront.net/publications/Nemotron_4_340B_8T_0.pdf )</p>"
            },
            "media": {
                "url": "https://blogs.nvidia.com/blog/nemotron-4-synthetic-data-generation-llm-training/ ",
                "caption": "NVIDIA Releases Open Synthetic Data Generation Pipeline for Training Large Language Models",
                "credit": "Bo Adler, Niket Agarwal, Ashwath Aithal, Dong H. Anh, Pallab Bhattacharya, Annika Brundyn, Jared Casper, Bryan Catanzaro, Sharon Clay, Jonathan Cohen, Sirshak Das, Ayush Dattagupta, Olivier Delalleau, Leon Derczynski, Yi Dong, Daniel Egert, Ellie Evans, Aleksander Ficek, Denys Fridman, Shaona Ghosh, Boris Ginsburg, Igor Gitman, Tomasz Grzegorzek, Robert Hero, Jining Huang, Vibhu Jawa, Joseph Jennings, Aastha Jhunjhunwala, John Kamalu, Sadaf Khan, Oleksii Kuchaiev, Patrick LeGresley, Hui Li, Jiwei Liu, Zihan Liu, Eileen Long, Ameya Sunil Mahabaleshwarkar, Somshubra Majumdar, James Maki, Miguel Martinez, Maer Rodrigues de Melo, Ivan Moshkov, Deepak Narayanan, Sean Narenthiran, Jesus Navarro, Phong Nguyen, Osvald Nitski, Vahid Noroozi, Guruprasad Nutheti, Christopher Parisien, Jupinder Parmar, Mostofa Patwary, Krzysztof Pawelec, Wei Ping, Shrimai Prabhumoye, Rajarshi Roy, Trisha Saar, Vasanth Rao Naik Sabavat, Sanjeev Satheesh, Jane Polak Scowcroft, Jason Sewall, Pavel Shamis, Gerald Shen, Mohammad Shoeybi, Dave Sizer, Misha Smelyanskiy, Felipe Soares, Makesh Narsimhan Sreedhar, Dan Su, Sandeep Subramanian, Shengyang Sun, Shubham Toshniwal, Hao Wang, Zhilin Wang, Jiaxuan You, Jiaqi Zeng, Jimmy Zhang, Jing Zhang, Vivienne Zhang, Yian Zhang, Chen Zhu"
            }
        },
        {
            "start_date": {
                "year": 2024,
                "month": 6,
                "day": 13
            },
            "text": {
                "headline": "OpenVLA",
                "text": "<p>Large policies pretrained on a combination of Internet-scale vision-language data and diverse robot demonstrations have the potential to change how we teach robots new skills: rather than training new behaviors from scratch, we can fine-tune such vision-language-action (VLA) models to obtain robust, generalizable policies for visuomotor control. Yet, widespread adoption of VLAs for robotics has been challenging as 1) existing VLAs are largely closed and inaccessible to the public, and 2) prior work fails to explore methods for efficiently fine-tuning VLAs for new tasks, a key component for adoption. Addressing these challenges, we introduce OpenVLA, a 7B-parameter open-source VLA trained on a diverse collection of 970k real-world robot demonstrations. OpenVLA builds on a Llama 2 language model combined with a visual encoder that fuses pretrained features from DINOv2 and SigLIP. As a product of the added data diversity and new model components, OpenVLA demonstrates strong results for generalist manipulation, outperforming closed models such as RT-2-X (55B) by 16.5% in absolute task success rate across 29 tasks and multiple robot embodiments, with 7x fewer parameters. We further show that we can effectively fine-tune OpenVLA for new settings, with especially strong generalization results in multi-task environments involving multiple objects and strong language grounding abilities, and outperform expressive from-scratch imitation learning methods such as Diffusion Policy by 20.4%. We also explore compute efficiency; as a separate contribution, we show that OpenVLA can be fine-tuned on consumer GPUs via modern low-rank adaptation methods and served efficiently via quantization without a hit to downstream success rate. Finally, we release model checkpoints, fine-tuning notebooks, and our PyTorch codebase with built-in support for training VLAs at scale on Open X-Embodiment datasets.</p>"
            },
            "media": {
                "url": "https://openvla.github.io/\nhttps://arxiv.org/abs/2406.09246",
                "caption": "OpenVLA: An Open-Source Vision-Language-Action Mode",
                "credit": "Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Balakrishna, Suraj Nair, Rafael Rafailov, Ethan Foster, Grace Lam, Pannag Sanketi, Quan Vuong, Thomas Kollar, Benjamin Burchfiel, Russ Tedrake, Dorsa Sadigh, Sergey Levine, Percy Liang, Chelsea Finn"
            }
        },
        {
            "start_date": {
                "year": 2024,
                "month": 6,
                "day": 7
            },
            "text": {
                "headline": "Qwen2-72B",
                "text": "<p>After months of efforts, we are pleased to announce the evolution from Qwen1.5 to Qwen2. This time, we bring to you:\n- Pretrained and instruction-tuned models of 5 sizes, including Qwen2-0.5B, Qwen2-1.5B, Qwen2-7B, Qwen2-57B-A14B, and Qwen2-72B;\n- Having been trained on data in 27 additional languages besides English and Chinese;\n- State-of-the-art performance in a large number of benchmark evaluations;\n- Significantly improved performance in coding and mathematics;\n- Extended context length support up to 128K tokens with Qwen2-7B-Instruct and Qwen2-72B-Instruct.\n\n(Technical report to follow)</p>"
            },
            "media": {
                "url": "https://qwenlm.github.io/blog/qwen2/ \nhttps://arxiv.org/abs/2407.10671 ",
                "caption": "Hello Qwen2",
                "credit": "Qwen Team"
            }
        },
        {
            "start_date": {
                "year": 2024,
                "month": 5,
                "day": 21
            },
            "text": {
                "headline": "ALLaM adapted13B\n",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2407.15390\nhttps://huggingface.co/ALLaM-AI/ALLaM-7B-Instruct-previehttps://www.middleeastainews.com/p/sdaias-allam-arabic-llm-live-on-watsonxw",
                "caption": "ALLaM: Large Language Models for Arabic and English\n",
                "credit": "M Saiful Bari, Yazeed Alnumay, Norah A. Alzahrani, Nouf M. Alotaibi, Hisham A. Alyahya, Sultan AlRashed, Faisal A. Mirza, Shaykhah Z. Alsubaie, Hassan A. Alahmed, Ghadah Alabduljabbar, Raghad Alkhathran, Yousef Almushayqih, Raneem Alnajim, Salman Alsubaihi, Maryam Al Mansour, Majed Alrubaian, Ali Alammari, Zaki Alawami, Abdulmohsen Al-Thubaity, Ahmed Abdelali, Jeril Kuriakose, Abdalghani Abujabal, Nora Al-Twairesh, Areeb Alowisheq, Haidar Khan"
            }
        },
        {
            "start_date": {
                "year": 2024,
                "month": 5,
                "day": 21
            },
            "text": {
                "headline": "ALLaM adapted 70B ",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2407.15390",
                "caption": "ALLaM: Large Language Models for Arabic and English\n",
                "credit": "M Saiful Bari, Yazeed Alnumay, Norah A. Alzahrani, Nouf M. Alotaibi, Hisham A. Alyahya, Sultan AlRashed, Faisal A. Mirza, Shaykhah Z. Alsubaie, Hassan A. Alahmed, Ghadah Alabduljabbar, Raghad Alkhathran, Yousef Almushayqih, Raneem Alnajim, Salman Alsubaihi, Maryam Al Mansour, Majed Alrubaian, Ali Alammari, Zaki Alawami, Abdulmohsen Al-Thubaity, Ahmed Abdelali, Jeril Kuriakose, Abdalghani Abujabal, Nora Al-Twairesh, Areeb Alowisheq, Haidar Khan"
            }
        },
        {
            "start_date": {
                "year": 2024,
                "month": 5,
                "day": 21
            },
            "text": {
                "headline": "ALLaM 34B",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://openreview.net/pdf?id=MscdsFVZrN",
                "caption": "AI Models for Arabic and English",
                "credit": "M Saiful Bari, Yazeed Alnumay, Norah A. Alzahrani, Nouf M. Alotaibi, Hisham A. Alyahya, Sultan AlRashed, Faisal A. Mirza, Shaykhah Z. Alsubaie, Hassan A. Alahmed, Ghadah Alabduljabbar, Raghad Alkhathran, Yousef Almushayqih, Raneem Alnajim, Salman Alsubaihi, Maryam Al Mansour, Majed Alrubaian, Ali Alammari, Zaki Alawami, Abdulmohsen Al-Thubaity, Ahmed Abdelali, Jeril Kuriakose, Abdalghani Abujabal, Nora Al-Twairesh, Areeb Alowisheq, Haidar Khan"
            }
        },
        {
            "start_date": {
                "year": 2024,
                "month": 5,
                "day": 21
            },
            "text": {
                "headline": "ALLaM 7B",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2407.15390",
                "caption": "ALLaM: Large Language Models for Arabic and English\n",
                "credit": "M Saiful Bari, Yazeed Alnumay, Norah A. Alzahrani, Nouf M. Alotaibi, Hisham A. Alyahya, Sultan AlRashed, Faisal A. Mirza, Shaykhah Z. Alsubaie, Hassan A. Alahmed, Ghadah Alabduljabbar, Raghad Alkhathran, Yousef Almushayqih, Raneem Alnajim, Salman Alsubaihi, Maryam Al Mansour, Majed Alrubaian, Ali Alammari, Zaki Alawami, Abdulmohsen Al-Thubaity, Ahmed Abdelali, Jeril Kuriakose, Abdalghani Abujabal, Nora Al-Twairesh, Areeb Alowisheq, Haidar Khan"
            }
        },
        {
            "start_date": {
                "year": 2024,
                "month": 5,
                "day": 20
            },
            "text": {
                "headline": "Octo-Base",
                "text": "<p>Large policies pretrained on diverse robot datasets have the potential to transform robotic learning: instead of training new policies from scratch, such generalist robot policies may be finetuned with only a little in-domain data, yet generalize broadly. However, to be widely applicable across a range of robotic learning scenarios, environments, and tasks, such policies need to handle diverse sensors and action spaces, accommodate a variety of commonly used robotic platforms, and finetune readily and efficiently to new domains. In this work, we aim to lay the groundwork for developing open-source, widely applicable, generalist policies for robotic manipulation. As a first step, we introduce Octo, a large transformer-based policy trained on 800k trajectories from the Open X-Embodiment dataset, the largest robot manipulation dataset to date. It can be instructed via language commands or goal images and can be effectively finetuned to robot setups with new sensory inputs and action spaces within a few hours on standard consumer GPUs. In experiments across 9 robotic platforms, we demonstrate that Octo serves as a versatile policy initialization that can be effectively finetuned to new observation and action spaces. We also perform detailed ablations of design decisions for the Octo model, from architecture to training data, to guide future research on building generalist robot models.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2405.12213 ",
                "caption": "Octo: An Open-Source Generalist Robot Policy",
                "credit": "Octo Model Team, Dibya Ghosh, Homer Walke, Karl Pertsch, Kevin Black, Oier Mees, Sudeep Dasari, Joey Hejna, Tobias Kreiman, Charles Xu, Jianlan Luo, You Liang Tan, Lawrence Yunliang Chen, Pannag Sanketi, Quan Vuong, Ted Xiao, Dorsa Sadigh, Chelsea Finn, Sergey Levine"
            }
        },
        {
            "start_date": {
                "year": 2024,
                "month": 5,
                "day": 20
            },
            "text": {
                "headline": "GLM-4 (0520)",
                "text": "<p>We introduce ChatGLM, an evolving family of large language models that we have been developing over time. This report primarily focuses on the GLM-4 language series, which includes GLM-4, GLM-4-Air, and GLM-4-9B. They represent our most capable models that are trained with all the insights and lessons gained from the preceding three generations of ChatGLM. To date, the GLM-4 models are pre-trained on ten trillions of tokens mostly in Chinese and English, along with a small set of corpus from 24 languages, and aligned primarily for Chinese and English usage. The high-quality alignment is achieved via a multi-stage post-training process, which involves supervised fine-tuning and learning from human feedback. Evaluations show that GLM-4 1) closely rivals or outperforms GPT-4 in terms of general metrics such as MMLU, GSM8K, MATH, BBH, GPQA, and HumanEval, 2) gets close to GPT-4-Turbo in instruction following as measured by IFEval, 3) matches GPT-4 Turbo (128K) and Claude 3 for long context tasks, and 4) outperforms GPT-4 in Chinese alignments as measured by AlignBench. The GLM-4 All Tools model is further aligned to understand user intent and autonomously decide when and which tool(s) touse -- including web browser, Python interpreter, text-to-image model, and user-defined functions -- to effectively complete complex tasks. In practical applications, it matches and even surpasses GPT-4 All Tools in tasks like accessing online information via web browsing and solving math problems using Python interpreter. Over the course, we have open-sourced a series of models, including ChatGLM-6B (three generations), GLM-4-9B (128K, 1M), GLM-4V-9B, WebGLM, and CodeGeeX, attracting over 10 million downloads on Hugging face in the year 2023 alone. The open models can be accessed through this https URL and this https URL.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2406.12793",
                "caption": "ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools",
                "credit": "Aohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, Da Yin, Diego Rojas, Guanyu Feng, Hanlin Zhao, Hanyu Lai, Hao Yu, Hongning Wang, Jiadai Sun, Jiajie Zhang, Jiale Cheng, Jiayi Gui, Jie Tang, Jing Zhang, Juanzi Li, Lei Zhao, Lindong Wu, Lucen Zhong, Mingdao Liu, Minlie Huang, Peng Zhang, Qinkai Zheng, Rui Lu, Shuaiqi Duan, Shudan Zhang, Shulin Cao, Shuxun Yang, Weng Lam Tam, Wenyi Zhao, Xiao Liu, Xiao Xia, Xiaohan Zhang, Xiaotao Gu, Xin Lv, Xinghan Liu, Xinyi Liu, Xinyue Yang, Xixuan Song, Xunkai Zhang, Yifan An, Yifan Xu, Yilin Niu, Yuantao Yang, Yueyan Li, Yushi Bai, Yuxiao Dong, Zehan Qi, Zhaoyu Wang, Zhen Yang, Zhengxiao Du, Zhenyu Hou, Zihan Wang"
            }
        },
        {
            "start_date": {
                "year": 2024,
                "month": 5,
                "day": 15
            },
            "text": {
                "headline": "VILA1.5-13B",
                "text": "<p>Visual language models (VLMs) rapidly progressed with\nthe recent success of large language models. There have\nbeen growing efforts on visual instruction tuning to extend\nthe LLM with visual inputs, but lacks an in-depth study of\nthe visual language pre-training process, where the model\nlearns to perform joint modeling on both modalities. In this\nwork, we examine the design options for VLM pre-training\nby augmenting LLM towards VLM through step-by-step controllable comparisons. We introduce three main findings:\n(1) freezing LLMs during pre-training can achieve decent\nzero-shot performance, but lack in-context learning capability, which requires unfreezing the LLM; (2) interleaved pretraining data is beneficial whereas image-text pairs alone\nare not optimal; (3) re-blending text-only instruction data\nto image-text data during instruction fine-tuning not only\nremedies the degradation of text-only tasks, but also boosts\nVLM task accuracy. With an enhanced pre-training recipe\nwe build VILA, a Visual Language model family that consistently outperforms the state-of-the-art models, e.g., LLaVA1.5, across main benchmarks without bells and whistles.\nMulti-modal pre-training also helps unveil appealing properties of VILA, including multi-image reasoning, enhanced\nin-context learning, and better world knowledge. VILA is\nalso deployable on Jetson Orin for on-device VLM.\n</p>"
            },
            "media": {
                "url": "https://arxiv.org/pdf/2312.07533",
                "caption": "VILA: On Pre-training for Visual Language Models",
                "credit": "Ji Lin, Hongxu Yin, Wei Ping, Yao Lu, Pavlo Molchanov, Andrew Tao, Huizi Mao, Jan Kautz, Mohammad Shoeybi, Song Han"
            }
        },
        {
            "start_date": {
                "year": 2024,
                "month": 5,
                "day": 13
            },
            "text": {
                "headline": "Yi-Large",
                "text": "<p></p>"
            },
            "media": {
                "url": "",
                "caption": "",
                "credit": "Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, Kaidong Yu, Peng Liu, Qiang Liu, Shawn Yue, Senbin Yang, Shiming Yang, Tao Yu, Wen Xie, Wenhao Huang, Xiaohui Hu, Xiaoyi Ren, Xinyao Niu, Pengcheng Nie, Yuchi Xu, Yudong Liu, Yue Wang, Yuxuan Cai, Zhenyu Gu, Zhiyuan Liu, Zonghong Dai"
            }
        },
        {
            "start_date": {
                "year": 2024,
                "month": 5,
                "day": 13
            },
            "text": {
                "headline": "GPT-4o",
                "text": "<p>We’re announcing GPT-4o, our new flagship model that can reason across audio, vision, and text in real time.\n\nGPT-4o (“o” for “omni”) is a step towards much more natural human-computer interaction—it accepts as input any combination of text, audio, image, and video and generates any combination of text, audio, and image outputs. It can respond to audio inputs in as little as 232 milliseconds, with an average of 320 milliseconds, which is similar to human response time(opens in a new window) in a conversation. It matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages, while also being much faster and 50% cheaper in the API. GPT-4o is especially better at vision and audio understanding compared to existing models.</p>"
            },
            "media": {
                "url": "https://openai.com/index/hello-gpt-4o/ \nhttps://openai.com/index/gpt-4o-system-card/",
                "caption": "Hello GPT-4o",
                "credit": "Aidan Clark, Alex Paino, Jacob Menick, Liam Fedus, Luke Metz, Clemens Winter, Lia Guy, Sam Schoenholz, Daniel Levy, Nitish Keskar, Alex Carney, Alex Paino, Ian Sohl, Qiming Yuan, Reimar Leike, Arka Dhar, Brydon Eastman, Mia Glaese, Ben Sokolowsky, Andrew Kondrich, Felipe Petroski Such, Henrique Ponde de Oliveira Pinto, Jiayi Weng, Randall Lin, Youlong Cheng, Nick Ryder, Lauren Itow, Barret Zoph, John Schulman, Mianna Chen, Adam Lerer, Adam P. Goucher, Adam Perelman, Akila Welihinda, Alec Radford, Alex Borzunov, Alex Carney, Alex Chow, Alex Paino, Alex Renzin, Alex Tachard Passos, Alexi Christakis, Ali Kamali, Allison Moyer, Allison Tam, Amin Tootoonchian, Ananya Kumar, Andrej Karpathy, Andrey Mishchenko, Andrew Cann, Andrew Kondrich, Andrew Tulloch, Angela Jiang, Antoine Pelisse, Anuj Gosalia, Avi Nayak, Avital Oliver, Behrooz Ghorbani, Ben Leimberger, Ben Wang, Blake Samic, Brian Guarraci, Brydon Eastman, Camillo Lugaresi, Chak Li, Charlotte Barette, Chelsea Voss, Chong Zhang, Chris Beaumont, Chris Hallacy, Chris Koch, Christian Gibson, Christopher Hesse, Colin Wei, Daniel Kappler, Daniel Levin, Daniel Levy, David Farhi, David Mely, David Sasaki, Dimitris Tsipras, Doug Li, Duc Phong Nguyen, Duncan Findlay, Edmund Wong, Ehsan Asdar, Elizabeth Proehl, Elizabeth Yang, Eric Peterson, Eric Sigler, Eugene Brevdo, Farzad Khorasani, Francis Zhang, Gene Oden, Geoff Salmon, Hadi Salman, Haiming Bao, Heather Schmidt, Hongyu Ren, Hyung Won Chung, Ian Kivlichan, Ian O'Connell, Ian Osband, Ilya Kostrikov, Ingmar Kanitscheider, Jacob Coxon, James Crooks, James Lennon, Jason Teplitz, Jason Wei, Jason Wolfe, Jay Chen, Jeff Harris, Jiayi Weng, Jie Tang, Joanne Jang, Jonathan Ward, Jonathan McKay, Jong Wook Kim, Josh Gross, Josh Kaplan, Joy Jiao, Joyce Lee, Juntang Zhuang, Kai Fricke, Kavin Karthik, Kenny Hsu, Kiel Howe, Kyle Luther, Larry Kai, Lauren Itow, Leo Chen, Lia Guy, Lien Mamitsuka, Lilian Weng, Long Ouyang, Louis Feuvrier, Lukas Kondraciuk, Lukasz Kaiser, Lyric Doshi, Mada Aflak, Maddie Simens, Madeleine Thompson, Marat Dukhan, Marvin Zhang, Mateusz Litwin, Max Johnson, Mayank Gupta, Mia Glaese, Michael Janner, Michael Petrov, Michael Wu, Michelle Fradin, Michelle Pokrass, Miguel Oom Temudo de Castro, Mikhail Pavlov, Minal Khan, Mo Bavarian, Natalia Gimelshein, Natalie Staudacher, Nick Stathas, Nik Tezak, Nithanth Kudige, Noel Bundick, Ofir Nachum, Oleg Boiko, Oleg Murk, Olivier Godement, Owen Campbell-Moore, Philip Pronin, Philippe Tillet, Rachel Lim, Rajan Troll, Randall Lin, Rapha gontijo lopes, Raul Puri, Reah Miyara, Reimar Leike, Renaud Gaubert, Reza Zamani, Rob Honsby, Rohit Ramchandani, Rory Carmichael, Ruslan Nigmatullin, Ryan Cheu, Scott Gray, Sean Grove, Sean Metzger, Shantanu Jain, Shengjia Zhao, Sherwin Wu, Shuaiqi (Tony) Xia, Sonia Phene, Spencer Papay, Steve Coffey, Steve Lee, Steve Lee, Stewart Hall, Suchir Balaji, Tal Broda, Tal Stramer, Tarun Gogineni, Ted Sanders, Thomas Cunninghman, Thomas Dimson, Thomas Raoux, Tianhao Zheng, Tina Kim, Todd Underwood, Tristan Heywood, Valerie Qi, Vinnie Monaco, Vlad Fomenko, Weiyi Zheng, Wenda Zhou, Wojciech Zaremba, Yash Patil, Yilei, Qian, Yongjik Kim, Youlong Cheng, Yuchen He, Yuchen Zhang, Yujia Jin, Yunxing Dai, Yury Malkov"
            }
        },
        {
            "start_date": {
                "year": 2024,
                "month": 4,
                "day": 18
            },
            "text": {
                "headline": "Llama 3-70B",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://ai.meta.com/blog/meta-llama-3/",
                "caption": "Introducing Meta Llama 3: The most capable openly available LLM to date",
                "credit": "Aaditya Singh; Aaron Grattafiori; Abhimanyu Dubey; Abhinav Jauhri; Abhinav Pandey; Abhishek Kadian; Adam Kelsey; Adi Gangidi; Ahmad Al-Dahle; Amit Sangani; Ahuva Goldstand; Aiesha Letman; Ajay Menon; Akhil Mathur; Alan Schelten; Alex Vaughan; Amy Yang; Andrei Lupu; Andres Alvarado; Andrew Gallagher; Andrew Gu; Andrew Ho; Andrew Poulton; Andrew Ryan; Angela Fan; Ankit Ramchandani; Anthony Hartshorn; Archi Mitra; Archie Sravankumar; Artem Korenev; Arun Rao; Ashley Gabriel; Ashwin Bharambe; Assaf Eisenman; Aston Zhang; Ash JJhaveri; Aurelien Rodriguez; Austen Gregerson; Ava Spataru; Baptiste Roziere; Ben Maurer; Benjamin Leonhardi; Bernie Huang; Bhargavi Paranjape; Bing Liu; Binh Tang; Bobbie Chern; Brani Stojkovic; Brian Fuller; Catalina Mejia Arenas; Chao Zhou; Charlotte Caucheteux; Chaya Nayak; Ching-Hsiang Chu; Chloe Bi; Chris Cai; Chris Cox; Chris Marra; Chris McConnell; Christian Keller; Christoph Feichtenhofer; Christophe Touret; Chunyang Wu; Corinne Wong; Cristian Canton Ferrer; Damien Allonsius; Daniel Kreymer; Daniel Haziza; Daniel Li; Danielle Pintz; Danny Livshits; Danny Wyatt; David Adkins; David Esiobu; David Xu; Davide Testuggine; Delia David; Devi Parikh; Dhruv Choudhary; Dhruv Mahajan; Diana Liskovich; Diego Garcia-Olano; Diego Perino; Dieuwke Hupkes; Dingkang Wang; Dustin Holland; Egor Lakomkin; Elina Lobanova; Xiaoqing Ellen Tan; Emily Dinan; Eric Smith; Erik Brinkman; Esteban Arcaute; Filip Radenovic; Firat Ozgenel; Francesco Caggioni; Frank Seide; Frank Zhang; Gabriel Synnaeve; Gabriella Schwarz; Gabrielle Lee; Gada Badeer; Georgia Anderson; Graeme Nail; Gregoire Mialon; Guan Pang; Guillem Cucurell; Hailey Nguyen; Hamid Shojanazeri; Hannah Korevaar; Hannah Wang; Haroun Habeeb; Harrison Rudolph; Henry Aspegren; Hu Xu; Hugo Touvron; Iga Kozlowska; Igor Molybog; Igor Tufanov; Iliyan Zarov; Imanol Arrieta Ibarra; Irina-Elena Veliche; Isabel Kloumann; Ishan Misra; Ivan Evtimov; Jacob Xu; Jade Copet; Jake Weissman; Jan Geffert; Jana Vranes; Japhet Asher; Jason Park; Jay Mahadeokar; Jean-Baptiste Gaya; Jeet Shah; Jelmer van der Linde; Jennifer Chan; Jenny Hong; Jenya Lee; Jeremy Fu; Jeremy Teboul; Jianfeng Chi; Jianyu Huang; Jie Wang; Jiecao Yu; Joanna Bitton; Joe Spisak; Joelle Pineau; Jon Carvill; Jongsoo Park; Joseph Rocca; Joshua Johnstun; Junteng Jia; Kalyan Vasuden Alwala; Kam Hou U; Kate Plawiak; Kartikeya Upasani; Kaushik Veeraraghavan; Ke Li; Kenneth Heafield; Kevin Stone; Khalid El-Arini; Krithika Iyer; Kshitiz Malik; Kuenley Chiu; Kunal Bhalla; Kyle Huang; Lakshya Garg; Lauren Rantala-Yeary; Laurens van der Maaten; Lawrence Chen; Leandro Silva; Lee Bell; Lei Zhang; Liang Tan; Louis Martin; Lovish Madaan; Luca Wehrstedt; Lukas Blecher; Luke de Oliveira; Madeline Muzzi; Madian Khabsa; Manav Avlani; Mannat Singh; Manohar Paluri; Mark Zuckerberg; Marcin Kardas; Martynas Mankus; Mathew Oldham; Mathieu Rita; Matthew Lennie; Maya Pavlova; Meghan Keneally; Melanie Kambadur; Mihir Patel; Mikayel Samvelyan; Mike Clark; Mike Lewis; Min Si; Mitesh Kumar Singh; Mo Metanat; Mona Hassan; Naman Goyal; Narjes Torabi; Nicolas Usunier; Nikolay Bashlykov; Nikolay Bogoychev; Niladri Chatterji; Ning Dong; Oliver Aobo Yang; Olivier Duchenne; Onur Celebi; Parth Parekh; Patrick Alrassy; Paul Saab; Pavan Balaji; Pedro Rittner; Pengchuan Zhang; Pengwei Li; Petar Vasic; Peter Weng; Polina Zvyagina; Prajjwal Bhargava; Pratik Dubal; Praveen Krishnan; Punit Singh Koura; Qing He; Rachel Rodriguez; Ragavan Srinivasan; Rahul Mitra; Ramon Calderer; Raymond Li; Robert Stojnic; Roberta Raileanu; Robin Battey; Rocky Wang; Rohit Girdhar; Rohit Patel; Romain Sauvestre; Ronnie Polidoro; Roshan Sumbaly; Ross Taylor; Ruan Silva; Rui Hou; Rui Wang; Russ Howes; Ruty Rinott; Saghar Hosseini; Sai Jayesh Bondu; Samyak Datta; Sanjay Singh; Sara Chugh; Sargun Dhillon; Satadru Pan; Sean Bell; Sergey Edunov; Shaoliang Nie; Sharan Narang; Sharath Raparthy; Shaun Lindsay; Sheng Feng; Sheng Shen; Shenghao Lin; Shiva Shankar; Shruti Bhosale; Shun Zhang; Simon Vandenhende; Sinong Wang; Seohyun Sonia Kim; Soumya Batra; Sten Sootla; Steve Kehoe; Suchin Gururangan; Sumit Gupta; Sunny Virk; Sydney Borodinsky; Tamar Glaser; Tamar Herman; Tamara Best; Tara Fowler; Thomas Georgiou; Thomas Scialom; Tianhe Li; Todor Mihaylov; Tong Xiao; Ujjwal Karn; Vedanuj Goswami; Vibhor Gupta; Vignesh Ramanathan; Viktor Kerkez; Vinay Satish Kumar; Vincent Gonguet; Vish Vogeti; Vlad Poenaru; Vlad Tiberiu Mihailescu; Vladan Petrovic; Vladimir Ivanov; Wei Li; Weiwei Chu; Wenhan Xiong; Wenyin Fu; Wes Bouaziz; Whitney Meers; Will Constable; Xavier Martinet; Xiaojian Wu; Xinbo Gao; Xinfeng Xie; Xuchao Jia; Yaelle Goldschlag; Yann LeCun; Yashesh Gaur; Yasmine Babaei; Ye Qi; Yenda Li; Yi Wen; Yiwen Song; Youngjin Nam; Yuchen Hao; Yuchen Zhang; Yun Wang; Yuning Mao; Yuzi He; Zacharie Delpierre Coudert; Zachary DeVito; Zahra Hankir; Zhaoduo Wen; Zheng Yan; Zhengxing Chen; Zhenyu Yang; Zoe Papakipos"
            }
        },
        {
            "start_date": {
                "year": 2024,
                "month": 4,
                "day": 15
            },
            "text": {
                "headline": "Reka Core",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://publications.reka.ai/reka-core-tech-report.pdf",
                "caption": "Reka Core, Flash, and Edge: A Series of Powerful\nMultimodal Language Models",
                "credit": "Aitor Ormazabal Che Zheng Cyprien de Masson d’Autume Dani Yogatama\nDeyu Fu Donovan Ong Eric Chen Eugenie Lamprecht Hai Pham Isaac Ong\nKaloyan Aleksiev Lei Li Matthew Henderson Max Bain Mikel Artetxe\nNishant Relan Piotr Padlewski Qi Liu Ren Chen Samuel Phua\nYazheng Yang Yi Tay Yuqi Wang Zhongkai Zhu Zhihui Xie"
            }
        },
        {
            "start_date": {
                "year": 2024,
                "month": 3,
                "day": 29
            },
            "text": {
                "headline": "ReALM",
                "text": "<p>Reference resolution is an important problem, one that is essential to understand and successfully handle context of different kinds. This context includes both previous turns and context that pertains to non-conversational entities, such as entities on the user's screen or those running in the background. While LLMs have been shown to be extremely powerful for a variety of tasks, their use in reference resolution, particularly for non-conversational entities, remains underutilized. This paper demonstrates how LLMs can be used to create an extremely effective system to resolve references of various types, by showing how reference resolution can be converted into a language modeling problem, despite involving forms of entities like those on screen that are not traditionally conducive to being reduced to a text-only modality. We demonstrate large improvements over an existing system with similar functionality across different types of references, with our smallest model obtaining absolute gains of over 5% for on-screen references. We also benchmark against GPT-3.5 and GPT-4, with our smallest model achieving performance comparable to that of GPT-4, and our larger models substantially outperforming it.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2403.20329",
                "caption": "ReALM: Reference Resolution As Language Modeling",
                "credit": "Joel Ruben Antony Moniz, Soundarya Krishnan, Melis Ozyildirim, Prathamesh Saraf, Halim Cagri Ates, Yuan Zhang, Hong Yu, Nidhi Rajshree"
            }
        },
        {
            "start_date": {
                "year": 2024,
                "month": 3,
                "day": 27
            },
            "text": {
                "headline": "DBRX",
                "text": "<p>Today, we are excited to introduce DBRX, an open, general-purpose LLM created by Databricks. Across a range of standard benchmarks, DBRX sets a new state-of-the-art for established open LLMs. Moreover, it provides the open community and enterprises building their own LLMs with capabilities that were previously limited to closed model APIs; according to our measurements, it surpasses GPT-3.5, and it is competitive with Gemini 1.0 Pro. It is an especially capable code model, surpassing specialized models like CodeLLaMA-70B on programming, in addition to its strength as a general-purpose LLM.</p>"
            },
            "media": {
                "url": "https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm",
                "caption": "Introducing DBRX: A New State-of-the-Art Open LLM",
                "credit": "Mosaic Research Team"
            }
        },
        {
            "start_date": {
                "year": 2024,
                "month": 3,
                "day": 14
            },
            "text": {
                "headline": "MM1-30B",
                "text": "<p>In this work, we discuss building performant Multimodal Large Language Models (MLLMs). In particular, we study the importance of various architecture components and data choices. Through careful and comprehensive ablations of the image encoder, the vision language connector, and various pre-training data choices, we identified several crucial design lessons. For example, we demonstrate that for large-scale multimodal pre-training using a careful mix of image-caption, interleaved image-text, and text-only data is crucial for achieving state-of-the-art (SOTA) few-shot results across multiple benchmarks, compared to other published pre-training results. Further, we show that the image encoder together with image resolution and the image token count has substantial impact, while the vision-language connector design is of comparatively negligible importance. By scaling up the presented recipe, we build MM1, a family of multimodal models up to 30B parameters, including both dense models and mixture-of-experts (MoE) variants, that are SOTA in pre-training metrics and achieve competitive performance after supervised fine-tuning on a range of established multimodal benchmarks. Thanks to large-scale pre-training, MM1 enjoys appealing properties such as enhanced in-context learning, and multi-image reasoning, enabling few-shot chain-of-thought prompting.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2403.09611",
                "caption": "MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training",
                "credit": "Brandon McKinzie, Zhe Gan, Jean-Philippe Fauconnier, Sam Dodge, Bowen Zhang, Philipp Dufter, Dhruti Shah, Xianzhi Du, Futang Peng, Floris Weers, Anton Belyi, Haotian Zhang, Karanjeet Singh, Doug Kang, Ankur Jain, Hongyu Hè, Max Schwarzer, Tom Gunter, Xiang Kong, Aonan Zhang, Jianyu Wang, Chong Wang, Nan Du, Tao Lei, Sam Wiseman, Guoli Yin, Mark Lee, Zirui Wang, Ruoming Pang, Peter Grasch, Alexander Toshev, Yinfei Yang"
            }
        },
        {
            "start_date": {
                "year": 2024,
                "month": 3,
                "day": 7
            },
            "text": {
                "headline": "Inflection-2.5",
                "text": "<p>At Inflection, our mission is to create a personal AI for everyone. Last May, we released Pi—a personal AI, designed to be empathetic, helpful, and safe. In November we announced a new major foundation model, Inflection-2, the second best LLM in the world at the time.\n\nNow we are adding IQ to Pi’s exceptional EQ.\n\nWe are launching Inflection-2.5, our upgraded in-house model that is competitive with all the world's leading LLMs like GPT-4 and Gemini. It couples raw capability with our signature personality and unique empathetic fine-tuning. Inflection-2.5 is available to all Pi's users today, at pi.ai, on iOS, on Android, or our new desktop app.</p>"
            },
            "media": {
                "url": "https://inflection.ai/inflection-2-5",
                "caption": "Inflection-2.5: meet the world's best personal AI",
                "credit": ""
            }
        },
        {
            "start_date": {
                "year": 2024,
                "month": 3,
                "day": 4
            },
            "text": {
                "headline": "Claude 3 Sonnet",
                "text": "<p>We introduce Claude 3, a new family of large multimodal models – Claude 3 Opus, our most capable offering, Claude 3 Sonnet, which provides a combination of skills and speed, and Claude 3 Haiku, our fastest and least expensive model. All new models have vision capabilities that enable them to process and analyze image data. The Claude 3 family demonstrates strong performance across benchmark evaluations and sets a new standard on\nmeasures of reasoning, math, and coding. Claude 3 Opus achieves state-of-the-art results on evaluations like GPQA [1], MMLU [2], MMMU [3] and many more. Claude 3 Haiku performs as well or better than Claude 2 [4] on most pure-text tasks, while Sonnet and Opus significantly outperform it. Additionally, these models exhibit improved fluency in non-English languages, making them more versatile for a global audience. In this report, we provide an in-depth analysis of our evaluations, focusing on core capabilities, safety, societal impacts, and the catastrophic risk assessments we committed to in our Responsible Scaling Policy [5].\n</p>"
            },
            "media": {
                "url": "https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf",
                "caption": "The Claude 3 Model Family: Opus, Sonnet, Haiku",
                "credit": ""
            }
        },
        {
            "start_date": {
                "year": 2024,
                "month": 3,
                "day": 4
            },
            "text": {
                "headline": "Claude 3 Opus",
                "text": "<p>We introduce Claude 3, a new family of large multimodal models – Claude 3 Opus, our most capable offering, Claude 3 Sonnet, which provides a combination of skills and speed, and Claude 3 Haiku, our fastest and least expensive model. All new models have vision capabilities that enable them to process and analyze image data. The Claude 3 family demonstrates strong performance across benchmark evaluations and sets a new standard on\nmeasures of reasoning, math, and coding. Claude 3 Opus achieves state-of-the-art results on evaluations like GPQA [1], MMLU [2], MMMU [3] and many more. Claude 3 Haiku performs as well or better than Claude 2 [4] on most pure-text tasks, while Sonnet and Opus significantly outperform it. Additionally, these models exhibit improved fluency in non-English languages, making them more versatile for a global audience. In this report, we provide an in-depth analysis of our evaluations, focusing on core capabilities, safety, societal impacts, and the catastrophic risk assessments we committed to in our Responsible Scaling Policy [5].\n</p>"
            },
            "media": {
                "url": "https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf",
                "caption": "The Claude 3 Model Family: Opus, Sonnet, Haiku",
                "credit": ""
            }
        },
        {
            "start_date": {
                "year": 2024,
                "month": 3,
                "day": 4
            },
            "text": {
                "headline": "Aramco Metabrain AI",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://www.offshore-technology.com/news/saudi-aramco-unveils-industry-first-generative-ai-model/",
                "caption": "Saudi Aramco unveils industry’s first generative AI model",
                "credit": "Saudi Aramco"
            }
        },
        {
            "start_date": {
                "year": 2024,
                "month": 2,
                "day": 26
            },
            "text": {
                "headline": "Mistral Large",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://mistral.ai/news/mistral-large/",
                "caption": "Mistral Large, our new flagship model",
                "credit": ""
            }
        },
        {
            "start_date": {
                "year": 2024,
                "month": 2,
                "day": 23
            },
            "text": {
                "headline": "MegaScale (Production)",
                "text": "<p>We present the design, implementation and engineering experience in building and deploying MegaScale, a production system for training large language models (LLMs) at the scale of more than 10,000 GPUs. Training LLMs at this scale brings unprecedented challenges to training efficiency and stability. We take a full-stack approach that co-designs the algorithmic and system components across model block and optimizer design, computation and communication overlapping, operator optimization, data pipeline, and network performance tuning. Maintaining high efficiency throughout the training process (i.e., stability) is an important consideration in production given the long extent of LLM training jobs. Many hard stability issues only emerge at large scale, and in-depth observability is the key to address them. We develop a set of diagnosis tools to monitor system components and events deep in the stack, identify root causes, and derive effective techniques to achieve fault tolerance and mitigate stragglers. MegaScale achieves 55.2% Model FLOPs Utilization (MFU) when training a 175B LLM model on 12,288 GPUs, improving the MFU by 1.34x compared to Megatron-LM. We share our operational experience in identifying and fixing failures and stragglers. We hope by articulating the problems and sharing our experience from a systems perspective, this work can inspire future LLM systems research.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2402.15627",
                "caption": "MegaScale: Scaling Large Language Model Training to More Than 10,000 GPUs",
                "credit": "Ziheng Jiang, Haibin Lin, Yinmin Zhong, Qi Huang, Yangrui Chen, Zhi Zhang, Yanghua Peng, Xiang Li, Cong Xie, Shibiao Nong, Yulu Jia, Sun He, Hongmin Chen, Zhihao Bai, Qi Hou, Shipeng Yan, Ding Zhou, Yiyao Sheng, Zhuo Jiang, Haohan Xu, Haoran Wei, Zhang Zhang, Pengfei Nie, Leqi Zou, Sida Zhao, Liang Xiang, Zherui Liu, Zhe Li, Xiaoying Jia, Jianxi Ye, Xin Jin, Xin Liu"
            }
        },
        {
            "start_date": {
                "year": 2024,
                "month": 2,
                "day": 15
            },
            "text": {
                "headline": "Sora Turbo",
                "text": "<p>Our video generation model is rolling out at sora.com⁠(opens in a new window).\n\nEarlier this year, we introduced Sora⁠, our model that can create realistic videos from text, and shared our initial research progress⁠ on world simulation. Sora serves as a foundation for AI that understands and simulates reality—an important step towards developing models that can interact with the physical world.\n\nWe developed a new version of Sora—Sora Turbo—that is significantly faster than the model we previewed in February. We’re releasing it today as a standalone product at Sora.com to ChatGPT Plus and Pro users.</p>"
            },
            "media": {
                "url": "https://openai.com/index/sora-is-here/",
                "caption": "Sora is here",
                "credit": ""
            }
        },
        {
            "start_date": {
                "year": 2024,
                "month": 2,
                "day": 15
            },
            "text": {
                "headline": "Sora",
                "text": "<p>Sora is OpenAI’s video generation model, designed to take text, image, and video inputs and generate a new video as an output. Users can create videos up to 1080p resolution (20 seconds max) in various formats, generate new content from text, or enhance, remix, and blend their own assets. Users will be able to explore the Featured and Recent feeds which showcase community creations and offer inspiration for new ideas. Sora builds on learnings from DALL·E and GPT models, and is designed to give people expanded tools for storytelling and creative expression. \n\nSora is a diffusion model, which generates a video by starting off with a base video that looks like static noise and gradually transforms it by removing the noise over many steps. By giving the model foresight of many frames at a time, we’ve solved a challenging problem of making sure a subject stays the same even when it goes out of view temporarily. Similar to GPT models, Sora uses a transformer architecture, unlocking superior scaling performance. </p>"
            },
            "media": {
                "url": "https://openai.com/index/video-generation-models-as-world-simulators/",
                "caption": "Video generation models as world simulators",
                "credit": ""
            }
        },
        {
            "start_date": {
                "year": 2024,
                "month": 2,
                "day": 15
            },
            "text": {
                "headline": "Gemini 1.5 Pro",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf",
                "caption": "Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context",
                "credit": "Gemini Team"
            }
        },
        {
            "start_date": {
                "year": 2024,
                "month": 2,
                "day": 12
            },
            "text": {
                "headline": "Aya",
                "text": "<p>Recent breakthroughs in large language models (LLMs) have centered around a handful of data-rich languages. What does it take to broaden access to breakthroughs beyond first-class citizen languages? Our work introduces Aya, a massively multilingual generative language model that follows instructions in 101 languages of which over 50% are considered as lower-resourced. Aya outperforms mT0 and BLOOMZ on the majority of tasks while covering double the number of languages. We introduce extensive new evaluation suites that broaden the state-of-art for multilingual eval across 99 languages -- including discriminative and generative tasks, human evaluation, and simulated win rates that cover both held-out tasks and in-distribution performance. Furthermore, we conduct detailed investigations on the optimal finetuning mixture composition, data pruning, as well as the toxicity, bias, and safety of our models. We open-source our instruction datasets and our model at this https://huggingface.co/CohereForAI/aya-101</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2402.07827",
                "caption": "Aya Model: An Instruction Finetuned Open-Access Multilingual Language Model",
                "credit": "Ahmet Üstün, Viraat Aryabumi, Zheng-Xin Yong, Wei-Yin Ko, Daniel D'souza, Gbemileke Onilude, Neel Bhandari, Shivalika Singh, Hui-Lee Ooi, Amr Kayid, Freddie Vargus, Phil Blunsom, Shayne Longpre, Niklas Muennighoff, Marzieh Fadaee, Julia Kreutzer, Sara Hooker"
            }
        },
        {
            "start_date": {
                "year": 2024,
                "month": 2,
                "day": 4
            },
            "text": {
                "headline": "Qwen1.5-72B",
                "text": "<p>In recent months, our focus has been on developing a “good” model while optimizing the developer experience. As we progress towards Qwen1.5, the next iteration in our Qwen series, this update arrives just before the Chinese New Year. With Qwen1.5, we are open-sourcing base and chat models across six sizes: 0.5B, 1.8B, 4B, 7B, 14B, and 72B. In line with tradition, we’re also providing quantized models, including Int4 and Int8 GPTQ models, as well as AWQ and GGUF quantized models. To enhance the developer experience, we’ve merged Qwen1.5’s code into Hugging Face transformers, making it accessible with transformers>=4.37.0 without needing trust_remote_code.</p>"
            },
            "media": {
                "url": "https://qwenlm.github.io/blog/qwen1.5/",
                "caption": "Introducing Qwen1.5",
                "credit": "Qwen Team"
            }
        },
        {
            "start_date": {
                "year": 2024,
                "month": 1,
                "day": 25
            },
            "text": {
                "headline": "Qwen-VL-Max",
                "text": "<p>Along with the rapid development of our large language model Qwen, we leveraged Qwen’s capabilities and unified multimodal pretraining to address the limitations of multimodal models in generalization, and we opensourced multimodal model Qwen-VL in Sep. 2023. Recently, the Qwen-VL series has undergone a significant upgrade with the launch of two enhanced versions, Qwen-VL-Plus and Qwen-VL-Max. The key technical advancements in these versions include:\n\nSubstantially boost in image-related reasoning capabilities;\nConsiderable enhancement in recognizing, extracting, and analyzing details within images and texts contained therein;\nSupport for high-definition images with resolutions above one million pixels and images of various aspect ratios.</p>"
            },
            "media": {
                "url": "https://qwenlm.github.io/blog/qwen-vl/",
                "caption": "Introducing Qwen-VL",
                "credit": ""
            }
        },
        {
            "start_date": {
                "year": 2024,
                "month": 1,
                "day": 17
            },
            "text": {
                "headline": "AlphaGeometry",
                "text": "<p>Proving mathematical theorems at the olympiad level represents a notable milestone in human-level automated reasoning1,2,3,4, owing to their reputed difficulty among the world’s best talents in pre-university mathematics. Current machine-learning approaches, however, are not applicable to most mathematical domains owing to the high cost of translating human proofs into machine-verifiable format. The problem is even worse for geometry because of its unique translation challenges1,5, resulting in severe scarcity of training data. We propose AlphaGeometry, a theorem prover for Euclidean plane geometry that sidesteps the need for human demonstrations by synthesizing millions of theorems and proofs across different levels of complexity. AlphaGeometry is a neuro-symbolic system that uses a neural language model, trained from scratch on our large-scale synthetic data, to guide a symbolic deduction engine through infinite branching points in challenging problems. On a test set of 30 latest olympiad-level problems, AlphaGeometry solves 25, outperforming the previous best method that only solves ten problems and approaching the performance of an average International Mathematical Olympiad (IMO) gold medallist. Notably, AlphaGeometry produces human-readable proofs, solves all geometry problems in the IMO 2000 and 2015 under human expert evaluation and discovers a generalized version of a translated IMO theorem in 2004.</p>"
            },
            "media": {
                "url": "https://www.nature.com/articles/s41586-023-06747-5",
                "caption": "Solving olympiad geometry without human demonstrations",
                "credit": "Trieu H. Trinh, Yuhuai Wu, Quoc V. Le, He He, Thang Luong"
            }
        },
        {
            "start_date": {
                "year": 2024,
                "month": 1,
                "day": 1
            },
            "text": {
                "headline": "Palmyra X 003",
                "text": "<p>Palmyra X 003, is a top-performing instruct model, built specifically for structured text completion rather than conversational use. Available in AI Studio no-code apps, the Writer Framework, and via API, it delivers precise, contextually accurate responses to instructions across diverse workflows. Ranked #3 on HELM at release, Palmyra X 003 remains a powerful option for enterprises needing reliable, instruct-focused AI.</p>"
            },
            "media": {
                "url": "https://writer.com/llms/palmyra-x-003/",
                "caption": "Palmyra X 003 Instruct",
                "credit": "Writer"
            }
        },
        {
            "start_date": {
                "year": 2024,
                "month": 1,
                "day": 1
            },
            "text": {
                "headline": "Kimi Explorer",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://www.53ai.com/news/LargeLanguageModel/2024101137012.html",
                "caption": "",
                "credit": ""
            }
        },
        {
            "start_date": {
                "year": 2023,
                "month": 12,
                "day": 29
            },
            "text": {
                "headline": "CoRe",
                "text": "<p>Large-scale pre-trained language models (PLMs) bring new opportunities to challenging problems, especially those that need high-level intelligence, such as the math word problem (MWPs). However, directly applying existing PLMs to MWPs can fail as the generation process lacks sufficient supervision and thus lacks fast adaptivity as humans. We notice that human reasoning has a dual reasoning framework that consists of an immediate reaction system (system 1) and a delicate reasoning system (system 2), where the entire reasoning is determined by their interaction. This inspires us to develop a cooperative reasoning-induced PLM for solving MWPs, called Cooperative Reasoning (CoRe), resulting in a human-like reasoning architecture with system 1 as the generator and system 2 as the verifier. In our approach, the generator is responsible for generating reasoning paths, and the verifiers are used to supervise the evaluation in order to obtain reliable feedback for the generator. We evaluate our CoRe framework on several mathematical reasoning datasets and achieve decent improvement over state-of-the-art methods, up to 9.6% increase over best baselines.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2210.16257",
                "caption": "Solving Math Word Problems via Cooperative Reasoning induced Language Models",
                "credit": "Xinyu Zhu, Junjie Wang, Lin Zhang, Yuxiang Zhang, Ruyi Gan, Jiaxing Zhang, Yujiu Yang"
            }
        },
        {
            "start_date": {
                "year": 2023,
                "month": 12,
                "day": 21
            },
            "text": {
                "headline": "nekomata-14b",
                "text": "<p>We conduct continual pre-training of qwen-14b on 66B tokens from a mixture of Japanese and English datasets. The continual pre-training significantly improves the model's performance on Japanese tasks. It also enjoys the following great features provided by the original Qwen model.\n\n* The inclusive Qwen vocabulary (vocab size > 150k) enables the model to processs Japanese texts much more efficiently than the previously released youri series.\n* The model supports a maximum sequence length of 8192.\n\nThe name nekomata comes from the Japanese word 猫又/ねこまた/Nekomata, which is a kind of Japanese mythical creature (妖怪/ようかい/Youkai).</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2404.01657",
                "caption": "rinna/nekomata-14b",
                "credit": "Tianyu Zhao, Akio Kaga, Kei Sawada"
            }
        },
        {
            "start_date": {
                "year": 2023,
                "month": 12,
                "day": 19
            },
            "text": {
                "headline": "Gemini Nano-2",
                "text": "<p>This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks - notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2312.11805",
                "caption": "Gemini: A Family of Highly Capable Multimodal Models",
                "credit": "Gemini Team"
            }
        },
        {
            "start_date": {
                "year": 2023,
                "month": 12,
                "day": 19
            },
            "text": {
                "headline": "Gemini Nano-1",
                "text": "<p>This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks - notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2312.11805",
                "caption": "Gemini: A Family of Highly Capable Multimodal Models",
                "credit": "Gemini Team"
            }
        },
        {
            "start_date": {
                "year": 2023,
                "month": 12,
                "day": 14
            },
            "text": {
                "headline": "FunSearch",
                "text": "<p>Large language models (LLMs) have demonstrated tremendous capabilities in solving complex tasks, from quantitative reasoning to understanding natural language. However, LLMs sometimes suffer from confabulations (or hallucinations), which can result in them making plausible but incorrect statements1,2. This hinders the use of current large models in scientific discovery. Here we introduce FunSearch (short for searching in the function space), an evolutionary procedure based on pairing a pretrained LLM with a systematic evaluator. We demonstrate the effectiveness of this approach to surpass the best-known results in important problems, pushing the boundary of existing LLM-based approaches3. Applying FunSearch to a central problem in extremal combinatorics—the cap set problem—we discover new constructions of large cap sets going beyond the best-known ones, both in finite dimensional and asymptotic cases. This shows that it is possible to make discoveries for established open problems using LLMs. We showcase the generality of FunSearch by applying it to an algorithmic problem, online bin packing, finding new heuristics that improve on widely used baselines. In contrast to most computer search approaches, FunSearch searches for programs that describe how to solve a problem, rather than what the solution is. Beyond being an effective and scalable strategy, discovered programs tend to be more interpretable than raw solutions, enabling feedback loops between domain experts and FunSearch, and the deployment of such programs in real-world applications.</p>"
            },
            "media": {
                "url": "https://www.nature.com/articles/s41586-023-06924-6\nhttps://deepmind.google/discover/blog/funsearch-making-new-discoveries-in-mathematical-sciences-using-large-language-models/",
                "caption": "Mathematical discoveries from program search with large language models",
                "credit": "Bernardino Romera-Paredes, Mohammadamin Barekatain, Alexander Novikov, Matej Balog, M. Pawan Kumar, Emilien Dupont, Francisco J. R. Ruiz, Jordan S. Ellenberg, Pengming Wang, Omar Fawzi, Pushmeet Kohli, Alhussein Fawzi "
            }
        },
        {
            "start_date": {
                "year": 2023,
                "month": 12,
                "day": 14
            },
            "text": {
                "headline": "CogAgent",
                "text": "<p>People are spending an enormous amount of time on digital devices through graphical user interfaces (GUIs), e.g., computer or smartphone screens. Large language models (LLMs) such as ChatGPT can assist people in tasks like writing emails, but struggle to understand and interact with GUIs, thus limiting their potential to increase automation levels. In this paper, we introduce CogAgent, an 18-billion-parameter visual language model (VLM) specializing in GUI understanding and navigation. By utilizing both low-resolution and high-resolution image encoders, CogAgent supports input at a resolution of 1120*1120, enabling it to recognize tiny page elements and text. As a generalist visual language model, CogAgent achieves the state of the art on five text-rich and four general VQA benchmarks, including VQAv2, OK-VQA, Text-VQA, ST-VQA, ChartQA, infoVQA, DocVQA, MM-Vet, and POPE. CogAgent, using only screenshots as input, outperforms LLM-based methods that consume extracted HTML text on both PC and Android GUI navigation tasks -- Mind2Web and AITW, advancing the state of the art. The model and codes are available at this https URL .</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2312.08914",
                "caption": "CogAgent: A Visual Language Model for GUI Agents",
                "credit": "Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxuan Zhang, Juanzi Li, Bin Xu, Yuxiao Dong, Ming Ding, Jie Tang"
            }
        },
        {
            "start_date": {
                "year": 2023,
                "month": 12,
                "day": 11
            },
            "text": {
                "headline": "Mixtral 8x7B",
                "text": "<p>Today, the team is proud to release Mixtral 8x7B, a high-quality sparse mixture of experts model (SMoE) with open weights. Licensed under Apache 2.0. Mixtral outperforms Llama 2 70B on most benchmarks with 6x faster inference. It is the strongest open-weight model with a permissive license and the best model overall regarding cost/performance trade-offs. In particular, it matches or outperforms GPT3.5 on most standard benchmarks.</p>"
            },
            "media": {
                "url": "https://mistral.ai/news/mixtral-of-experts/, https://arxiv.org/abs/2401.04088",
                "caption": "Mixtral of experts: A high quality Sparse Mixture-of-Experts.",
                "credit": "Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Lélio Renard Lavaud, Louis Ternon, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Théophile Gervet, Thibaut Lavril, Thomas Wang, Timothée Lacroix, William El Sayed."
            }
        },
        {
            "start_date": {
                "year": 2023,
                "month": 12,
                "day": 8
            },
            "text": {
                "headline": "SeamlessM4T",
                "text": "<p>Large-scale automatic speech translation systems today lack key features that help machine-mediated communication feel seamless when compared to human-to-human dialogue. In this work, we introduce a family of models that enable end-to-end expressive and multilingual translations in a streaming fashion. First, we contribute an improved version of the massively multilingual and multimodal SeamlessM4T model-SeamlessM4T v2. This newer model, incorporating an updated UnitY2 framework, was trained on more low-resource language data. SeamlessM4T v2 provides the foundation on which our next two models are initiated. SeamlessExpressive enables translation that preserves vocal styles and prosody. Compared to previous efforts in expressive speech research, our work addresses certain underexplored aspects of prosody, such as speech rate and pauses, while also preserving the style of one's voice. As for SeamlessStreaming, our model leverages the Efficient Monotonic Multihead Attention mechanism to generate low-latency target translations without waiting for complete source utterances. As the first of its kind, SeamlessStreaming enables simultaneous speech-to-speech/text translation for multiple source and target languages. To ensure that our models can be used safely and responsibly, we implemented the first known red-teaming effort for multimodal machine translation, a system for the detection and mitigation of added toxicity, a systematic evaluation of gender bias, and an inaudible localized watermarking mechanism designed to dampen the impact of deepfakes. Consequently, we bring major components from SeamlessExpressive and SeamlessStreaming together to form Seamless, the first publicly available system that unlocks expressive cross-lingual communication in real-time. The contributions to this work are publicly released and accessible at this https URL</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2312.05187, https://huggingface.co/facebook/seamless-m4t-v2-large",
                "caption": "Seamless: Multilingual Expressive and Streaming Speech Translation",
                "credit": "Loïc Barrault, Yu-An Chung, Mariano Coria Meglioli, David Dale, Ning Dong, Mark Duppenthaler, Paul-Ambroise Duquenne, Brian Ellis, Hady Elsahar, Justin Haaheim, John Hoffman, Min-Jae Hwang, Hirofumi Inaguma, Christopher Klaiber, Ilia Kulikov, Pengwei Li, Daniel Licht, Jean Maillard, Ruslan Mavlyutov, Alice Rakotoarison, Kaushik Ram Sadagopan, Abinesh Ramakrishnan, Tuan Tran, Guillaume Wenzek, Yilin Yang, Ethan Ye, Ivan Evtimov, Pierre Fernandez, Cynthia Gao, Prangthip Hansanti, Elahe Kalbassi, Amanda Kallet, Artyom Kozhevnikov, Gabriel Mejia Gonzalez, Robin San Roman, Christophe Touret, Corinne Wong, Carleigh Wood, Bokai Yu, Pierre Andrews, Can Balioglu, Peng-Jen Chen, Marta R. Costa-jussà, Maha Elbayad, Hongyu Gong, Francisco Guzmán, Kevin Heffernan, Somya Jain, Justine Kao, Ann Lee, Xutai Ma, Alex Mourachko, Benjamin Peloquin, Juan Pino, Sravya Popuri, Christophe Ropers, Safiyyah Saleem, Holger Schwenk, Anna Sun, Paden Tomasello, Changhan Wang, Jeff Wang, Skyler Wang, Mary Williamson"
            }
        },
        {
            "start_date": {
                "year": 2023,
                "month": 12,
                "day": 7
            },
            "text": {
                "headline": "Llama Guard",
                "text": "<p>We introduce Llama Guard, an LLM-based input-output safeguard model geared towards Human-AI conversation use cases. Our model incorporates a safety risk taxonomy, a valuable tool for categorizing a specific set of safety risks found in LLM prompts (i.e., prompt classification). This taxonomy is also instrumental in classifying the responses generated by LLMs to these prompts, a process we refer to as response classification. For the purpose of both prompt and response classification, we have meticulously gathered a dataset of high quality. Llama Guard, a Llama2-7b model that is instruction-tuned on our collected dataset, albeit low in volume, demonstrates strong performance on existing benchmarks such as the OpenAI Moderation Evaluation dataset and ToxicChat, where its performance matches or exceeds that of currently available content moderation tools. Llama Guard functions as a language model, carrying out multi-class classification and generating binary decision scores. Furthermore, the instruction fine-tuning of Llama Guard allows for the customization of tasks and the adaptation of output formats. This feature enhances the model's capabilities, such as enabling the adjustment of taxonomy categories to align with specific use cases, and facilitating zero-shot or few-shot prompting with diverse taxonomies at the input. We are making Llama Guard model weights available and we encourage researchers to further develop and adapt them to meet the evolving needs of the community for AI safety.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2312.06674",
                "caption": "Llama Guard: LLM-based Input-Output Safeguard for Human-AI Conversations",
                "credit": "Hakan Inan, Kartikeya Upasani, Jianfeng Chi, Rashi Rungta, Krithika Iyer, Yuning Mao, Davide Testuggine, Madian Khabsa"
            }
        },
        {
            "start_date": {
                "year": 2023,
                "month": 12,
                "day": 6
            },
            "text": {
                "headline": "Gemini 1.0 Ultra",
                "text": "<p>This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.\n</p>"
            },
            "media": {
                "url": "https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf",
                "caption": "Gemini: A Family of Highly Capable Multimodal Models",
                "credit": "Gemini Team"
            }
        },
        {
            "start_date": {
                "year": 2023,
                "month": 12,
                "day": 6
            },
            "text": {
                "headline": "Gemini 1.0 Pro",
                "text": "<p>This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.\n</p>"
            },
            "media": {
                "url": "https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf",
                "caption": "Gemini: A Family of Highly Capable Multimodal Models",
                "credit": "Gemini Team"
            }
        },
        {
            "start_date": {
                "year": 2023,
                "month": 12,
                "day": 1
            },
            "text": {
                "headline": "Mamba-24M (SC09)",
                "text": "<p>Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5× higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2312.00752",
                "caption": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces",
                "credit": "Albert Gu, Tri Dao"
            }
        },
        {
            "start_date": {
                "year": 2023,
                "month": 11,
                "day": 30
            },
            "text": {
                "headline": "Qwen-72B",
                "text": "<p>Qwen-72B is the 72B-parameter version of the large language model series, Qwen (abbr. Tongyi Qianwen), proposed by Alibaba Cloud. Qwen-72B is a Transformer-based large language model, which is pretrained on a large volume of data, including web texts, books, codes, etc. Additionally, based on the pretrained Qwen-72B, we release Qwen-72B-Chat, a large-model-based AI assistant, which is trained with alignment techniques.</p>"
            },
            "media": {
                "url": "https://huggingface.co/Qwen/Qwen-72B",
                "caption": "",
                "credit": "Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, Tianhang Zhu"
            }
        },
        {
            "start_date": {
                "year": 2023,
                "month": 11,
                "day": 29
            },
            "text": {
                "headline": "PPLX-70B-Online",
                "text": "<p>We’re excited to share two new PPLX models: pplx-7b-online and pplx-70b-online! Our online models are focused on delivering helpful, up-to-date, and factual responses, and are publicly available via pplx-api, making it a first-of-its-kind API. pplx-7b-online and pplx-70b-online are also accessible via Perplexity Labs, our LLM playground.</p>"
            },
            "media": {
                "url": "https://blog.perplexity.ai/blog/introducing-pplx-online-llms",
                "caption": "Introducing PPLX Online LLMs ",
                "credit": "Lauren Yang, Kevin Hu, Aarash Heydari, Gradey Wang, Dmitry Pervukhin, Nikhil Thota, Alexandr Yarats, Max Morozov, Denis Yarats"
            }
        },
        {
            "start_date": {
                "year": 2023,
                "month": 11,
                "day": 29
            },
            "text": {
                "headline": "GNoME for crystal discovery",
                "text": "<p>Novel functional materials enable fundamental breakthroughs across technological applications from clean energy to information processing. From microchips to batteries and photovoltaics, discovery of inorganic crystals has been bottlenecked by expensive trial-and-error approaches. Concurrently, deep-learning models for language, vision and biology have showcased emergent predictive capabilities with increasing data and computation. Here we show that graph networks trained at scale can reach unprecedented levels of generalization, improving the efficiency of materials discovery by an order of magnitude. Building on 48,000 stable crystals identified in continuing studies15,16,17, improved efficiency enables the discovery of 2.2 million structures below the current convex hull, many of which escaped previous human chemical intuition. Our work represents an order-of-magnitude expansion in stable materials known to humanity. Stable discoveries that are on the final convex hull will be made available to screen for technological applications, as we demonstrate for layered materials and solid-electrolyte candidates. Of the stable structures, 736 have already been independently experimentally realized. The scale and diversity of hundreds of millions of first-principles calculations also unlock modelling capabilities for downstream applications, leading in particular to highly accurate and robust learned interatomic potentials that can be used in condensed-phase molecular-dynamics simulations and high-fidelity zero-shot prediction of ionic conductivity.</p>"
            },
            "media": {
                "url": "https://www.nature.com/articles/s41586-023-06735-9",
                "caption": "Scaling deep learning for materials discovery",
                "credit": "Amil Merchant, Simon Batzner, Samuel S. Schoenholz, Muratahan Aykol, Gowoon Cheon, Ekin Dogus Cubuk"
            }
        },
        {
            "start_date": {
                "year": 2023,
                "month": 11,
                "day": 22
            },
            "text": {
                "headline": "Inflection-2",
                "text": "<p>Today we are proud to announce that we have completed training of Inflection-2, the best model in the world for its compute class and the second most capable LLM in the world today. Our mission at Inflection is to create a personal AI for everyone. Just a few months ago, we announced Inflection-1 — a best-in-class language model that currently powers Pi. Our new model, Inflection-2, is substantially more capable than Inflection-1, demonstrating much improved factual knowledge, better stylistic control, and dramatically improved reasoning.</p>"
            },
            "media": {
                "url": "https://inflection.ai/inflection-2",
                "caption": "Inflection-2: The Next Step Up",
                "credit": ""
            }
        },
        {
            "start_date": {
                "year": 2023,
                "month": 11,
                "day": 21
            },
            "text": {
                "headline": "Claude 2.1",
                "text": "<p>Our latest model, Claude 2.1, is now available over API in our Console and is powering our claude.ai chat experience. Claude 2.1 delivers advancements in key capabilities for enterprises—including an industry-leading 200K token context window, significant reductions in rates of model hallucination, system prompts and our new beta feature: tool use.</p>"
            },
            "media": {
                "url": "https://www.anthropic.com/index/claude-2-1",
                "caption": "Introducing Claude 2.1",
                "credit": ""
            }
        },
        {
            "start_date": {
                "year": 2023,
                "month": 11,
                "day": 16
            },
            "text": {
                "headline": "AndesGPT",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://www.oppo.com/en/newsroom/press/2023-oppo-developers-conference-odc23/",
                "caption": "",
                "credit": ""
            }
        },
        {
            "start_date": {
                "year": 2023,
                "month": 11,
                "day": 15
            },
            "text": {
                "headline": "Nemotron-3-8B",
                "text": "<p>Large language models (LLMs) are revolutionizing data science, enabling advanced capabilities in natural language understanding, AI, and machine learning. Custom LLMs, tailored for domain-specific insights, are finding increased traction in enterprise applications.\n\nThe NVIDIA Nemotron-3 8B family of foundation models is a powerful new tool for building production-ready generative AI applications for the enterprise–fostering innovations ranging from customer service AI chatbots to cutting-edge AI products.</p>"
            },
            "media": {
                "url": "https://developer.nvidia.com/blog/nvidia-ai-foundation-models-build-custom-enterprise-chatbots-and-co-pilots-with-production-ready-llms/\n\nhttps://catalog.ngc.nvidia.com/orgs/nvidia/teams/nemo/models/nemotron-3-8b-base-4k",
                "caption": "NVIDIA AI Foundation Models: Build Custom Enterprise Chatbots and Co-Pilots with Production-Ready LLMs",
                "credit": ""
            }
        },
        {
            "start_date": {
                "year": 2023,
                "month": 11,
                "day": 14
            },
            "text": {
                "headline": "Qwen-Audio-Chat",
                "text": "<p> Recently, instruction-following audio-language models have received broad attention for audio interaction with humans. However, the absence of pre-trained audio models capable of handling diverse audio types and tasks has hindered progress in this field. Consequently, most existing works have only been able to support a limited range of interaction capabilities. In this paper, we develop the Qwen-Audio model and address this limitation by scaling up audio-language pre-training to cover over 30 tasks and various audio types, such as human speech, natural sounds, music, and songs, to facilitate universal audio understanding abilities. However, directly co-training all tasks and datasets can lead to interference issues, as the textual labels associated with different datasets exhibit considerable variations due to differences in task focus, language, granularity of annotation, and text structure. To overcome the one-to-many interference, we carefully design a multi-task training framework by conditioning on a sequence of hierarchical tags to the decoder for encouraging knowledge sharing and avoiding interference through shared and specified tags respectively. Remarkably, Qwen-Audio achieves impressive performance across diverse benchmark tasks without requiring any task-specific fine-tuning, surpassing its counterparts. Building upon the capabilities of Qwen-Audio, we further develop Qwen-Audio-Chat, which allows for input from various audios and text inputs, enabling multi-turn dialogues and supporting various audio-central scenarios. </p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2311.07919",
                "caption": "Qwen-Audio: Advancing Universal Audio Understanding via Unified Large-Scale Audio-Language Models",
                "credit": "Yunfei Chu, Jin Xu, Xiaohuan Zhou, Qian Yang, Shiliang Zhang, Zhijie Yan, Chang Zhou, Jingren Zhou"
            }
        },
        {
            "start_date": {
                "year": 2023,
                "month": 11,
                "day": 14
            },
            "text": {
                "headline": "GraphCast",
                "text": "<p>Global medium-range weather forecasting is critical to decision-making across many social and economic domains. Traditional numerical weather prediction uses increased compute resources to improve forecast accuracy but does not directly use historical weather data to improve the underlying model. Here, we introduce GraphCast, a machine learning–based method trained directly from reanalysis data. It predicts hundreds of weather variables for the next 10 days at 0.25° resolution globally in under 1 minute. GraphCast significantly outperforms the most accurate operational deterministic systems on 90% of 1380 verification targets, and its forecasts support better severe event prediction, including tropical cyclone tracking, atmospheric rivers, and extreme temperatures. GraphCast is a key advance in accurate and efficient weather forecasting and helps realize the promise of machine learning for modeling complex dynamical systems.</p>"
            },
            "media": {
                "url": "https://www.science.org/doi/epdf/10.1126/science.adi2336",
                "caption": "Learning skillful medium-range globalweather forecasting",
                "credit": "Remi Lam, Alvaro Sanchez-Gonzalez, Matthew Willson, Peter Wirnsberger, Meire Fortunato, Ferran Alet, Suman Ravuri, Timo Ewalds, Zach Eaton-Rosen, Weihua Hu, Alexander Merose, Stephan Hoyer, George Holland, Oriol Vinyals, Jacklynn Stott, Alexander Pritzel, Shakir Mohamed, Peter Battaglia"
            }
        },
        {
            "start_date": {
                "year": 2023,
                "month": 11,
                "day": 13
            },
            "text": {
                "headline": "Volcano 13B",
                "text": "<p>Large multimodal models (LMMs) suffer from multimodal hallucination, where they provide incorrect responses misaligned with the given visual information. Recent works have conjectured that one of the reasons behind multimodal hallucination might be due to the vision encoder failing to ground on the image properly. To mitigate this issue, we propose a novel approach that leverages self-feedback as visual cues. Building on this approach, we introduce Volcano, a multimodal self-feedback guided revision model. Volcano generates natural language feedback to its initial response based on the provided visual information and utilizes this feedback to self-revise its initial response. Volcano effectively reduces multimodal hallucination and achieves state-of-the-art on MMHal-Bench, POPE, and GAVIE. It also improves on general multimodal abilities and outperforms previous models on MM-Vet and MMBench. Through a qualitative analysis, we show that Volcano's feedback is properly grounded on the image than the initial response. This indicates that Volcano can provide itself with richer visual information, helping alleviate multimodal hallucination. We publicly release Volcano models of 7B and 13B sizes along with the data and code at this https URL.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2311.07362",
                "caption": "Volcano: Mitigating Multimodal Hallucination through Self-Feedback Guided Revision",
                "credit": "Seongyun Lee, Sue Hyun Park, Yongrae Jo, Minjoon Seo"
            }
        },
        {
            "start_date": {
                "year": 2023,
                "month": 11,
                "day": 13
            },
            "text": {
                "headline": "SPHINX (Llama 2 13B)",
                "text": "<p>We present SPHINX, a versatile multi-modal large language model (MLLM) with a joint mixing of model weights, tuning tasks, and visual embeddings. First, for stronger vision-language alignment, we unfreeze the large language model (LLM) during pre-training, and introduce a weight mix strategy between LLMs trained by real-world and synthetic data. By directly integrating the weights from two domains, the mixed LLM can efficiently incorporate diverse semantics with favorable robustness. Then, to enable multi-purpose capabilities, we mix a variety of tasks for joint visual instruction tuning, and design task-specific instructions to avoid inter-task conflict. In addition to the basic visual question answering, we include more challenging tasks such as region-level understanding, caption grounding, document layout detection, and human pose estimation, contributing to mutual enhancement over different scenarios. Additionally, we propose to extract comprehensive visual embeddings from various network architectures, pre-training paradigms, and information granularity, providing language models with more robust image representations. Based on our proposed joint mixing, SPHINX exhibits superior multi-modal understanding capabilities on a wide range of applications. On top of this, we further propose an efficient strategy aiming to better capture fine-grained appearances of high-resolution images. With a mixing of different scales and high-resolution sub-images, SPHINX attains exceptional visual parsing and reasoning performance on existing evaluation benchmarks. We hope our work may cast a light on the exploration of joint mixing in future MLLM research.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2311.07575",
                "caption": "SPHINX: The Joint Mixing of Weights, Tasks, and Visual Embeddings for Multi-modal Large Language Models",
                "credit": "Ziyi Lin, Chris Liu, Renrui Zhang, Peng Gao, Longtian Qiu, Han Xiao, Han Qiu, Chen Lin, Wenqi Shao, Keqin Chen, Jiaming Han, Siyuan Huang, Yichi Zhang, Xuming He, Hongsheng Li, Yu Qiao"
            }
        },
        {
            "start_date": {
                "year": 2023,
                "month": 11,
                "day": 8
            },
            "text": {
                "headline": "MultiBand Diffusion",
                "text": "<p>Deep generative models can generate high-fidelity audio conditioned on various types of representations (e.g., mel-spectrograms, Mel-frequency Cepstral Coefficients (MFCC)). Recently, such models have been used to synthesize audio waveforms conditioned on highly compressed representations. Although such methods produce impressive results, they are prone to generate audible artifacts when the conditioning is flawed or imperfect. An alternative modeling approach is to use diffusion models. However, these have mainly been used as speech vocoders (i.e., conditioned on mel-spectrograms) or generating relatively low sampling rate signals. In this work, we propose a high-fidelity multi-band diffusion-based framework that generates any type of audio modality (e.g., speech, music, environmental sounds) from low-bitrate discrete representations. At equal bit rate, the proposed approach outperforms state-of-the-art generative techniques in terms of perceptual quality. Training and, evaluation code, along with audio samples, are available on the facebookresearch/audiocraft Github page.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2308.02560",
                "caption": "From Discrete Tokens to High-Fidelity Audio Using Multi-Band Diffusion",
                "credit": "Robin San Roman, Yossi Adi, Antoine Deleforge, Romain Serizel, Gabriel Synnaeve, Alexandre Défossez"
            }
        },
        {
            "start_date": {
                "year": 2023,
                "month": 11,
                "day": 7
            },
            "text": {
                "headline": "OmniVec",
                "text": "<p>Majority of research in learning based methods has been towards designing and training networks for specific tasks. However, many of the learning based tasks, across modalities, share commonalities and could be potentially tackled in a joint framework. We present an approach in such direction, to learn multiple tasks, in multiple modalities, with a unified architecture. The proposed network is composed of task specific encoders, a common trunk in the middle, followed by task specific prediction heads. We first pre-train it by self-supervised masked training, followed by sequential training for the different tasks. We train the network on all major modalities, e.g.\\ visual, audio, text and 3D, and report results on 22 diverse and challenging public benchmarks. We demonstrate empirically that, using a joint network to train across modalities leads to meaningful information sharing and this allows us to achieve state-of-the-art results on most of the benchmarks. We also show generalization of the trained network on cross-modal tasks as well as unseen datasets and tasks.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2311.05709v1",
                "caption": "OmniVec: Learning robust representations with cross modal sharing",
                "credit": "Siddharth Srivastava, Gaurav Sharma"
            }
        },
        {
            "start_date": {
                "year": 2023,
                "month": 11,
                "day": 7
            },
            "text": {
                "headline": "mPLUG-Owl2",
                "text": "<p>Multi-modal Large Language Models (MLLMs) have demonstrated impressive instruction abilities across various open-ended tasks. However, previous methods primarily focus on enhancing multi-modal capabilities. In this work, we introduce a versatile multi-modal large language model, mPLUG-Owl2, which effectively leverages modality collaboration to improve performance in both text and multi-modal tasks. mPLUG-Owl2 utilizes a modularized network design, with the language decoder acting as a universal interface for managing different modalities. Specifically, mPLUG-Owl2 incorporates shared functional modules to facilitate modality collaboration and introduces a modality-adaptive module that preserves modality-specific features. Extensive experiments reveal that mPLUG-Owl2 is capable of generalizing both text tasks and multi-modal tasks and achieving state-of-the-art performances with a single generic model. Notably, mPLUG-Owl2 is the first MLLM model that demonstrates the modality collaboration phenomenon in both pure-text and multi-modal scenarios, setting a pioneering path in the development of future multi-modal foundation models.\n</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2311.04257",
                "caption": "mPLUG-Owl2: Revolutionizing Multi-modal Large Language Model with Modality Collaboration",
                "credit": "Qinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Anwen Hu, Haowei Liu, Qi Qian, Ji Zhang, Fei Huang, Jingren Zhou"
            }
        },
        {
            "start_date": {
                "year": 2023,
                "month": 11,
                "day": 6
            },
            "text": {
                "headline": "GPT-4 Turbo",
                "text": "<p>Today, we shared dozens of new additions and improvements, and reduced pricing across many parts of our platform. These include:\n\nNew GPT-4 Turbo model that is more capable, cheaper and supports a 128K context window</p>"
            },
            "media": {
                "url": "https://openai.com/blog/new-models-and-developer-products-announced-at-devday",
                "caption": "New models and developer products announced at DevDay",
                "credit": ""
            }
        },
        {
            "start_date": {
                "year": 2023,
                "month": 11,
                "day": 6
            },
            "text": {
                "headline": "CogVLM-17B",
                "text": "<p>We introduce CogVLM, a powerful open-source visual language foundation model. Different from the popular shallow alignment method which maps image features into the input space of language model, CogVLM bridges the gap between the frozen pretrained language model and image encoder by a trainable visual expert module in the attention and FFN layers. As a result, CogVLM enables deep fusion of vision language features without sacrificing any performance on NLP tasks. CogVLM-17B achieves state-of-the-art performance on 10 classic cross-modal benchmarks, including NoCaps, Flicker30k captioning, RefCOCO, RefCOCO+, RefCOCOg, Visual7W, GQA, ScienceQA, VizWiz VQA and TDIUC, and ranks the 2nd on VQAv2, OKVQA, TextVQA, COCO captioning, etc., surpassing or matching PaLI-X 55B. Codes and checkpoints are available at this https URL. </p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2311.03079\nhttps://huggingface.co/THUDM/cogvlm-chat-hf\nhttps://github.com/THUDM/CogVLM\n",
                "caption": "CogVLM: Visual Expert for Pretrained Language Models",
                "credit": "Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan Song, Jiazheng Xu, Bin Xu, Juanzi Li, Yuxiao Dong, Ming Ding, Jie Tang"
            }
        },
        {
            "start_date": {
                "year": 2023,
                "month": 11,
                "day": 5
            },
            "text": {
                "headline": "LLaVA 1.5",
                "text": "<p>Large multimodal models (LMM) have recently shown encouraging progress with visual instruction tuning. In this note, we show that the fully-connected vision-language cross-modal connector in LLaVA is surprisingly powerful and data-efficient. With simple modifications to LLaVA, namely, using CLIP-ViT-L-336px with an MLP projection and adding academic-task-oriented VQA data with simple response formatting prompts, we establish stronger baselines that achieve state-of-the-art across 11 benchmarks. Our final 13B checkpoint uses merely 1.2M publicly available data, and finishes full training in ~1 day on a single 8-A100 node. We hope this can make state-of-the-art LMM research more accessible. Code and model will be publicly available. </p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2310.03744,\nhttps://huggingface.co/liuhaotian/llava-v1.5-13b",
                "caption": "Improved Baselines with Visual Instruction Tuning",
                "credit": "Haotian Liu, Chunyuan Li, Yuheng Li, Yong Jae Lee"
            }
        },
        {
            "start_date": {
                "year": 2023,
                "month": 11,
                "day": 4
            },
            "text": {
                "headline": "Grok-1",
                "text": "<p>Grok is an AI modeled after the Hitchhiker’s Guide to the Galaxy, so intended to answer almost anything and, far harder, even suggest what questions to ask!\n\nGrok is designed to answer questions with a bit of wit and has a rebellious streak, so please don’t use it if you hate humor!\n\nA unique and fundamental advantage of Grok is that it has real-time knowledge of the world via the 𝕏 platform. It will also answer spicy questions that are rejected by most other AI systems.\n\nGrok is still a very early beta product – the best we could do with 2 months of training – so expect it to improve rapidly with each passing week with your help.</p>"
            },
            "media": {
                "url": "https://x.ai/model-card/, https://x.ai/blog/grok-os",
                "caption": "Announcing Grok",
                "credit": ""
            }
        },
        {
            "start_date": {
                "year": 2023,
                "month": 11,
                "day": 3
            },
            "text": {
                "headline": "RT-Trajectory",
                "text": "<p>Generalization remains one of the most important desiderata for robust robot learning systems. While recently proposed approaches show promise in generalization to novel objects, semantic concepts, or visual distribution shifts, generalization to new tasks remains challenging. For example, a language-conditioned policy trained on pick-and-place tasks will not be able to generalize to a folding task, even if the arm trajectory of folding is similar to pick-and-place. Our key insight is that this kind of generalization becomes feasible if we represent the task through rough trajectory sketches. We propose a policy conditioning method using such rough trajectory sketches, which we call RT-Trajectory, that is practical, easy to specify, and allows the policy to effectively perform new tasks that would otherwise be challenging to perform. We find that trajectory sketches strike a balance between being detailed enough to express low-level motion-centric guidance while being coarse enough to allow the learned policy to interpret the trajectory sketch in the context of situational visual observations. In addition, we show how trajectory sketches can provide a useful interface to communicate with robotic policies: they can be specified through simple human inputs like drawings or videos, or through automated methods such as modern image-generating or waypoint-generating methods. We evaluate RT-Trajectory at scale on a variety of real-world robotic tasks, and find that RT-Trajectory is able to perform a wider range of tasks compared to language-conditioned and goal-conditioned policies, when provided the same training data.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2311.01977",
                "caption": "RT-Trajectory: Robotic Task Generalization via Hindsight Trajectory Sketches",
                "credit": "Jiayuan Gu, Sean Kirmani, Paul Wohlhart, Yao Lu, Montserrat Gonzalez Arenas, Kanishka Rao, Wenhao Yu, Chuyuan Fu, Keerthana Gopalakrishnan, Zhuo Xu, Priya Sundaresan, Peng Xu, Hao Su, Karol Hausman, Chelsea Finn, Quan Vuong, Ted Xiao"
            }
        },
        {
            "start_date": {
                "year": 2023,
                "month": 11,
                "day": 3
            },
            "text": {
                "headline": "BLUUMI",
                "text": "<p>Large language models (LLMs) excel in many tasks in NLP and beyond, but most open models have very limited coverage of smaller languages and LLM work tends to focus on languages where nearly unlimited data is available for pretraining. In this work, we study the challenges of creating LLMs for Finnish, a language spoken by less than 0.1% of the world population. We compile an extensive dataset of Finnish combining web crawls, news, social media and eBooks. We pursue two approaches to pretrain models: 1) we train seven monolingual models from scratch (186M to 13B parameters) dubbed FinGPT, 2) we continue the pretraining of the multilingual BLOOM model on a mix of its original training data and Finnish, resulting in a 176 billion parameter model we call BLUUMI. For model evaluation, we introduce FIN-bench, a version of BIG-bench with Finnish tasks. We also assess other model qualities such as toxicity and bias. Our models and tools are openly available at this https URL.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2311.05640",
                "caption": "FinGPT: Large Generative Models for a Small Language",
                "credit": "Risto Luukkonen, Ville Komulainen, Jouni Luoma, Anni Eskelinen, Jenna Kanerva, Hanna-Mari Kupari, Filip Ginter, Veronika Laippala, Niklas Muennighoff, Aleksandra Piktus, Thomas Wang, Nouamane Tazi, Teven Le Scao, Thomas Wolf, Osma Suominen, Samuli Sairanen, Mikko Merioksa, Jyrki Heinonen, Aija Vahtola, Samuel Antao, Sampo Pyysalo"
            }
        },
        {
            "start_date": {
                "year": 2023,
                "month": 11,
                "day": 2
            },
            "text": {
                "headline": "Yi-34B",
                "text": "<p>The Yi series models are large language models trained from scratch by developers at 01.AI.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2403.04652",
                "caption": "Yi: Open Foundation Models by 01.AI",
                "credit": "Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, Kaidong Yu, Peng Liu, Qiang Liu, Shawn Yue, Senbin Yang, Shiming Yang, Tao Yu, Wen Xie, Wenhao Huang, Xiaohui Hu, Xiaoyi Ren, Xinyao Niu, Pengcheng Nie, Yuchi Xu, Yudong Liu, Yue Wang, Yuxuan Cai, Zhenyu Gu, Zhiyuan Liu, Zonghong Dai"
            }
        },
        {
            "start_date": {
                "year": 2023,
                "month": 11,
                "day": 2
            },
            "text": {
                "headline": "Cohere Embed",
                "text": "<p>We're excited to introduce Embed v3, our latest and most advanced embeddings model. Embed v3 offers state-of-the-art performance per trusted MTEB and BEIR benchmarks.</p>"
            },
            "media": {
                "url": "https://txt.cohere.com/introducing-embed-v3/",
                "caption": "Cohere Command & Embed on Amazon Bedrock",
                "credit": "Nils Reimers, Elliott Choi, Amr Kayid, Alekhya Nandula, Manoj Govindassamy, Abdullah Elkady"
            }
        },
        {
            "start_date": {
                "year": 2023,
                "month": 10,
                "day": 30
            },
            "text": {
                "headline": "Skywork-13B",
                "text": "<p>In this technical report, we present Skywork-13B, a family of large language models (LLMs) trained on a corpus of over 3.2 trillion tokens drawn from both English and Chinese texts. This bilingual foundation model is the most extensively trained and openly published LLMs of comparable size to date. We introduce a two-stage training methodology using a segmented corpus, targeting general purpose training and then domain-specific enhancement training, respectively. We show that our model not only excels on popular benchmarks, but also achieves state of the art performance in Chinese language modeling on diverse domains. Furthermore, we propose a novel leakage detection method, demonstrating that test data contamination is a pressing issue warranting further investigation by the LLM community. To spur future research, we release Skywork-13B along with checkpoints obtained during intermediate stages of the training process. We are also releasing part of our SkyPile corpus, a collection of over 150 billion tokens of web text, which is the largest high quality open Chinese pre-training corpus to date. We hope Skywork-13B and our open corpus will serve as a valuable open-source resource to democratize access to high-quality LLMs.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2310.19341",
                "caption": "Skywork: A More Open Bilingual Foundation Model",
                "credit": "Tianwen Wei, Liang Zhao, Lichang Zhang, Bo Zhu, Lijie Wang, Haihua Yang, Biye Li, Cheng Cheng, Weiwei Lü, Rui Hu, Chenxia Li, Liu Yang, Xilin Luo, Xuejie Wu, Lunan Liu, Wenjun Cheng, Peng Cheng, Jianhao Zhang, Xiaoyu Zhang, Lei Lin, Xiaokun Wang, Yutuan Ma, Chuanhai Dong, Yanqi Sun, Yifu Chen, Yongyi Peng, Xiaojuan Liang, Shuicheng Yan, Han Fang, Yahui Zhou"
            }
        },
        {
            "start_date": {
                "year": 2023,
                "month": 10,
                "day": 27
            },
            "text": {
                "headline": "ChatGLM3-6B",
                "text": "<p>On October 27, 2023, at the 2023 China Computer Conference (CNCC), Zhipu AI launched the fully self-developed third-generation large base model ChatGLM3 and related series of products.</p>"
            },
            "media": {
                "url": "https://www.zhipuai.cn/en/news/76",
                "caption": "Zhipu AI launches third-generation base model",
                "credit": "Aohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, Da Yin, Diego Rojas, Guanyu Feng, Hanlin Zhao, Hanyu Lai, Hao Yu, Hongning Wang, Jiadai Sun, Jiajie Zhang, Jiale Cheng, Jiayi Gui, Jie Tang, Jing Zhang, Juanzi Li, Lei Zhao, Lindong Wu, Lucen Zhong, Mingdao Liu, Minlie Huang, Peng Zhang, Qinkai Zheng, Rui Lu, Shuaiqi Duan, Shudan Zhang, Shulin Cao, Shuxun Yang, Weng Lam Tam, Wenyi Zhao, Xiao Liu, Xiao Xia, Xiaohan Zhang, Xiaotao Gu, Xin Lv, Xinghan Liu, Xinyi Liu, Xinyue Yang, Xixuan Song, Xunkai Zhang, Yifan An, Yifan Xu, Yilin Niu, Yuantao Yang, Yueyan Li, Yushi Bai, Yuxiao Dong, Zehan Qi, Zhaoyu Wang, Zhen Yang, Zhengxiao Du, Zhenyu Hou, Zihan Wang"
            }
        },
        {
            "start_date": {
                "year": 2023,
                "month": 10,
                "day": 26
            },
            "text": {
                "headline": "DiT-XL/2 + CADS",
                "text": "<p>While conditional diffusion models are known to have good coverage of the data distribution, they still face limitations in output diversity, particularly when sampled with a high classifier-free guidance scale for optimal image quality or when trained on small datasets. We attribute this problem to the role of the conditioning signal in inference and offer an improved sampling strategy for diffusion models that can increase generation diversity, especially at high guidance scales, with minimal loss of sample quality. Our sampling strategy anneals the conditioning signal by adding scheduled, monotonically decreasing Gaussian noise to the conditioning vector during inference to balance diversity and condition alignment. Our Condition-Annealed Diffusion Sampler (CADS) can be used with any pretrained model and sampling algorithm, and we show that it boosts the diversity of diffusion models in various conditional generation tasks. Further, using an existing pretrained diffusion model, CADS achieves a new state-of-the-art FID of 1.70 and 2.31 for class-conditional ImageNet generation at 256×256 and 512×512 respectively.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2310.17347v2",
                "caption": "CADS: Unleashing the Diversity of Diffusion Models through Condition-Annealed Sampling",
                "credit": "Seyedmorteza Sadat, Jakob Buhmann, Derek Bradley, Otmar Hilliges, Romann M. Weber"
            }
        },
        {
            "start_date": {
                "year": 2023,
                "month": 10,
                "day": 26
            },
            "text": {
                "headline": "CODEFUSION (Python)",
                "text": "<p>Imagine a developer who can only change their last line of code, how often would they have to start writing a function from scratch before it is correct? Auto-regressive models for code generation from natural language have a similar limitation: they do not easily allow reconsidering earlier tokens generated. We introduce CodeFusion, a pre-trained diffusion code generation model that addresses this limitation by iteratively denoising a complete program conditioned on the encoded natural language. We evaluate CodeFusion on the task of natural language to code generation for Bash, Python, and Microsoft Excel conditional formatting (CF) rules. Experiments show that CodeFusion (75M parameters) performs on par with state-of-the-art auto-regressive systems (350M-175B parameters) in top-1 accuracy and outperforms them in top-3 and top-5 accuracy due to its better balance in diversity versus quality.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2310.17680 (was withdrawn)",
                "caption": "CODEFUSION: A Pre-trained Diffusion Model for Code Generation",
                "credit": "Mukul Singh, José Cambronero, Sumit Gulwani, Vu Le, Carina Negreanu, Gust Verbruggen"
            }
        },
        {
            "start_date": {
                "year": 2023,
                "month": 10,
                "day": 19
            },
            "text": {
                "headline": "DALL·E 3",
                "text": "<p>We show that prompt following abilities of text-to-image models can be substantially improved by training on highly descriptive generated image captions.\nExisting text-to-image models struggle to follow detailed image descriptions and often ignore words or confuse the meaning of prompts. We hypothesize that this issue stems from noisy and inaccurate image captions in the training dataset. We address this by training a bespoke image captioner and use it to recaption the training dataset. We then train several text-to-image models and find that training on these synthetic captions reliably improves prompt following ability. Finally, we use these findings to build DALL-E 3: a new text-to-image generation system, and benchmark its performance on an evaluation designed to measure prompt following, coherence, and aesthetics, finding that it compares favorably to competitors. We publish samples and code for these evaluations so that future research can continue optimizing this important aspect of text-to-image systems.</p>"
            },
            "media": {
                "url": "https://cdn.openai.com/papers/dall-e-3.pdf",
                "caption": "Improving Image Generation with Better Captions",
                "credit": "James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, Wesam Manassra, Prafulla Dhariwal, Casey Chu, Yunxin Jiao, Aditya Ramesh"
            }
        },
        {
            "start_date": {
                "year": 2023,
                "month": 10,
                "day": 17
            },
            "text": {
                "headline": "ERNIE 4.0",
                "text": "<p>Baidu, Inc. (NASDAQ: BIDU and HKEX: 9888), a leading AI company with strong Internet foundation, today hosted its annual flagship technology conference Baidu World 2023 in Beijing, marking the conference's return to an offline format after four years. With the theme \"Prompt the World,\" this year's Baidu World conference saw Baidu launch ERNIE 4.0, Baidu's next-generation and most powerful foundation model offering drastically enhanced core AI capabilities. Baidu also showcased some of its most popular applications, solutions, and products re-built around the company's state-of-the-art generative AI.                                                                               \n\nRobin Li, Co-founder, Chairman and CEO of Baidu, announced ERNIE 4.0 at Baidu World 2023\n\"ERNIE 4.0 has achieved a full upgrade with drastically improved performance in understanding, generation, reasoning, and memory,\" Robin Li, Co-founder, Chairman and CEO of Baidu, said at the event. \"These four core capabilities form the foundation of AI-native applications and have now unleashed unlimited opportunities for new innovations.\"\n\n</p>"
            },
            "media": {
                "url": "https://www.prnewswire.com/news-releases/baidu-launches-ernie-4-0-foundation-model-leading-a-new-wave-of-ai-native-applications-301958681.html",
                "caption": "Baidu Launches ERNIE 4.0 Foundation Model, Leading a New Wave of AI-Native Applications",
                "credit": ""
            }
        },
        {
            "start_date": {
                "year": 2023,
                "month": 10,
                "day": 13
            },
            "text": {
                "headline": "RT-2-X",
                "text": "<p>Large, high-capacity models trained on diverse datasets have shown remarkable successes on efficiently tackling downstream applications. In domains from NLP to Computer Vision, this has led to a consolidation of pretrained models, with general pretrained backbones serving as a starting point for many applications. Can such a consolidation happen in robotics? Conventionally, robotic learning methods train a separate model for every application, every robot, and even every environment. Can we instead train generalist X-robot policy that can be adapted efficiently to new robots, tasks, and environments? In this paper, we provide datasets in standardized data formats and models to make it possible to explore this possibility in the context of robotic manipulation, alongside experimental results that provide an example of effective X-robot policies. We assemble a dataset from 22 different robots collected through a collaboration between 21 institutions, demonstrating 527 skills (160266 tasks). We show that a high-capacity model trained on this data, which we call RT-X, exhibits positive transfer and improves the capabilities of multiple robots by leveraging experience from other platforms. More details can be found on the project website this https URL.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2310.08864",
                "caption": "Open X-Embodiment: Robotic Learning Datasets and RT-X Models",
                "credit": "Open X-Embodiment Collaboration"
            }
        },
        {
            "start_date": {
                "year": 2023,
                "month": 10,
                "day": 11
            },
            "text": {
                "headline": "Ferret (13B)",
                "text": "<p>We introduce Ferret, a new Multimodal Large Language Model (MLLM) capable of understanding spatial referring of any shape or granularity within an image and accurately grounding open-vocabulary descriptions. To unify referring and grounding in the LLM paradigm, Ferret employs a novel and powerful hybrid region representation that integrates discrete coordinates and continuous features jointly to represent a region in the image. To extract the continuous features of versatile regions, we propose a spatial-aware visual sampler, adept at handling varying sparsity across different shapes. Consequently, Ferret can accept diverse region inputs, such as points, bounding boxes, and free-form shapes. To bolster the desired capability of Ferret, we curate GRIT, a comprehensive refer-and-ground instruction tuning dataset including 1.1M samples that contain rich hierarchical spatial knowledge, with 95K hard negative data to promote model robustness. The resulting model not only achieves superior performance in classical referring and grounding tasks, but also greatly outperforms existing MLLMs in region-based and localization-demanded multimodal chatting. Our evaluations also reveal a significantly improved capability of describing image details and a remarkable alleviation in object hallucination.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2310.07704",
                "caption": "Ferret: Refer and Ground Anything Anywhere at Any Granularity",
                "credit": "Haoxuan You, Haotian Zhang, Zhe Gan, Xianzhi Du, Bowen Zhang, Zirui Wang, Liangliang Cao, Shih-Fu Chang, Yinfei Yang"
            }
        },
        {
            "start_date": {
                "year": 2023,
                "month": 10,
                "day": 7
            },
            "text": {
                "headline": "FinGPT-13B",
                "text": "<p>In the swiftly expanding domain of Natural Language Processing (NLP), the potential of GPT-based models for the financial sector is increasingly evident. However, the integration of these models with financial datasets presents challenges, notably in determining their adeptness and relevance. This paper introduces a distinctive approach anchored in the Instruction Tuning paradigm for open-source large language models, specifically adapted for financial contexts. Through this methodology, we capitalize on the interoperability of open-source models, ensuring a seamless and transparent integration. We begin by explaining the Instruction Tuning paradigm, highlighting its effectiveness for immediate integration. The paper presents a benchmarking scheme designed for end-to-end training and testing, employing a cost-effective progression. Firstly, we assess basic competencies and fundamental tasks, such as Named Entity Recognition (NER) and sentiment analysis to enhance specialization. Next, we delve into a comprehensive model, executing multi-task operations by amalgamating all instructional tunings to examine versatility. Finally, we explore the zero-shot capabilities by earmarking unseen tasks and incorporating novel datasets to understand adaptability in uncharted terrains. Such a paradigm fortifies the principles of openness and reproducibility, laying a robust foundation for future investigations in open-source financial large language models (FinLLMs).</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2310.04793; https://github.com/AI4Finance-Foundation/FinGPT",
                "caption": "FinGPT: Instruction Tuning Benchmark for Open-Source Large Language Models in Financial Datasets",
                "credit": "Neng Wang, Hongyang Yang, Christina Dan Wang"
            }
        },
        {
            "start_date": {
                "year": 2023,
                "month": 10,
                "day": 1
            },
            "text": {
                "headline": "CTM (CIFAR-10)",
                "text": "<p>Consistency Models (CM) (Song et al., 2023) accelerate score-based diffusion model sampling at the cost of sample quality but lack a natural way to trade-off quality for speed. To address this limitation, we propose Consistency Trajectory Model (CTM), a generalization encompassing CM and score-based models as special cases. CTM trains a single neural network that can -- in a single forward pass -- output scores (i.e., gradients of log-density) and enables unrestricted traversal between any initial and final time along the Probability Flow Ordinary Differential Equation (ODE) in a diffusion process. CTM enables the efficient combination of adversarial training and denoising score matching loss to enhance performance and achieves new state-of-the-art FIDs for single-step diffusion model sampling on CIFAR-10 (FID 1.73) and ImageNet at 64X64 resolution (FID 2.06). CTM also enables a new family of sampling schemes, both deterministic and stochastic, involving long jumps along the ODE solution trajectories. It consistently improves sample quality as computational budgets increase, avoiding the degradation seen in CM. Furthermore, CTM's access to the score accommodates all diffusion model inference techniques, including exact likelihood computation.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2310.02279v1",
                "caption": "Consistency Trajectory Models: Learning Probability Flow ODE Trajectory of Diffusion",
                "credit": "Dongjun Kim, Chieh-Hsin Lai, Wei-Hsiang Liao, Naoki Murata, Yuhta Takida, Toshimitsu Uesaka, Yutong He, Yuki Mitsufuji, Stefano Ermon"
            }
        },
        {
            "start_date": {
                "year": 2023,
                "month": 9,
                "day": 28
            },
            "text": {
                "headline": "Amazon Titan",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://aws.amazon.com/bedrock/titan/",
                "caption": "",
                "credit": ""
            }
        },
        {
            "start_date": {
                "year": 2023,
                "month": 9,
                "day": 27
            },
            "text": {
                "headline": "Show-1",
                "text": "<p>Significant advancements have been achieved in the realm of large-scale pre-trained text-to-video Diffusion Models (VDMs). However, previous methods either rely solely on pixel-based VDMs, which come with high computational costs, or on latent-based VDMs, which often struggle with precise text-video alignment. In this paper, we are the first to propose a hybrid model, dubbed as Show-1, which marries pixel-based and latent-based VDMs for text-to-video generation. Our model first uses pixel-based VDMs to produce a low-resolution video of strong text-video correlation. After that, we propose a novel expert translation method that employs the latent-based VDMs to further upsample the low-resolution video to high resolution. Compared to latent VDMs, Show-1 can produce high-quality videos of precise text-video alignment; Compared to pixel VDMs, Show-1 is much more efficient (GPU memory usage during inference is 15G vs 72G). We also validate our model on standard video generation benchmarks. Our code and model weights are publicly available at this https URL.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2309.15818",
                "caption": "Show-1: Marrying Pixel and Latent Diffusion Models for Text-to-Video Generation",
                "credit": "David Junhao Zhang, Jay Zhangjie Wu, Jia-Wei Liu, Rui Zhao, Lingmin Ran, Yuchao Gu, Difei Gao, Mike Zheng Shou"
            }
        },
        {
            "start_date": {
                "year": 2023,
                "month": 9,
                "day": 25
            },
            "text": {
                "headline": "GPT-4V",
                "text": "<p>GPT-4 with vision (GPT-4V) enables users to instruct GPT-4 to analyze image inputs provided by the user, and is the latest capability we are making broadly available. Incorporating additional modalities (such as image inputs) into large language models (LLMs) is viewed by some as a key frontier in artificial intelligence research and development. Multimodal LLMs offer the possibility of expanding the impact of language-only systems with novel interfaces and capabilities, enabling them to solve new tasks and provide novel experiences for their users. In this system card, we analyze the safety properties of GPT-4V. Our work on safety for GPT-4V builds on the work done for GPT-4 and here we dive deeper into the evaluations, preparation, and mitigation work done specifically for image inputs.</p>"
            },
            "media": {
                "url": "https://cdn.openai.com/papers/GPTV_System_Card.pdf",
                "caption": "GPT-4V(ision) system card",
                "credit": ""
            }
        },
        {
            "start_date": {
                "year": 2023,
                "month": 9,
                "day": 22
            },
            "text": {
                "headline": "AlphaMissense",
                "text": "<p>The vast majority of missense variants observed in the human genome are of unknown clinical significance. We present AlphaMissense, an adaptation of AlphaFold fine-tuned on human and primate variant population frequency databases to predict missense variant pathogenicity. By combining structural context and evolutionary conservation, our model achieves state-of-the-art results across a wide range of genetic and experimental benchmarks, all without explicitly training on such data. The average pathogenicity score of genes is also predictive for their cell essentiality, capable of identifying short essential genes that existing statistical approaches are underpowered to detect. As a resource to the community, we provide a database of predictions for all possible human single amino acid substitutions and classify 89% of missense variants as either likely benign or likely pathogenic.</p>"
            },
            "media": {
                "url": "https://www.science.org/doi/10.1126/science.adg7492",
                "caption": "Accurate proteome-wide missense variant effect prediction with AlphaMissense",
                "credit": "Jun Cheng, Guido Novati, Joshua Pan, Clare Bycroft, Akvile ̇Žemgulyte ̇, Taylor Applebaum, Alexander Pritzel, Lai Hong Wong, Michal Zielinski, Tobias Sargeant, Rosalia G. Schneider,Andrew W. Senior, John Jumper, Demis Hassabis, Pushmeet Kohli,Žiga Avsec"
            }
        },
        {
            "start_date": {
                "year": 2023,
                "month": 9,
                "day": 12
            },
            "text": {
                "headline": "Robot Parkour",
                "text": "<p>Parkour is a grand challenge for legged locomotion that requires robots to overcome various obstacles rapidly in complex environments. Existing methods can generate either diverse but blind locomotion skills or vision-based but specialized skills by using reference animal data or complex rewards. However, autonomous parkour requires robots to learn generalizable skills that are both vision-based and diverse to perceive and react to various scenarios. In this work, we propose a system for learning a single end-to-end vision-based parkour policy of diverse parkour skills using a simple reward without any reference motion data. We develop a reinforcement learning method inspired by direct collocation to generate parkour skills, including climbing over high obstacles, leaping over large gaps, crawling beneath low barriers, squeezing through thin slits, and running. We distill these skills into a single vision-based parkour policy and transfer it to a quadrupedal robot using its egocentric depth camera. We demonstrate that our system can empower two different low-cost robots to autonomously select and execute appropriate parkour skills to traverse challenging real-world environments.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2309.05665",
                "caption": "Robot Parkour Learning",
                "credit": "Ziwen Zhuang, Zipeng Fu, Jianren Wang, Christopher Atkeson, Soeren Schwertfeger, Chelsea Finn, Hang Zhao"
            }
        },
        {
            "start_date": {
                "year": 2023,
                "month": 9,
                "day": 6
            },
            "text": {
                "headline": "Falcon-180B",
                "text": "<p>Falcon 180B is a super-powerful language model with 180 billion parameters, trained on 3.5 trillion tokens. It's currently at the top of the Hugging Face Leaderboard for pre-trained Open Large Language Models and is available for both research and commercial use.\n\nThis model performs exceptionally well in various tasks like reasoning, coding, proficiency, and knowledge tests, even beating competitors like Meta's LLaMA 2.\n\nAmong closed source models, it ranks just behind OpenAI's GPT 4, and performs on par with Google's PaLM 2 Large, which powers Bard, despite being half the size of the model.</p>"
            },
            "media": {
                "url": "https://falconllm.tii.ae/falcon-180b.html; https://arxiv.org/abs/2311.16867",
                "caption": "The Falcon Series of Open Language Models",
                "credit": "Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, Mérouane Debbah, Étienne Goffinet, Daniel Hesslow, Julien Launay, Quentin Malartic, Daniele Mazzotta, Badreddine Noune, Baptiste Pannier, Guilherme Penedo"
            }
        },
        {
            "start_date": {
                "year": 2023,
                "month": 8,
                "day": 30
            },
            "text": {
                "headline": "Swift",
                "text": "<p>First-person view (FPV) drone racing is a televised sport in which professional competitors pilot high-speed aircraft through a 3D circuit. Each pilot sees the environment from the perspective of their drone by means of video streamed from an onboard camera. Reaching the level of professional pilots with an autonomous drone is challenging because the robot needs to fly at its physical limits while estimating its speed and location in the circuit exclusively from onboard sensors1. Here we introduce Swift, an autonomous system that can race physical vehicles at the level of the human world champions. The system combines deep reinforcement learning (RL) in simulation with data collected in the physical world. Swift competed against three human champions, including the world champions of two international leagues, in real-world head-to-head races. Swift won several races against each of the human champions and demonstrated the fastest recorded race time. This work represents a milestone for mobile robotics and machine intelligence2, which may inspire the deployment of hybrid learning-based solutions in other physical systems.</p>"
            },
            "media": {
                "url": "https://www.nature.com/articles/s41586-023-06419-4",
                "caption": "Champion-level drone racing using deep reinforcement learning",
                "credit": "Elia Kaufmann, Leonard Bauersfeld, Antonio Loquercio, Matthias Müller, Vladlen Koltun, Davide Scaramuzza "
            }
        },
        {
            "start_date": {
                "year": 2023,
                "month": 8,
                "day": 29
            },
            "text": {
                "headline": "Jais",
                "text": "<p>We introduce Jais and Jais-chat, new state-of-the-art Arabic-centric foundation and instruction-tuned open generative large language models (LLMs). The models are based on the GPT-3 decoder-only architecture and are pretrained on a mixture of Arabic and English texts, including source code in various programming languages. With 13 billion parameters, they demonstrate better knowledge and reasoning capabilities in Arabic than any existing open Arabic and multilingual models by a sizable margin, based on extensive evaluation. Moreover, the models are competitive in English compared to English-centric open models of similar size, despite being trained on much less English data. We provide a detailed description of the training, the tuning, the safety alignment, and the evaluation of the models. We release two open versions of the model —the foundation Jais model, and an instruction-tuned Jais-chat variant— with the aim of promoting research on Arabic LLMs.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2308.16149",
                "caption": "Jais and Jais-chat: Arabic-Centric Foundation and Instruction-Tuned Open Generative Large Language Models",
                "credit": "Neha Sengupta, Sunil Kumar Sahu, Bokang Jia, Satheesh Katipomu, Haonan Li, Fajri Koto, Osama Mohammed Afzal, Samta Kamboj, Onkar Pandit, Rahul Pal, Lalit Pradhan, Zain Muhammad Mujahid, Massa Baali, Alham Fikri Aji, Zhengzhong Liu, Andy Hock, Andrew Feldman, Jonathan Lee, Andrew Jackson, Preslav Nakov, Timothy Baldwin, Eric Xing"
            }
        },
        {
            "start_date": {
                "year": 2023,
                "month": 8,
                "day": 28
            },
            "text": {
                "headline": "PeptideBERT",
                "text": "<p>Recent advances in Language Models have enabled the protein modeling community with a powerful tool since protein sequences can be represented as text. Specifically, by taking advantage of Transformers, sequence-to-property prediction will be amenable without the need for explicit structural data. In this work, inspired by recent progress in Large Language Models (LLMs), we introduce PeptideBERT, a protein language model for predicting three key properties of peptides (hemolysis, solubility, and non- fouling). The PeptideBert utilizes the ProtBERT pretrained transformer model with 12 attention heads and 12 hidden layers. We then finetuned the pretrained model for the three downstream tasks. Our model has achieved state of the art (SOTA) for predicting Hemolysis, which is a task for determining peptide’s potential to induce red blood cell lysis. Our PeptideBert non-fouling model also achieved remarkable accuracy in predicting peptide’s capacity to resist non-specific interactions. This model, trained predominantly on shorter sequences, benefits from the dataset where negative examples are largely associated with insoluble peptides. Codes, models, and data used in this study are freely available at: https://github.com/ChakradharG/PeptideBERT</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2309.03099",
                "caption": "PeptideBERT: A language Model based on Transformers for Peptide Property Prediction",
                "credit": "Chakradhar Guntuboina, Adrita Das, Parisa Mollaei, Seongwon Kim, and Amir Barati Farimani"
            }
        },
        {
            "start_date": {
                "year": 2023,
                "month": 8,
                "day": 24
            },
            "text": {
                "headline": "Qwen-VL",
                "text": "<p>We introduce the Qwen-VL series, a set of large-scale vision-language models designed to perceive and understand both text and images. Comprising Qwen-VL and Qwen-VL-Chat, these models exhibit remarkable performance in tasks like image captioning, question answering, visual localization, and flexible interaction. The evaluation covers a wide range of tasks including zero-shot captioning, visual or document visual question answering, and grounding. We demonstrate the Qwen-VL outperforms existing Large Vision Language Models (LVLMs). We present their architecture, training, capabilities, and performance, highlighting their contributions to advancing multimodal artificial intelligence. Code, demo and models are available at https://github.com/QwenLM/Qwen-VL.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2308.12966",
                "caption": "Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond",
                "credit": "Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, Jingren Zhou"
            }
        },
        {
            "start_date": {
                "year": 2023,
                "month": 8,
                "day": 5
            },
            "text": {
                "headline": "GGNN",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://www.nature.com/articles/s42003-023-05133-1",
                "caption": "Integration of pre-trained protein language models into geometric deep learning networks",
                "credit": "Fang Wu, Lirong Wu, Dragomir Radev, Jinbo Xu and Stan Z. Li"
            }
        },
        {
            "start_date": {
                "year": 2023,
                "month": 7,
                "day": 28
            },
            "text": {
                "headline": "RT-2",
                "text": "<p>We study how vision-language models trained on Internet-scale data can be incorporated directly into end-to-end robotic control to boost generalization and enable emergent semantic reasoning. Our goal is to enable a single end-to-end trained model to both learn to map robot observations to actions and enjoy the benefits of large-scale pretraining on language and vision-language data from the web. To this end, we propose to co-fine-tune state-of-the-art vision-language models on both robotic trajectory data and Internet-scale vision-language tasks, such as visual question answering. In contrast to other approaches, we propose a simple, general recipe to achieve this goal: in order to fit both natural language responses and robotic actions into the same format, we express the actions as text tokens and incorporate them directly into the training set of the model in the same way as natural language tokens. We refer to such category of models as vision-language-action models (VLA) and instantiate an example of such a model, which we call RT-2. Our extensive evaluation (6k evaluation trials) shows that our approach leads to performant robotic policies and enables RT-2 to obtain a range of emergent capabilities from Internet-scale training. This includes significantly improved generalization to novel objects, the ability to interpret commands not present in the robot training data (such as placing an object onto a particular number or icon), and the ability to perform rudimentary reasoning in response to user commands (such as picking up the smallest or largest object, or the one closest to another object). We further show that incorporating chain of thought reasoning allows RT-2 to perform multi-stage semantic reasoning, for example figuring out which object to pick up for use as an improvised hammer (a rock), or which type of drink is best suited for someone who is tired (an energy drink).</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2307.15818",
                "caption": "RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control",
                "credit": "Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, Pete Florence, Chuyuan Fu, Montse Gonzalez Arenas, Keerthana Gopalakrishnan, Kehang Han, Karol Hausman, Alexander Herzog, Jasmine Hsu, Brian Ichter, Alex Irpan, Nikhil Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Isabel Leal, Lisa Lee, Tsang-Wei Edward Lee, Sergey Levine, Yao Lu, Henryk Michalewski, Igor Mordatch, Karl Pertsch, Kanishka Rao, Krista Reymann, Michael Ryoo, Grecia Salazar, Pannag Sanketi, Pierre Sermanet, Jaspiar Singh, Anikait Singh, Radu Soricut, Huong Tran, Vincent Vanhoucke, Quan Vuong, Ayzaan Wahid, Stefan Welker, Paul Wohlhart, Jialin Wu, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Tianhe Yu, Brianna Zitkovich"
            }
        },
        {
            "start_date": {
                "year": 2023,
                "month": 7,
                "day": 26
            },
            "text": {
                "headline": "AudioLM",
                "text": "<p>We introduce AudioLM, a framework for high-quality audio generation with long-term consistency. AudioLM maps the input audio to a sequence of discrete tokens and casts audio generation as a language modeling task in this representation space. We show how existing audio tokenizers provide different trade-offs between reconstruction quality and long-term structure, and we propose a hybrid tokenization scheme to achieve both objectives. Namely, we leverage the discretized activations of a masked language model pre-trained on audio to capture long-term structure and the discrete codes produced by a neural audio codec to achieve high-quality synthesis. By training on large corpora of raw audio waveforms, AudioLM learns to generate natural and coherent continuations given short prompts. When trained on speech, and without any transcript or annotation, AudioLM generates syntactically and semantically plausible speech continuations while also maintaining speaker identity and prosody for unseen speakers. Furthermore, we demonstrate how our approach extends beyond speech by generating coherent piano music continuations, despite being trained without any symbolic representation of music.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2209.03143",
                "caption": "AudioLM: a Language Modeling Approach to Audio Generation",
                "credit": "Zalán Borsos, Raphaël Marinier, Damien Vincent, Eugene Kharitonov, Olivier Pietquin, Matt Sharifi, Dominik Roblek, Olivier Teboul, David Grangier, Marco Tagliasacchi, Neil Zeghidour"
            }
        },
        {
            "start_date": {
                "year": 2023,
                "month": 7,
                "day": 18
            },
            "text": {
                "headline": "Llama 2-70B",
                "text": "<p>In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closedsource models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.</p>"
            },
            "media": {
                "url": "https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/\nhttps://arxiv.org/abs/2307.09288",
                "caption": "Llama 2: Open Foundation and Fine-Tuned Chat Models",
                "credit": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, Thomas Scialom\n"
            }
        },
        {
            "start_date": {
                "year": 2023,
                "month": 7,
                "day": 18
            },
            "text": {
                "headline": "Llama 2-7B",
                "text": "<p>In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closedsource models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.</p>"
            },
            "media": {
                "url": "https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/",
                "caption": "Llama 2: Open Foundation and Fine-Tuned Chat Models",
                "credit": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, Thomas Scialom\n"
            }
        },
        {
            "start_date": {
                "year": 2023,
                "month": 7,
                "day": 11
            },
            "text": {
                "headline": "Claude 2",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://www.anthropic.com/index/claude-2, https://www-files.anthropic.com/production/images/Model-Card-Claude-2.pdf",
                "caption": "",
                "credit": ""
            }
        },
        {
            "start_date": {
                "year": 2023,
                "month": 7,
                "day": 6
            },
            "text": {
                "headline": "xTrimoPGLM -100B",
                "text": "<p>Protein language models have shown remarkable success in learning biological information from protein sequences. However, most existing models are limited by either autoencoding or autoregressive pre-training objectives, which makes them struggle to handle protein understanding and generation tasks concurrently. This paper proposes a unified protein language model, xTrimoPGLM, to address these two types of tasks simultaneously through an innovative pre-training framework. Our key technical contribution is an exploration of the compatibility and the potential for joint optimization of the two types of objectives, which has led to a strategy for training xTrimoPGLM at an unprecedented scale of 100 billion parameters and 1 trillion training tokens. Our extensive experiments reveal that xTrimoPGLM significantly outperforms other advanced baselines in diverse protein understanding tasks (13 out of 15 tasks across four categories) and generates novel protein sequences which are structurally similar to natural ones. Furthermore, using the same xTrimoPGLM framework, we train an antibody-specific model (xTrimoPGLM-Ab) using 1 billion parameters. This model set a new record in predicting antibody naturalness and structures, both essential to the field of antibody-based drug design, and demonstrated a significantly faster inference speed than AlphaFold2. These results highlight the substantial capability and versatility of xTrimoPGLM in understanding and generating protein sequences.</p>"
            },
            "media": {
                "url": "https://www.biorxiv.org/content/10.1101/2023.07.05.547496v4",
                "caption": "xTrimoPGLM: Unified 100B-Scale Pre-trained Transformer for Deciphering the Language of Protein",
                "credit": "Bo Chen, Xingyi Cheng, Yangli-ao Geng, Shen Li, Xin Zeng, Boyan Wang, Jing Gong, Chiming Liu, Aohan Zeng, Yuxiao Dong, Jie Tang, Le Song"
            }
        },
        {
            "start_date": {
                "year": 2023,
                "month": 7,
                "day": 6
            },
            "text": {
                "headline": "InternLM",
                "text": "<p>Pre-training a bilingual 100B Foundation model on data with over a trillion tokens, the model exhibits excellent performance in scenarios such as Chinese, English, and coding due to the appropriate data ratio. Based on the foundation model, the application of high-quality human annotated dialogue data combined with RLHF technology enables the InternLM large language model to respond to complex commands during human interaction, while also demonstrating responses in line with human morality and values.</p>"
            },
            "media": {
                "url": "https://internlm.org/",
                "caption": "",
                "credit": ""
            }
        },
        {
            "start_date": {
                "year": 2023,
                "month": 7,
                "day": 5
            },
            "text": {
                "headline": "Pangu-Weather",
                "text": "<p>Weather forecasting is important for science and society. At present, the most accurate forecast system is the numerical weather prediction (NWP) method, which represents atmospheric states as discretized grids and numerically solves partial diferential equations that describe the transition between those states1 . However, this procedure is computationally expensive. Recently, artifcial-intelligence-based methods2 have shown potential in accelerating weather forecasting by orders of magnitude, but the forecast accuracy is still signifcantly lower than that of NWP methods. Here we introduce an artifcial-intelligence-based method for accurate, medium-range global weather forecasting. We show that three-dimensional deep networks equipped with\nEarth-specifc priors are efective at dealing with complex patterns in weather data, and that a hierarchical temporal aggregation strategy reduces accumulation errors in medium-range forecasting. Trained on 39 years of global data, our program, Pangu-Weather, obtains stronger deterministic forecast results on reanalysis data in all tested variables when compared with the world’s best NWP system, the operational integrated forecasting system of the European Centre for Medium-Range Weather Forecasts (ECMWF)3\n. Our method also works well with extreme weather forecasts and ensemble forecasts. When initialized with reanalysis data, the accuracy of tracking\ntropical cyclones is also higher than that of ECMWF-HRES.</p>"
            },
            "media": {
                "url": "https://www.nature.com/articles/s41586-023-06185-3, https://www.huaweicloud.com/intl/en-us/news/20230707180809498.html,\nhttps://www.huawei.com/en/news/2023/7/pangu-ai-model-nature-publish",
                "caption": "Accurate medium-range global weather forecasting with 3D neural networks",
                "credit": "Kaifeng Bi, Lingxi Xie, Hengheng Zhang, Xin Chen, Xiaotao Gu, Qi Tian"
            }
        },
        {
            "start_date": {
                "year": 2023,
                "month": 7,
                "day": 4
            },
            "text": {
                "headline": "Stable Diffusion XL (SDXL)",
                "text": "<p>We present SDXL, a latent diffusion model for text-to-image synthesis. Compared to previous versions of Stable Diffusion, SDXL leverages a three times larger UNet backbone: The increase of model parameters is mainly due to more attention blocks and a larger cross-attention context as SDXL uses a second text encoder. We design multiple novel conditioning schemes and train SDXL on multiple aspect ratios. We also introduce a refinement model which is used to improve the visual fidelity of samples generated by SDXL using a post-hoc image-to-image technique. We demonstrate that SDXL shows drastically improved performance compared the previous versions of Stable Diffusion and achieves results competitive with those of black-box state-of-the-art image generators. In the spirit of promoting open research and fostering transparency in large model training and evaluation, we provide access to code and model weights at this https URL</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2307.01952",
                "caption": "SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis",
                "credit": "Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, Robin Rombach"
            }
        },
        {
            "start_date": {
                "year": 2023,
                "month": 6,
                "day": 27
            },
            "text": {
                "headline": "HyenaDNA",
                "text": "<p>Genomic (DNA) sequences encode an enormous amount of information for gene regulation and protein synthesis. Similar to natural language models, researchers have proposed foundation models in genomics to learn generalizable features from unlabeled genome data that can then be fine-tuned for downstream tasks such as identifying regulatory elements. Due to the quadratic scaling of attention, previous Transformer-based genomic models have used 512 to 4k tokens as context (<0.001% of the human genome), significantly limiting the modeling of long-range interactions in DNA. In addition, these methods rely on tokenizers or fixed k-mers to aggregate meaningful DNA units, losing single nucleotide resolution where subtle genetic variations can completely alter protein function via single nucleotide polymorphisms (SNPs). Recently, Hyena, a large language model based on implicit convolutions was shown to match attention in quality while allowing longer context lengths and lower time complexity. Leveraging Hyena's new long-range capabilities, we present HyenaDNA, a genomic foundation model pretrained on the human reference genome with context lengths of up to 1 million tokens at the single nucleotide-level - an up to 500x increase over previous dense attention-based models. HyenaDNA scales sub-quadratically in sequence length (training up to 160x faster than Transformer), uses single nucleotide tokens, and has full global context at each layer. We explore what longer context enables - including the first use of in-context learning in genomics. On fine-tuned benchmarks from the Nucleotide Transformer, HyenaDNA reaches state-of-the-art (SotA) on 12 of 18 datasets using a model with orders of magnitude less parameters and pretraining data. On the GenomicBenchmarks, HyenaDNA surpasses SotA on 7 of 8 datasets on average by +10 accuracy points.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2306.15794",
                "caption": "HyenaDNA: Long-Range Genomic Sequence Modeling at Single Nucleotide Resolution",
                "credit": "Eric Nguyen, Michael Poli, Marjan Faizi, Armin W. Thomas, Callum Birch Sykes, Michael Wornow, Aman Patel, Clayton Rabideau, Stefano Massaroli, Yoshua Bengio, Stefano Ermon, Stephen A. Baccus, Christopher Ré"
            }
        },
        {
            "start_date": {
                "year": 2023,
                "month": 6,
                "day": 27
            },
            "text": {
                "headline": "ERNIE 3.5",
                "text": "<p></p>"
            },
            "media": {
                "url": "http://research.baidu.com/Blog/index-view?id=185",
                "caption": "Introducing ERNIE 3.5: Baidu’s Knowledge-Enhanced Foundation Model Takes a Giant Leap Forward",
                "credit": ""
            }
        },
        {
            "start_date": {
                "year": 2023,
                "month": 6,
                "day": 20
            },
            "text": {
                "headline": "RoboCat",
                "text": "<p>The ability to leverage heterogeneous robotic experience from different robots and tasks to quickly master novel skills and embodiments has the potential to transform robot learning. Inspired by recent advances in foundation models for vision and language, we propose a foundation agent for robotic manipulation. This agent, named RoboCat, is a visual goal-conditioned decision transformer capable of consuming multi-embodiment action-labelled visual experience. This data spans a large repertoire of motor control skills from simulated and real robotic arms with varying sets of observations and actions. With RoboCat, we demonstrate the ability to generalise to new tasks and robots, both zero-shot as well as through adaptation using only 100--1000 examples for the target task. We also show how a trained model itself can be used to generate data for subsequent training iterations, thus providing a basic building block for an autonomous improvement loop. We investigate the agent's capabilities, with large-scale evaluations both in simulation and on three different real robot embodiments. We find that as we grow and diversify its training data, RoboCat not only shows signs of cross-task transfer, but also becomes more efficient at adapting to new tasks.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2306.11706",
                "caption": "RoboCat: A Self-Improving Foundation Agent for Robotic Manipulation",
                "credit": "Konstantinos Bousmalis, Giulia Vezzani, Dushyant Rao, Coline Devin, Alex X. Lee, Maria Bauza, Todor Davchev, Yuxiang Zhou, Agrim Gupta, Akhil Raju, Antoine Laurens, Claudio Fantacci, Valentin Dalibard, Martina Zambelli, Murilo Martins, Rugile Pevceviciute, Michiel Blokzijl, Misha Denil, Nathan Batchelor, Thomas Lampe, Emilio Parisotto, Konrad Żołna, Scott Reed, Sergio Gómez Colmenarejo, Jon Scholz, Abbas Abdolmaleki, Oliver Groth, Jean-Baptiste Regli, Oleg Sushkov, Tom Rothörl, José Enrique Chen, Yusuf Aytar, Dave Barker, Joy Ortiz, Martin Riedmiller, Jost Tobias Springenberg, Raia Hadsell, Francesco Nori, Nicolas Heess"
            }
        },
        {
            "start_date": {
                "year": 2023,
                "month": 6,
                "day": 8
            },
            "text": {
                "headline": "MusicGen",
                "text": "<p>We tackle the task of conditional music generation. We introduce MusicGen, a single Language Model (LM) that operates over several streams of compressed discrete music representation, i.e., tokens. Unlike prior work, MusicGen is comprised of a single-stage transformer LM together with efficient token interleaving patterns, which eliminates the need for cascading several models, e.g., hierarchically or upsampling. Following this approach, we demonstrate how MusicGen can generate high-quality samples, while being conditioned on textual description or melodic features, allowing better controls over the generated output. We conduct extensive empirical evaluation, considering both automatic and human studies, showing the proposed approach is superior to the evaluated baselines on a standard text-to-music benchmark. Through ablation studies, we shed light over the importance of each of the components comprising MusicGen. Music samples, code, and models are available at this https URL.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2306.05284",
                "caption": "Simple and Controllable Music Generation",
                "credit": "Jade Copet, Felix Kreuk, Itai Gat, Tal Remez, David Kant, Gabriel Synnaeve, Yossi Adi, Alexandre Défossez"
            }
        },
        {
            "start_date": {
                "year": 2023,
                "month": 6,
                "day": 6
            },
            "text": {
                "headline": "LTM-1",
                "text": "<p>Magic’s LTM-1 enables 50x larger context windows than transformers\nMagic's trained a Large Language Model (LLM) that’s able to take in the gigantic amounts of context when generating suggestions. For our coding assistant, this means Magic can now see your entire repository of code.</p>"
            },
            "media": {
                "url": "https://magic.dev/blog/ltm-1",
                "caption": "LTM-1: an LLM with a 5,000,000 token context window",
                "credit": ""
            }
        },
        {
            "start_date": {
                "year": 2023,
                "month": 5,
                "day": 29
            },
            "text": {
                "headline": "PaLI-X",
                "text": "<p>We present the training recipe and results of scaling up PaLI-X, a multilingual vision and language model, both in terms of size of the components and the breadth of its training task mixture. Our model achieves new levels of performance on a wide-range of varied and complex tasks, including multiple image-based captioning and question-answering tasks, image-based document understanding and few-shot (in-context) learning, as well as object detection, video question answering, and video captioning. PaLI-X advances the state-of-the-art on most vision-and-language benchmarks considered (25+ of them). Finally, we observe emerging capabilities, such as complex counting and multilingual object detection, tasks that are not explicitly in the training mix.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2305.18565",
                "caption": "PaLI-X: On Scaling up a Multilingual Vision and Language Model",
                "credit": "Xi Chen, Josip Djolonga, Piotr Padlewski, Basil Mustafa, Soravit Changpinyo, Jialin Wu, Carlos Riquelme Ruiz, Sebastian Goodman, Xiao Wang, Yi Tay, Siamak Shakeri, Mostafa Dehghani, Daniel Salz, Mario Lucic, Michael Tschannen, Arsha Nagrani, Hexiang Hu, Mandar Joshi, Bo Pang, Ceslee Montgomery, Paulina Pietrzyk, Marvin Ritter, AJ Piergiovanni, Matthias Minderer, Filip Pavetic, Austin Waters, Gang Li, Ibrahim Alabdulmohsin, Lucas Beyer, Julien Amelot, Kenton Lee, Andreas Peter Steiner, Yang Li, Daniel Keysers, Anurag Arnab, Yuanzhong Xu, Keran Rong, Alexander Kolesnikov, Mojtaba Seyedhosseini, Anelia Angelova, Xiaohua Zhai, Neil Houlsby, Radu Soricut"
            }
        },
        {
            "start_date": {
                "year": 2023,
                "month": 5,
                "day": 23
            },
            "text": {
                "headline": "Goat-7B",
                "text": "<p>We introduce Goat, a fine-tuned LLaMA model that significantly outperforms GPT-4 on a range of arithmetic tasks. Fine-tuned on a synthetically generated dataset, Goat achieves state-of-the-art performance on BIG-bench arithmetic sub-task. In particular, the zero-shot Goat-7B matches or even surpasses the accuracy achieved by the few-shot PaLM-540B. Surprisingly, Goat can achieve near-perfect accuracy on large-number addition and subtraction through supervised fine-tuning only, which is almost impossible with previous pretrained language models, such as Bloom, OPT, GPT-NeoX, etc. We attribute Goat's exceptional performance to LLaMA's consistent tokenization of numbers. To tackle more challenging tasks like large-number multiplication and division, we propose an approach that classifies tasks based on their learnability, and subsequently decomposes unlearnable tasks, such as multi-digit multiplication and division, into a series of learnable tasks by leveraging basic arithmetic principles. We thoroughly examine the performance of our model, offering a comprehensive evaluation of the effectiveness of our proposed decomposition steps. Additionally, Goat-7B can be easily trained using LoRA on a 24GB VRAM GPU, facilitating reproducibility for other researchers. We release our model, dataset, and the Python script for dataset generation.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2305.14201",
                "caption": "Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks",
                "credit": "Tiedong Liu, Bryan Kian Hsiang Low"
            }
        },
        {
            "start_date": {
                "year": 2023,
                "month": 5,
                "day": 20
            },
            "text": {
                "headline": "CodeT5+",
                "text": "<p>\"Large language models (LLMs) pretrained on vast source code have achieved prominent progress in code intelligence. However, existing code LLMs have two main limitations in terms of architecture and pretraining tasks. First, they often adopt a specific architecture (encoder-only or decoder-only) or rely on a unified encoder-decoder network for different downstream tasks. The former paradigm is limited by inflexibility in applications while in the latter, the model is treated as a single system for all tasks, leading to suboptimal performance on a subset of tasks. Secondly, they often employ a limited set of pretraining objectives which might not be relevant to some downstream tasks and hence result in substantial performance degrade. To address these limitations, we propose ``CodeT5+'', a family of encoder-decoder LLMs for code in which component modules can be flexibly combined to suit a wide range of downstream code tasks. Such flexibility is enabled by our proposed mixture of pretraining objectives to mitigate the pretrain-finetune discrepancy. These objectives cover span denoising, contrastive learning, text-code matching, and causal LM pretraining tasks, on both unimodal and bimodal multilingual code corpora. Furthermore, we propose to initialize CodeT5+ with frozen off-the-shelf LLMs without training from scratch to efficiently scale up our models, and explore instruction-tuning to align with natural language instructions. We extensively evaluate CodeT5+ on over 20 code-related benchmarks in different settings, including zero-shot, finetuning, and instruction-tuning. We observe state-of-the-art (SoTA) model performance on various code-related tasks, such as code generation and completion, math programming, and text-to-code retrieval tasks. Particularly, our instruction-tuned CodeT5+ 16B achieves new SoTA results on HumanEval code generation task against other open code LLMs.\"</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2305.07922",
                "caption": "CodeT5+: Open Code Large Language Models for Code Understanding and Generation",
                "credit": "Yue Wang, Hung Le, Akhilesh Deepak Gotmare, Nghi D.Q. Bui, Junnan Li, Steven C.H. Hoi"
            }
        },
        {
            "start_date": {
                "year": 2023,
                "month": 5,
                "day": 18
            },
            "text": {
                "headline": "ONE-PEACE",
                "text": "<p>In this work, we explore a scalable way for building a general representation model toward unlimited modalities. We release ONE-PEACE, a highly extensible model with 4B parameters that can seamlessly align and integrate representations across vision, audio, and language modalities. The architecture of ONE-PEACE comprises modality adapters, shared self-attention layers, and modality FFNs. This design allows for the easy extension of new modalities by adding adapters and FFNs, while also enabling multi-modal fusion through self-attention layers. To pretrain ONE-PEACE, we develop two modality-agnostic pretraining tasks, cross-modal aligning contrast and intra-modal denoising contrast, which align the semantic space of different modalities and capture fine-grained details within modalities concurrently. With the scaling-friendly architecture and pretraining tasks, ONE-PEACE has the potential to expand to unlimited modalities. Without using any vision or language pretrained model for initialization, ONE-PEACE achieves leading results on a wide range of uni-modal and multi-modal tasks, including image classification (ImageNet), semantic segmentation (ADE20K), audio-text retrieval (AudioCaps, Clotho), audio classification (ESC-50, FSD50K, VGGSound), audio question answering (AVQA), image-text retrieval (MSCOCO, Flickr30K), and visual grounding (RefCOCO/+/g). Code is available at this https URL.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2305.11172v1",
                "caption": "ONE-PEACE: Exploring One General Representation Model Toward Unlimited Modalities",
                "credit": "Peng Wang, Shijie Wang, Junyang Lin, Shuai Bai, Xiaohuan Zhou, Jingren Zhou, Xinggang Wang, Chang Zhou"
            }
        },
        {
            "start_date": {
                "year": 2023,
                "month": 5,
                "day": 18
            },
            "text": {
                "headline": "LIMA",
                "text": "<p>Large language models are trained in two stages: (1) unsupervised pretraining from raw text, to learn general-purpose representations, and (2) large scale instruction tuning and reinforcement learning, to better align to end tasks and user preferences. We measure the relative importance of these two stages by training LIMA, a 65B parameter LLaMa language model fine-tuned with the standard supervised loss on only 1,000 carefully curated prompts and responses, without any reinforcement learning or human preference modeling. LIMA demonstrates remarkably strong performance, learning to follow specific response formats from only a handful of examples in the training data, including complex queries that range from planning trip itineraries to speculating about alternate history. Moreover, the model tends to generalize well to unseen tasks that did not appear in the training data. In a controlled human study, responses from LIMA are either equivalent or strictly preferred to GPT-4 in 43% of cases; this statistic is as high as 58% when compared to Bard and 65% versus DaVinci003, which was trained with human feedback. Taken together, these results strongly suggest that almost all knowledge in large language models is learned during pretraining, and only limited instruction tuning data is necessary to teach models to produce high quality output.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2305.11206",
                "caption": "LIMA: Less Is More for Alignment",
                "credit": "Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer, Omer Levy"
            }
        },
        {
            "start_date": {
                "year": 2023,
                "month": 5,
                "day": 17
            },
            "text": {
                "headline": "CoEdiT-xxl",
                "text": "<p>We introduce COEDIT, a state-of-the-art text editing system for writing assistance. COEDIT takes instructions from the user specifying the attributes of the desired text, such as \"Make the sentence simpler\" or \"Write it in a more neutral style,\" and outputs the edited text. We present a large language model fine-tuned on a diverse collection of task-specific instructions for text editing (a total of 82K instructions). Our model (1) achieves state-of-the-art performance on various text editing benchmarks, (2) is competitive with publicly available largestsized LLMs trained on instructions while being ∼60x smaller, (3) is capable of generalizing to unseen edit instructions, and (4) exhibits abilities to generalize to composite instructions containing different combinations of edit actions. Through extensive qualitative and quantitative analysis, we show that writers prefer the edits suggested by COEDIT, relative to other stateof-the-art text editing models1.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2305.09857, https://huggingface.co/grammarly/coedit-large",
                "caption": "CoEdIT: Text Editing by Task-Specific Instruction Tuning",
                "credit": "Vipul Raheja, Dhruv Kumar, Ryan Koo, Dongyeop Kang"
            }
        },
        {
            "start_date": {
                "year": 2023,
                "month": 5,
                "day": 16
            },
            "text": {
                "headline": "Med-PaLM 2",
                "text": "<p>Recent artificial intelligence (AI) systems have reached milestones in \"grand challenges\" ranging from Go to protein-folding. The capability to retrieve medical knowledge, reason over it, and answer medical questions comparably to physicians has long been viewed as one such grand challenge.\nLarge language models (LLMs) have catalyzed significant progress in medical question answering; Med-PaLM was the first model to exceed a \"passing\" score in US Medical Licensing Examination (USMLE) style questions with a score of 67.2% on the MedQA dataset. However, this and other prior work suggested significant room for improvement, especially when models' answers were compared to clinicians' answers. Here we present Med-PaLM 2, which bridges these gaps by leveraging a combination of base LLM improvements (PaLM 2), medical domain finetuning, and prompting strategies including a novel ensemble refinement approach.\nMed-PaLM 2 scored up to 86.5% on the MedQA dataset, improving upon Med-PaLM by over 19% and setting a new state-of-the-art. We also observed performance approaching or exceeding state-of-the-art across MedMCQA, PubMedQA, and MMLU clinical topics datasets.\nWe performed detailed human evaluations on long-form questions along multiple axes relevant to clinical applications. In pairwise comparative ranking of 1066 consumer medical questions, physicians preferred Med-PaLM 2 answers to those produced by physicians on eight of nine axes pertaining to clinical utility (p < 0.001). We also observed significant improvements compared to Med-PaLM on every evaluation axis (p < 0.001) on newly introduced datasets of 240 long-form \"adversarial\" questions to probe LLM limitations.\nWhile further studies are necessary to validate the efficacy of these models in real-world settings, these results highlight rapid progress towards physician-level performance in medical question answering.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2305.09617",
                "caption": "Towards Expert-Level Medical Question Answering with Large Language Models",
                "credit": "Karan Singhal, Tao Tu, Juraj Gottweis, Rory Sayres, Ellery Wulczyn, Le Hou, Kevin Clark, Stephen Pfohl, Heather Cole-Lewis, Darlene Neal, Mike Schaekermann, Amy Wang, Mohamed Amin, Sami Lachgar, Philip Mansfield, Sushant Prakash, Bradley Green, Ewa Dominowska, Blaise Aguera y Arcas, Nenad Tomasev, Yun Liu, Renee Wong, Christopher Semturs, S. Sara Mahdavi, Joelle Barral, Dale Webster, Greg S. Corrado, Yossi Matias, Shekoofeh Azizi, Alan Karthikesalingam, Vivek Natarajan"
            }
        },
        {
            "start_date": {
                "year": 2023,
                "month": 5,
                "day": 11
            },
            "text": {
                "headline": "InstructBLIP",
                "text": "<p>Large-scale pre-training and instruction tuning have been successful at creating general-purpose language models with broad competence. However, building general-purpose vision-language models is challenging due to the rich input distributions and task diversity resulting from the additional visual input. Although vision-language pretraining has been widely studied, vision-language instruction tuning remains under-explored. In this paper, we conduct a systematic and comprehensive study on vision-language instruction tuning based on the pretrained BLIP-2 models. We gather 26 publicly available datasets, covering a wide variety of tasks and capabilities, and transform them into instruction tuning format. Additionally, we introduce an instruction-aware Query Transformer, which extracts informative features tailored to the given instruction. Trained on 13 held-in datasets, InstructBLIP attains state-of-the-art zero-shot performance across all 13 held-out datasets, substantially outperforming BLIP-2 and larger Flamingo models. Our models also lead to state-of-the-art performance when finetuned on individual downstream tasks (e.g., 90.7% accuracy on ScienceQA questions with image contexts). Furthermore, we qualitatively demonstrate the advantages of InstructBLIP over concurrent multimodal models. All InstructBLIP models are open-sourced at this https URL. </p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2305.06500",
                "caption": "InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning",
                "credit": "Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, Steven Hoi"
            }
        },
        {
            "start_date": {
                "year": 2023,
                "month": 5,
                "day": 10
            },
            "text": {
                "headline": "PaLM 2",
                "text": "<p>We introduce PaLM 2, a new state-of-the-art language model that has better multilingual and reasoning capabilities and is more compute-efficient than its predecessor PaLM (Chowdhery et al., 2022). PaLM 2 is a Transformer-based model trained using a mixture of objectives similar to UL2 (Tay et al., 2023). Through extensive evaluations on English and multilingual language, and reasoning tasks, we demonstrate that PaLM 2 has significantly improved quality on downstream tasks across different model sizes, while simultaneously exhibiting faster and more efficient inference compared to PaLM. This improved efficiency enables broader deployment while also allowing the model to respond faster, for a more natural pace of interaction. PaLM 2 demonstrates robust reasoning capabilities exemplified by large improvements over PaLM on BIG-Bench and other reasoning tasks. PaLM 2 exhibits stable performance on a suite of responsible AI evaluations, and enables inference-time control over toxicity without additional overhead or impact on other capabilities. Overall, PaLM 2 achieves state-of-the-art performance across a diverse set of tasks and capabilities.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2305.10403",
                "caption": "PaLM 2 Technical Report",
                "credit": "Andrew M. Dai, David R. So, Dmitry Lepikhin, Jonathan H. Clark, Maxim Krikun, Melvin Johnson, Nan Du, Rohan Anil, Siamak Shakeri, Xavier Garcia, Yanping Huang, Yi Tay, Yong Cheng, Yonghui Wu, Yuanzhong Xu, Yujing Zhang, Zachary Nado, Bryan Richter, Alex Polozov, Andrew Nystrom, Fangxiaoyu Feng, Hanzhao Lin, Jacob Austin, Jacob Devlin, Kefan Xiao, Orhan Firat, Parker Riley, Steven Zheng, Yuhuai Wu, Zhongtao Liu, Jiahui Yu, Guy Gur-Ari, Weikang Zhou, Sneha Kudugunta, Sunipa Dev, Frederick Liu, Gustavo Hernandez Abrego, Kelvin Xu, Abe Ittycheriah, Daniel Sohn, John Nham, Le Hou, Siyuan Qiao, Pidong Wang, Zirui Wang, Laurent El Shafey, Hyeontaek Lim, Marcello Maggioni, Michael Isard, Paul Barham, Qiao Zhang, Tao Wang, Yash Katariya, Aurko Roy, Benjamin Lee, Brennan Saeta, Ce Zheng, Hadi Hashemi, Junwhan Ahn, Rajkumar Samuel, Steven Hand, Zhifeng Chen, Kiran Vodrahalli, Aakanksha Chowdhery, Ethan Dyer, Emanuel Taropa, Vlad Feinberg, James Bradbury, Reiner Pope, Wei Li, YaGuang Li, Eric Chu, Jeffrey Hui, Joshua Howland, Vlad Fienber, Aroma Mahendru, Michele Catasta, Vedant Misra, Kevin Robinson, Maysam Moussalem, Sebastian Ruder, Erica Moreira, Eric Ni, Paige Bailey, Lucas Gonzalez, Alexandre Passos, Slav Petrov, Gaurav Mishra, Mark Omernick, Ambrose Slone, Andrea Hu, Colin Cherry, Denny Zhou, Jan Botha, John Wieting, Joshua Maynez, Kathleen Kenealy, Kevin Brooks, Linting Xue, Markus Freitag, Martin Polacek, Pengcheng Yin, Sebastian Gehrmann, Xuezhi Wang, Kathy Meier-Hellstern, Christopher A. Choquette-Choo, Daniel Smilkov, Emily Reif, Alicia Parrish, Alex Castro Ros, Clément Crepy, Dasha Valter, Jeremy Hurwitz, Katherine Lee, Mark Díaz, Marie Pellat, Matthew Jagielski, Renee Shelby, Shachi Dave"
            }
        },
        {
            "start_date": {
                "year": 2023,
                "month": 5,
                "day": 9
            },
            "text": {
                "headline": "StarCoder",
                "text": "<p>\"The BigCode community, an open-scientific collaboration working on the responsible development of Large Language Models for Code (Code LLMs), introduces StarCoder and StarCoderBase: 15.5B parameter models with 8K context length, infilling capabilities and fast large-batch inference enabled by multi-query attention. StarCoderBase is trained on 1 trillion tokens sourced from The Stack, a large collection of permissively licensed GitHub repositories with inspection tools and an opt-out process. We fine-tuned StarCoderBase on 35B Python tokens, resulting in the creation of StarCoder. We perform the most comprehensive evaluation of Code LLMs to date and show that StarCoderBase outperforms every open Code LLM that supports multiple programming languages and matches or outperforms the OpenAI code-cushman-001 model. Furthermore, StarCoder outperforms every model that is fine-tuned on Python, can be prompted to achieve 40\\% pass@1 on HumanEval, and still retains its performance on other programming languages. We take several important steps towards a safe open-access model release, including an improved PII redaction pipeline and a novel attribution tracing tool, and make the StarCoder models publicly available under a more commercially viable version of the Open Responsible AI Model license.\"</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2305.06161",
                "caption": "StarCoder: may the source be with you!",
                "credit": "Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, Qian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier Dehaene, Mishig Davaadorj, Joel Lamy-Poirier, João Monteiro, Oleh Shliazhko, Nicolas Gontier, Nicholas Meade, Armel Zebaze, Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu, Benjamin Lipkin, Muhtasham Oblokulov, Zhiruo Wang, Rudra Murthy, Jason Stillerman, Siva Sankalp Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang, Nour Fahmy, Urvashi Bhattacharyya, Wenhao Yu, Swayam Singh, Sasha Luccioni, Paulo Villegas, Maxim Kunakov, Fedor Zhdanov, Manuel Romero, Tony Lee, Nadav Timor, Jennifer Ding, Claire Schlesinger, Hailey Schoelkopf, Jan Ebert, Tri Dao, Mayank Mishra, Alex Gu, Jennifer Robinson, Carolyn Jane Anderson, Brendan Dolan-Gavitt, Danish Contractor, Siva Reddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos Muñoz Ferrandis, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, Harm de Vries"
            }
        },
        {
            "start_date": {
                "year": 2023,
                "month": 5,
                "day": 9
            },
            "text": {
                "headline": "ImageBind",
                "text": "<p>We present ImageBind, an approach to learn a joint embedding across six different modalities - images, text, audio, depth, thermal, and IMU data. We show that all combinations of paired data are not necessary to train such a joint embedding, and only image-paired data is sufficient to bind the modalities together. ImageBind can leverage recent large scale vision-language models, and extends their zero-shot capabilities to new modalities just by using their natural pairing with images. It enables novel emergent applications 'out-of-the-box' including cross-modal retrieval, composing modalities with arithmetic, cross-modal detection and generation. The emergent capabilities improve with the strength of the image encoder and we set a new state-of-the-art on emergent zero-shot recognition tasks across modalities, outperforming specialist supervised models. Finally, we show strong few-shot recognition results outperforming prior work, and that ImageBind serves as a new way to evaluate vision models for visual and non-visual tasks.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2305.05665, https://github.com/facebookresearch/ImageBind",
                "caption": "IMAGEBIND: One Embedding Space To Bind Them All",
                "credit": "Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala, Armand Joulin, Ishan Misra"
            }
        },
        {
            "start_date": {
                "year": 2023,
                "month": 4,
                "day": 26
            },
            "text": {
                "headline": "Agile Soccer Robot",
                "text": "<p>We investigate whether Deep Reinforcement Learning (Deep RL) is able to synthesize sophisticated and safe movement skills for a low-cost, miniature humanoid robot that can be composed into complex behavioral strategies in dynamic environments. We used Deep RL to train a humanoid robot with 20 actuated joints to play a simplified one-versus-one (1v1) soccer game. We first trained individual skills in isolation and then composed those skills end-to-end in a self-play setting. The resulting policy exhibits robust and dynamic movement skills such as rapid fall recovery, walking, turning, kicking and more; and transitions between them in a smooth, stable, and efficient manner - well beyond what is intuitively expected from the robot. The agents also developed a basic strategic understanding of the game, and learned, for instance, to anticipate ball movements and to block opponent shots. The full range of behaviors emerged from a small set of simple rewards. Our agents were trained in simulation and transferred to real robots zero-shot. We found that a combination of sufficiently high-frequency control, targeted dynamics randomization, and perturbations during training in simulation enabled good-quality transfer, despite significant unmodeled effects and variations across robot instances. Although the robots are inherently fragile, minor hardware modifications together with basic regularization of the behavior during training led the robots to learn safe and effective movements while still performing in a dynamic and agile way. Indeed, even though the agents were optimized for scoring, in experiments they walked 156% faster, took 63% less time to get up, and kicked 24% faster than a scripted baseline, while efficiently combining the skills to achieve the longer term objectives. Examples of the emergent behaviors and full 1v1 matches are available on the supplementary website.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2304.13653",
                "caption": "Learning Agile Soccer Skills for a Bipedal Robot with Deep Reinforcement Learning",
                "credit": "Tuomas Haarnoja, Ben Moran, Guy Lever, Sandy H. Huang, Dhruva Tirumala, Markus Wulfmeier, Jan Humplik, Saran Tunyasuvunakool, Noah Y. Siegel, Roland Hafner, Michael Bloesch, Kristian Hartikainen, Arunkumar Byravan, Leonard Hasenclever, Yuval Tassa, Fereshteh Sadeghi, Nathan Batchelor, Federico Casarini, Stefano Saliceti, Charles Game, Neil Sreendra, Kushal Patel, Marlon Gwira, Andrea Huber, Nicole Hurley, Francesco Nori, Raia Hadsell, Nicolas Heess"
            }
        },
        {
            "start_date": {
                "year": 2023,
                "month": 4,
                "day": 17
            },
            "text": {
                "headline": "LLaVA",
                "text": "<p>Instruction tuning large language models (LLMs) using machine-generated instruction-following data has improved zero-shot capabilities on new tasks, but the idea is less explored in the multimodal field. In this paper, we present the first attempt to use language-only GPT-4 to generate multimodal language-image instruction-following data. By instruction tuning on such generated data, we introduce LLaVA: Large Language and Vision Assistant, an end-to-end trained large multimodal model that connects a vision encoder and LLM for general-purpose visual and language understanding.Our early experiments show that LLaVA demonstrates impressive multimodel chat abilities, sometimes exhibiting the behaviors of multimodal GPT-4 on unseen images/instructions, and yields a 85.1% relative score compared with GPT-4 on a synthetic multimodal instruction-following dataset. When fine-tuned on Science QA, the synergy of LLaVA and GPT-4 achieves a new state-of-the-art accuracy of 92.53%. We make GPT-4 generated visual instruction tuning data, our model and code base publicly available. </p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2304.08485",
                "caption": "Visual Instruction Tuning",
                "credit": "Haotian Liu, Chunyuan Li, Qingyang Wu, Yong Jae Lee"
            }
        },
        {
            "start_date": {
                "year": 2023,
                "month": 4,
                "day": 14
            },
            "text": {
                "headline": "DINOv2",
                "text": "<p>The recent breakthroughs in natural language processing for model pretraining on large quantities of data have opened the way for similar foundation models in computer vision. These models could greatly simplify the use of images in any system by producing all-purpose visual features, i.e., features that work across image distributions and tasks without finetuning. This work shows that existing pretraining methods, especially self-supervised methods, can produce such features if trained on enough curated data from diverse sources. We revisit existing approaches and combine different techniques to scale our pretraining in terms of data and model size. Most of the technical contributions aim at accelerating and stabilizing the training at scale. In terms of data, we propose an automatic pipeline to build a dedicated, diverse, and curated image dataset instead of uncurated data, as typically done in the self-supervised literature. In terms of models, we train a ViT model (Dosovitskiy et al., 2020) with 1B parameters and distill it into a series of smaller models that surpass the best available all-purpose features, OpenCLIP (Ilharco et al., 2021) on most of the benchmarks at image and pixel levels. </p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2304.07193",
                "caption": "DINOv2: Learning Robust Visual Features without Supervision",
                "credit": "Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mahmoud Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Hervé Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, Piotr Bojanowski"
            }
        },
        {
            "start_date": {
                "year": 2023,
                "month": 4,
                "day": 9
            },
            "text": {
                "headline": "Incoder-6.7B",
                "text": "<p>Code is seldom written in a single left-to-right pass and is instead repeatedly edited and refined. We introduce InCoder, a unified generative model that can perform program synthesis (via left-to-right generation) as well as editing (via infilling). InCoder is trained to generate code files from a large corpus of permissively licensed code, where regions of code have been randomly masked and moved to the end of each file, allowing code infilling with bidirectional context. Our model is the first generative model that is able to directly perform zero-shot code infilling, which we evaluate on challenging tasks such as type inference, comment generation, and variable re-naming. We find that the ability to condition on bidirectional context substantially improves performance on these tasks, while still performing comparably on standard program synthesis benchmarks in comparison to left-to-right only models pretrained at similar scale. The InCoder models and code are publicly released. this https URL</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2204.05999",
                "caption": "InCoder: A Generative Model for Code Infilling and Synthesis",
                "credit": "Daniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric Wallace, Freda Shi, Ruiqi Zhong, Wen-tau Yih, Luke Zettlemoyer, Mike Lewis"
            }
        },
        {
            "start_date": {
                "year": 2023,
                "month": 4,
                "day": 5
            },
            "text": {
                "headline": "Segment Anything Model",
                "text": "<p>We introduce the Segment Anything (SA) project: a new task, model, and dataset for image segmentation. Using our efficient model in a data collection loop, we built the largest segmentation dataset to date (by far), with over 1 billion masks on 11M licensed and privacy respecting images. The model is designed and trained to be promptable, so it can transfer zero-shot to new image distributions and tasks. We evaluate its capabilities on numerous tasks and find that its zero-shot performance is impressive -- often competitive with or even superior to prior fully supervised results. We are releasing the Segment Anything Model (SAM) and corresponding dataset (SA-1B) of 1B masks and 11M images at this https URL to foster research into foundation models for computer vision.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2304.02643",
                "caption": "Segment Anything",
                "credit": "Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollár, Ross Girshick"
            }
        },
        {
            "start_date": {
                "year": 2023,
                "month": 3,
                "day": 30
            },
            "text": {
                "headline": "BloombergGPT",
                "text": "<p>The use of NLP in the realm of financial technology is broad and complex, with applications ranging from sentiment analysis and named entity recognition to question answering. Large Language Models (LLMs) have been shown to be effective on a variety of tasks; however, no LLM specialized for the financial domain has been reported in literature. In this work, we present BloombergGPT, a 50 billion parameter language model that is trained on a wide range of financial data. We construct a 363 billion token dataset based on Bloomberg's extensive data sources, perhaps the largest domain-specific dataset yet, augmented with 345 billion tokens from general purpose datasets. We validate BloombergGPT on standard LLM benchmarks, open financial benchmarks, and a suite of internal benchmarks that most accurately reflect our intended usage. Our mixed dataset training leads to a model that outperforms existing models on financial tasks by significant margins without sacrificing performance on general LLM benchmarks. Additionally, we explain our modeling choices, training process, and evaluation methodology. We release Training Chronicles (Appendix C) detailing our experience in training BloombergGPT.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2303.17564",
                "caption": "BloombergGPT: A Large Language Model for Finance",
                "credit": "Shijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski, Mark Dredze, Sebastian Gehrmann, Prabhanjan Kambadur, David Rosenberg, Gideon Mann"
            }
        },
        {
            "start_date": {
                "year": 2023,
                "month": 3,
                "day": 29
            },
            "text": {
                "headline": "VideoMAE V2",
                "text": "<p>Scale is the primary factor for building a powerful foundation model that could well generalize to a variety of downstream tasks. However, it is still challenging to train video foundation models with billions of parameters. This paper shows that video masked autoencoder (VideoMAE) is a scalable and general self-supervised pre-trainer for building video foundation models. We scale the VideoMAE in both model and data with a core design. Specifically, we present a dual masking strategy for efficient pre-training, with an encoder operating on a subset of video tokens and a decoder processing another subset of video tokens. Although VideoMAE is very efficient due to high masking ratio in encoder, masking decoder can still further reduce the overall computational cost. This enables the efficient pre-training of billion-level models in video. We also use a progressive training paradigm that involves an initial pre-training on a diverse multi-sourced unlabeled dataset, followed by a post-pre-training on a mixed labeled dataset. Finally, we successfully train a video ViT model with a billion parameters, which achieves a new state-of-the-art performance on the datasets of Kinetics (90.0% on K400 and 89.9% on K600) and Something-Something (68.7% on V1 and 77.0% on V2). In addition, we extensively verify the pre-trained video ViT models on a variety of downstream tasks, demonstrating its effectiveness as a general video representation learner. The code and model is available at \\url{this https URL}.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2303.16727v2",
                "caption": "VideoMAE V2: Scaling Video Masked Autoencoders with Dual Masking",
                "credit": "Limin Wang, Bingkun Huang, Zhiyu Zhao, Zhan Tong, Yinan He, Yi Wang, Yali Wang, Yu Qiao"
            }
        },
        {
            "start_date": {
                "year": 2023,
                "month": 3,
                "day": 27
            },
            "text": {
                "headline": "SigLIP 400M",
                "text": "<p>We propose a simple pairwise Sigmoid loss for Language-Image Pre-training (SigLIP). Unlike standard contrastive learning with softmax normalization, the sigmoid loss operates solely on image-text pairs and does not require a global view of the pairwise similarities for normalization. The sigmoid loss simultaneously allows further scaling up the batch size, while also performing better at smaller batch sizes. Combined with Locked-image Tuning, with only four TPUv4 chips, we train a SigLiT model that achieves 84.5% ImageNet zero-shot accuracy in two days. The disentanglement of the batch size from the loss further allows us to study the impact of examples vs pairs and negative to positive ratio. Finally, we push the batch size to the extreme, up to one million, and find that the benefits of growing batch size quickly diminish, with a more reasonable batch size of 32k being sufficient. We release our models at this https URL and hope our research motivates further explorations in improving the quality and efficiency of language-image pre-training.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2303.15343",
                "caption": "Sigmoid Loss for Language Image Pre-Training",
                "credit": "Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, Lucas Beyer"
            }
        },
        {
            "start_date": {
                "year": 2023,
                "month": 3,
                "day": 21
            },
            "text": {
                "headline": "Firefly",
                "text": "<p>Today, Adobe (Nasdaq:ADBE) introduced Adobe Firefly, a new family of creative generative AI models, first focused on the generation of images and text effects. Adobe Firefly will bring even more precision, power, speed and ease directly into Creative Cloud, Document Cloud, Experience Cloud and Adobe Express workflows where content is created and modified. Adobe Firefly will be part of a series of new Adobe Sensei generative AI services across Adobe’s clouds.</p>"
            },
            "media": {
                "url": "https://news.adobe.com/news/news-details/2023/Adobe-Unveils-Firefly-a-Family-of-new-Creative-Generative-AI/default.aspx",
                "caption": "Adobe Unveils Firefly, a Family of new Creative Generative AI",
                "credit": ""
            }
        },
        {
            "start_date": {
                "year": 2023,
                "month": 3,
                "day": 20
            },
            "text": {
                "headline": "PanGu-Σ",
                "text": "<p>The scaling of large language models has greatly improved natural language understanding, generation, and reasoning. In this work, we develop a system that trained a trillion-parameter language model on a cluster of Ascend 910 AI processors and MindSpore framework, and present the language model with 1.085T parameters named PanGu-{\\Sigma}. With parameter inherent from PanGu-{\\alpha}, we extend the dense Transformer model to sparse one with Random Routed Experts (RRE), and efficiently train the model over 329B tokens by using Expert Computation and Storage Separation(ECSS). This resulted in a 6.3x increase in training throughput through heterogeneous computing. Our experimental findings show that PanGu-{\\Sigma} provides state-of-the-art performance in zero-shot learning of various Chinese NLP downstream tasks. Moreover, it demonstrates strong abilities when fine-tuned in application data of open-domain dialogue, question answering, machine translation and code generation.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2303.10845",
                "caption": "PanGu-Σ: Towards Trillion Parameter Language Model with Sparse Heterogeneous Computing",
                "credit": "Xiaozhe Ren, Pingyi Zhou, Xinfan Meng, Xinjing Huang, Yadao Wang, Weichao Wang, Pengfei Li, Xiaoda Zhang, Alexander Podolskiy, Grigory Arshinov, Andrey Bout, Irina Piontkovskaya, Jiansheng Wei, Xin Jiang, Teng Su, Qun Liu, Jun Yao"
            }
        },
        {
            "start_date": {
                "year": 2023,
                "month": 3,
                "day": 20
            },
            "text": {
                "headline": "Gen-2",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://research.runwayml.com/gen2",
                "caption": "",
                "credit": "Gen-2 authors"
            }
        },
        {
            "start_date": {
                "year": 2023,
                "month": 3,
                "day": 15
            },
            "text": {
                "headline": "LEP-AD",
                "text": "<p>Predicting drug-target interactions is a tremendous challenge for drug development and lead optimization. Recent advances include training algorithms to learn drug-target interactions from data and molecular simulations. Here we utilize Evolutionary Scale Modeling (ESM-2) models to establish a Transformer protein language model for drug-target interaction predictions. Our architecture, LEPAD, combines pre-trained ESM-2 and Transformer-GCN models predicting binding affinity values. We report new best-in-class state-of-the-art results compared to competing methods such as SimBoost, DeepCPI, Attention-DTA, GraphDTA, and more using multiple datasets, including Davis, KIBA, DTC, Metz, ToxCast, and STITCH. Finally, we find that a pre-trained model with embedding of proteins (the LED-AD) outperforms a model using an explicit alpha-fold 3D representation of proteins (e.g., LEP-AD supervised by Alphafold). The LEP-AD model\nscales favorably in performance with the size of training data. Code available at https://github.com/adaga06/LEP-AD</p>"
            },
            "media": {
                "url": "https://www.biorxiv.org/content/10.1101/2023.03.14.532563v1.full.pdf",
                "caption": "LEP-AD: Language Embedding of Proteins and Attention to Drugs predicts Drug Target Interactions",
                "credit": "Anuj Daga, Sumeer Ahmad Khan, David Gomez Cabrero, Robert Hoehndorf, Narsis A. Kiani, Jesper Tegner"
            }
        },
        {
            "start_date": {
                "year": 2023,
                "month": 3,
                "day": 15
            },
            "text": {
                "headline": "GPT-4",
                "text": "<p>We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2303.08774",
                "caption": "GPT-4 Technical Report",
                "credit": "OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mohammad Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu, Hyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix, Simón Posada Fishman, Juston Forte, Isabella Fulford, Leo Gao, Elie Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes, Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn Jain et al. (181 additional authors not shown)"
            }
        },
        {
            "start_date": {
                "year": 2023,
                "month": 3,
                "day": 15
            },
            "text": {
                "headline": "Falcon-40B",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2311.16867; https://www.tii.ae/news/abu-dhabi-based-technology-innovation-institute-introduces-falcon-llm-foundational-large",
                "caption": "Abu Dhabi-based Technology Innovation Institute Introduces Falcon LLM: Foundational Large Language Model (LLM) outperforms GPT-3 with 40 Billion Parameters",
                "credit": ""
            }
        },
        {
            "start_date": {
                "year": 2023,
                "month": 3,
                "day": 14
            },
            "text": {
                "headline": "Claude",
                "text": "<p>Claude is a next-generation AI assistant based on Anthropic’s research into training helpful, honest, and harmless AI systems. Accessible through chat interface and API in our developer console, Claude is capable of a wide variety of conversational and text processing tasks while maintaining a high degree of reliability and predictability.</p>"
            },
            "media": {
                "url": "https://www.anthropic.com/index/introducing-claude",
                "caption": "Introducing Claude",
                "credit": ""
            }
        },
        {
            "start_date": {
                "year": 2023,
                "month": 3,
                "day": 6
            },
            "text": {
                "headline": "PaLM-E",
                "text": "<p>Large language models excel at a wide range of complex tasks. However, enabling general inference in the real world, e.g., for robotics problems, raises the challenge of grounding. We propose embodied language models to directly incorporate real-world continuous sensor modalities into language models and thereby establish the link between words and percepts. Input to our embodied language model are multi-modal sentences that interleave visual, continuous state estimation, and textual input encodings. We train these encodings end-to-end, in conjunction with a pre-trained large language model, for multiple embodied tasks including sequential robotic manipulation planning, visual question answering, and captioning. Our evaluations show that PaLM-E, a single large embodied multimodal model, can address a variety of embodied reasoning tasks, from a variety of observation modalities, on multiple embodiments, and further, exhibits positive transfer: the model benefits from diverse joint training across internet-scale language, vision, and visual-language domains. Our largest model, PaLM-E-562B with 562B parameters, in addition to being trained on robotics tasks, is a visual-language generalist with state-of-the-art performance on OK-VQA, and retains generalist language capabilities with increasing scale.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2303.03378",
                "caption": "PaLM-E: An Embodied Multimodal Language Model",
                "credit": "Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong Huang, Yevgen Chebotar, Pierre Sermanet, Daniel Duckworth, Sergey Levine, Vincent Vanhoucke, Karol Hausman, Marc Toussaint, Klaus Greff, Andy Zeng, Igor Mordatch, Pete Florence"
            }
        },
        {
            "start_date": {
                "year": 2023,
                "month": 3,
                "day": 5
            },
            "text": {
                "headline": "AudioGen",
                "text": "<p>We tackle the problem of generating audio samples conditioned on descriptive text captions. In this work, we propose AaudioGen, an auto-regressive generative model that generates audio samples conditioned on text inputs. AudioGen operates on a learnt discrete audio representation. The task of text-to-audio generation poses multiple challenges. Due to the way audio travels through a medium, differentiating ``objects'' can be a difficult task (e.g., separating multiple people simultaneously speaking). This is further complicated by real-world recording conditions (e.g., background noise, reverberation, etc.). Scarce text annotations impose another constraint, limiting the ability to scale models. Finally, modeling high-fidelity audio requires encoding audio at high sampling rate, leading to extremely long sequences. To alleviate the aforementioned challenges we propose an augmentation technique that mixes different audio samples, driving the model to internally learn to separate multiple sources. We curated 10 datasets containing different types of audio and text annotations to handle the scarcity of text-audio data points. For faster inference, we explore the use of multi-stream modeling, allowing the use of shorter sequences while maintaining a similar bitrate and perceptual quality. We apply classifier-free guidance to improve adherence to text. Comparing to the evaluated baselines, AudioGen outperforms over both objective and subjective metrics. Finally, we explore the ability of the proposed method to generate audio continuation conditionally and unconditionally. Samples: this https URL</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2209.15352",
                "caption": "AudioGen: Textually Guided Audio Generation",
                "credit": "Felix Kreuk, Gabriel Synnaeve, Adam Polyak, Uriel Singer, Alexandre Défossez, Jade Copet, Devi Parikh, Yaniv Taigman, Yossi Adi"
            }
        },
        {
            "start_date": {
                "year": 2023,
                "month": 3,
                "day": 2
            },
            "text": {
                "headline": "DiT-XL/2",
                "text": "<p>We explore a new class of diffusion models based on the transformer architecture. We train latent diffusion models of images, replacing the commonly-used U-Net backbone with a transformer that operates on latent patches. We analyze the scalability of our Diffusion Transformers (DiTs) through the lens of forward pass complexity as measured by Gflops. We find that DiTs with higher Gflops -- through increased transformer depth/width or increased number of input tokens -- consistently have lower FID. In addition to possessing good scalability properties, our largest DiT-XL/2 models outperform all prior diffusion models on the class-conditional ImageNet 512x512 and 256x256 benchmarks, achieving a state-of-the-art FID of 2.27 on the latter.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2212.09748",
                "caption": "Scalable Diffusion Models with Transformers",
                "credit": "William Peebles, Saining Xie"
            }
        },
        {
            "start_date": {
                "year": 2023,
                "month": 2,
                "day": 24
            },
            "text": {
                "headline": "LLaMA-65B",
                "text": "<p>We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2302.13971",
                "caption": "LLaMA: Open and Efficient Foundation Language Models",
                "credit": "Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample"
            }
        },
        {
            "start_date": {
                "year": 2023,
                "month": 2,
                "day": 13
            },
            "text": {
                "headline": "BASIC-L + Lion",
                "text": "<p>We present a method to formulate algorithm discovery as program search, and apply it to discover optimization algorithms for deep neural network training. We leverage efficient search techniques to explore an infinite and sparse program space. To bridge the large generalization gap between proxy and target tasks, we also introduce program selection and simplification strategies. Our method discovers a simple and effective optimization algorithm, Lion (EvoLved Sign Momentum). It is more memory-efficient than Adam as it only keeps track of the momentum. Different from adaptive optimizers, its update has the same magnitude for each parameter calculated through the sign operation. We compare Lion with widely used optimizers, such as Adam and Adafactor, for training a variety of models on different tasks. On image classification, Lion boosts the accuracy of ViT by up to 2% on ImageNet and saves up to 5x the pre-training compute on JFT. On vision-language contrastive learning, we achieve 88.3% zero-shot and 91.1% fine-tuning accuracy on ImageNet, surpassing the previous best results by 2% and 0.1%, respectively. On diffusion models, Lion outperforms Adam by achieving a better FID score and reducing the training compute by up to 2.3x. For autoregressive, masked language modeling, and fine-tuning, Lion exhibits a similar or better performance compared to Adam. Our analysis of Lion reveals that its performance gain grows with the training batch size. It also requires a smaller learning rate than Adam due to the larger norm of the update produced by the sign function. Additionally, we examine the limitations of Lion and identify scenarios where its improvements are small or not statistically significant. Lion is also successfully deployed in production systems such as Google search ads CTR model.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2302.06675v4",
                "caption": "Symbolic Discovery of Optimization Algorithms",
                "credit": "Xiangning Chen, Chen Liang, Da Huang, Esteban Real, Kaiyuan Wang, Yao Liu, Hieu Pham, Xuanyi Dong, Thang Luong, Cho-Jui Hsieh, Yifeng Lu, Quoc V. Le"
            }
        },
        {
            "start_date": {
                "year": 2023,
                "month": 2,
                "day": 10
            },
            "text": {
                "headline": "ViT-22B",
                "text": "<p>The scaling of Transformers has driven breakthrough capabilities for language models. At present, the largest large language models (LLMs) contain upwards of 100B parameters. Vision Transformers (ViT) have introduced the same architecture to image and video modelling, but these have not yet been successfully scaled to nearly the same degree; the largest dense ViT contains 4B parameters (Chen et al., 2022). We present a recipe for highly efficient and stable training of a 22B-parameter ViT (ViT-22B) and perform a wide variety of experiments on the resulting model. When evaluated on downstream tasks (often with a lightweight linear model on frozen features), ViT-22B demonstrates increasing performance with scale. We further observe other interesting benefits of scale, including an improved tradeoff between fairness and performance, state-of-the-art alignment to human visual perception in terms of shape/texture bias, and improved robustness. ViT-22B demonstrates the potential for \"LLM-like\" scaling in vision, and provides key steps towards getting there.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2302.05442v1",
                "caption": "Scaling Vision Transformers to 22 Billion Parameters",
                "credit": "Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer, Andreas Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin, Rodolphe Jenatton, Lucas Beyer, Michael Tschannen, Anurag Arnab, Xiao Wang, Carlos Riquelme, Matthias Minderer, Joan Puigcerver, Utku Evci, Manoj Kumar, Sjoerd van Steenkiste, Gamaleldin F. Elsayed, Aravindh Mahendran, Fisher Yu, Avital Oliver, Fantine Huot, Jasmijn Bastings, Mark Patrick Collier, Alexey Gritsenko, Vighnesh Birodkar, Cristina Vasconcelos, Yi Tay, Thomas Mensink, Alexander Kolesnikov, Filip Pavetić, Dustin Tran, Thomas Kipf, Mario Lučić, Xiaohua Zhai, Daniel Keysers, Jeremiah Harmsen, Neil Houlsby"
            }
        },
        {
            "start_date": {
                "year": 2023,
                "month": 2,
                "day": 9
            },
            "text": {
                "headline": "ProteinDT",
                "text": "<p>Current AI-assisted protein design mainly utilizes protein sequential and structural information. Meanwhile, there exists tremendous knowledge curated by humans in the text format describing proteins’ high-level functionalities. Yet, whether the incorporation of such text data can help protein design tasks has not been explored. To bridge this gap, we propose ProteinDT, a multi-modal framework that leverages textual descriptions for protein design. ProteinDT consists of three subsequent steps: ProteinCLAP which aligns the representation of two modalities, a facilitator that generates the protein representation from the text modality, and a decoder that creates the protein sequences from the representation. To train ProteinDT, we construct a large dataset, SwissProtCLAP, with 441K text and protein pairs. We quantitatively verify the effectiveness of ProteinDT on three challenging tasks: (1) over 90% accuracy for text-guided protein generation; (2) best hit ratio on 10 zero-shot text-guided protein editing tasks; (3) superior performance on four out of six protein property prediction benchmarks.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2302.04611",
                "caption": "A Text-guided Protein Design Framework",
                "credit": "Shengchao Liu, Yanjing Li, Zhuoxinran Li, Anthony Gitter, Yutao Zhu, Jiarui Lu, Zhao Xu, Weili Nie, Arvind Ramanathan, Chaowei Xiao, Jian Tang, Hongyu Guo, and Anima Anandkumar"
            }
        },
        {
            "start_date": {
                "year": 2023,
                "month": 2,
                "day": 6
            },
            "text": {
                "headline": "Gen-1",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2302.03011",
                "caption": "Structure and Content-Guided Video Synthesis with Diffusion Models",
                "credit": "Patrick Esser, Johnathan Chiu, Parmida Atighehchian, Jonathan Granskog, Anastasis Germanidis"
            }
        },
        {
            "start_date": {
                "year": 2023,
                "month": 1,
                "day": 30
            },
            "text": {
                "headline": "Flan T5-XXL + BLIP-2",
                "text": "<p>The cost of vision-and-language pre-training has become increasingly prohibitive due to end-to-end training of large-scale models. This paper proposes BLIP-2, a generic and efficient pre-training strategy that bootstraps vision-language pre-training from off-the-shelf frozen pre-trained image encoders and frozen large language models. BLIP-2 bridges the modality gap with a lightweight Querying Transformer, which is pre-trained in two stages. The first stage bootstraps vision-language representation learning from a frozen image encoder. The second stage bootstraps vision-to-language generative learning from a frozen language model. BLIP-2 achieves state-of-the-art performance on various vision-language tasks, despite having significantly fewer trainable parameters than existing methods. For example, our model outperforms Flamingo80B by 8.7% on zero-shot VQAv2 with 54x fewer trainable parameters. We also demonstrate the model's emerging capabilities of zero-shot image-to-text generation that can follow natural language instructions.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2301.12597, https://huggingface.co/Salesforce/blip2-flan-t5-xl",
                "caption": "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models",
                "credit": "Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi"
            }
        },
        {
            "start_date": {
                "year": 2023,
                "month": 1,
                "day": 30
            },
            "text": {
                "headline": "BLIP-2 (Q-Former)",
                "text": "<p>The cost of vision-and-language pre-training has become increasingly prohibitive due to end-to-end training of large-scale models. This paper proposes BLIP-2, a generic and efficient pre-training strategy that bootstraps vision-language pre-training from off-the-shelf frozen pre-trained image encoders and frozen large language models. BLIP-2 bridges the modality gap with a lightweight Querying Transformer, which is pre-trained in two stages. The first stage bootstraps vision-language representation learning from a frozen image encoder. The second stage bootstraps vision-to-language generative learning from a frozen language model. BLIP-2 achieves state-of-the-art performance on various vision-language tasks, despite having significantly fewer trainable parameters than existing methods. For example, our model outperforms Flamingo80B by 8.7% on zero-shot VQAv2 with 54x fewer trainable parameters. We also demonstrate the model's emerging capabilities of zero-shot image-to-text generation that can follow natural language instructions.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2301.12597",
                "caption": "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models",
                "credit": "Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi"
            }
        },
        {
            "start_date": {
                "year": 2023,
                "month": 1,
                "day": 27
            },
            "text": {
                "headline": "DDPM-IP (CelebA)",
                "text": "<p>Denoising Diffusion Probabilistic Models have shown an impressive generation quality, although their long sampling chain leads to high computational costs. In this paper, we observe that a long sampling chain also leads to an error accumulation phenomenon, which is similar to the exposure bias problem in autoregressive text generation. Specifically, we note that there is a discrepancy between training and testing, since the former is conditioned on the ground truth samples, while the latter is conditioned on the previously generated results. To alleviate this problem, we propose a very simple but effective training regularization, consisting in perturbing the ground truth samples to simulate the inference time prediction errors. We empirically show that, without affecting the recall and precision, the proposed input perturbation leads to a significant improvement in the sample quality while reducing both the training and the inference times. For instance, on CelebA 64×64, we achieve a new state-of-the-art FID score of 1.27, while saving 37.5% of the training time. The code is publicly available at this https URL</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2301.11706v3",
                "caption": "Input Perturbation Reduces Exposure Bias in Diffusion Models",
                "credit": "Mang Ning, Enver Sangineto, Angelo Porrello, Simone Calderara, Rita Cucchiara"
            }
        },
        {
            "start_date": {
                "year": 2023,
                "month": 1,
                "day": 26
            },
            "text": {
                "headline": "MusicLM",
                "text": "<p>We introduce MusicLM, a model generating high-fidelity music from text descriptions such as \"a calming violin melody backed by a distorted guitar riff\". MusicLM casts the process of conditional music generation as a hierarchical sequence-to-sequence modeling task, and it generates music at 24 kHz that remains consistent over several minutes. Our experiments show that MusicLM outperforms previous systems both in audio quality and adherence to the text description. Moreover, we demonstrate that MusicLM can be conditioned on both text and a melody in that it can transform whistled and hummed melodies according to the style described in a text caption. To support future research, we publicly release MusicCaps, a dataset composed of 5.5k music-text pairs, with rich text descriptions provided by human experts.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2301.11325",
                "caption": "MusicLM: Generating Music From Text",
                "credit": "Andrea Agostinelli, Timo I. Denk, Zalán Borsos, Jesse Engel, Mauro Verzetti, Antoine Caillon, Qingqing Huang, Aren Jansen, Adam Roberts, Marco Tagliasacchi, Matt Sharifi, Neil Zeghidour, Christian Frank"
            }
        },
        {
            "start_date": {
                "year": 2023,
                "month": 1,
                "day": 16
            },
            "text": {
                "headline": "Ankh_large",
                "text": "<p>As opposed to scaling-up protein language models (PLMs), we seek improving performance via protein-specific optimization. Although the proportionality between the language model size and the richness of its learned representations is validated, we prioritize accessibility and pursue a path of data-efficient, cost-reduced, and knowledge-guided optimization. Through over twenty experiments ranging from masking, architecture, and pre-training data, we derive insights from protein-specific experimentation into building a model that interprets the language of life, optimally. We present Ankh, the first general-purpose PLM trained on Google’s TPU-v4 surpassing the state-of-the-art performance with fewer parameters (<10% for pre-training, <7% for inference, and <30% for the embedding dimension). We provide a representative range of structure and function benchmarks where Ankh excels. We further provide a protein variant generation analysis on High-N and One-N input data scales where Ankh succeeds in learning protein evolutionary conservation-mutation trends and introducing functional diversity while retaining key structural-functional characteristics. We dedicate our work to promoting accessibility to research innovation via attainable resources.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2301.06568",
                "caption": "Ankh: Optimized Protein Language Model Unlocks General-Purpose Modelling",
                "credit": "Ahmed Elnaggar, Hazem Essam, Wafaa Salah-Eldin, Walid Moustafa, Mohamed Elkerdawy, Charlotte Rochereau, Burkhard Rost"
            }
        },
        {
            "start_date": {
                "year": 2023,
                "month": 1,
                "day": 15
            },
            "text": {
                "headline": "Nucleotide Transformer",
                "text": "<p>Closing the gap between measurable genetic information and observable traits is a longstanding challenge in genomics. Yet, the prediction of molecular phenotypes from DNA sequences alone remains limited and inaccurate, often driven by the scarcity of annotated data and the inability to transfer learnings between prediction tasks. Here, we present an extensive study of foundation models pre-trained on DNA sequences, named the Nucleotide Transformer, integrating information from 3,202 diverse human genomes, as well as 850 genomes from a wide range of species, including model and non-model organisms. These transformer models yield transferable, context-specific representations of nucleotide sequences, which allow for accurate molecular phenotype prediction even in low-data settings. We show that the representations alone match or outperform specialized methods on 11 of 18 prediction tasks, and up to 15 after fine-tuning. Despite no supervision, the transformer models learnt to focus attention on key genomic elements, including those that regulate gene expression, such as enhancers. Lastly, we demonstrate that utilizing model representations alone can improve the prioritization of functional genetic variants. The training and application of foundational models in genomics explored in this study provide a widely applicable stepping stone to bridge the gap of accurate molecular phenotype prediction from DNA sequence alone.\n</p>"
            },
            "media": {
                "url": "https://www.biorxiv.org/content/10.1101/2023.01.11.523679v1.full.pdf",
                "caption": "The Nucleotide Transformer: Building and Evaluating Robust\nFoundation Models for Human Genomics",
                "credit": "Hugo Dalla-Torre, Liam Gonzalez, Javier Mendoza Revilla, Nicolas Lopez Carranza, Adam Henryk Grzywaczewski, Francesco Oteri, Christian Dallago, Evan Trop, Hassan Sirelkhatim, Guillaume Richard, Marcin Skwark, Karim Beguir,\nMarie Lopez, Thomas Pierrot"
            }
        },
        {
            "start_date": {
                "year": 2023,
                "month": 1,
                "day": 10
            },
            "text": {
                "headline": "DreamerV3",
                "text": "<p>General intelligence requires solving tasks across many domains. Current reinforcement learning algorithms carry this potential but are held back by the resources and knowledge required to tune them for new tasks. We present DreamerV3, a general and scalable algorithm based on world models that outperforms previous approaches across a wide range of domains with fixed hyperparameters. These domains include continuous and discrete actions, visual and low-dimensional inputs, 2D and 3D worlds, different data\nbudgets, reward frequencies, and reward scales. We observe favorable scaling properties of DreamerV3, with larger models directly translating to higher data-efficiency and final performance. Applied out of the box, DreamerV3 is the first algorithm to collect diamonds in Minecraft from scratch without human data or curricula, a long-standing challenge in artificial intelligence. Our general algorithm makes reinforcement learning broadly applicable and allows scaling to hard decision making problems\n</p>"
            },
            "media": {
                "url": "https://arxiv.org/pdf/2301.04104v1",
                "caption": "Mastering Diverse Domains through World Models",
                "credit": "Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, Timothy Lillicrap "
            }
        },
        {
            "start_date": {
                "year": 2023,
                "month": 1,
                "day": 5
            },
            "text": {
                "headline": "VALL-E",
                "text": "<p>We introduce a language modeling approach for text to speech synthesis (TTS). Specifically, we train a neural codec language model (called Vall-E) using discrete codes derived from an off-the-shelf neural audio codec model, and regard TTS as a conditional language modeling task rather than continuous signal regression as in previous work. During the pre-training stage, we scale up the TTS training data to 60K hours of English speech which is hundreds of times larger than existing systems. Vall-E emerges in-context learning capabilities and can be used to synthesize high-quality personalized speech with only a 3-second enrolled recording of an unseen speaker as an acoustic prompt. Experiment results show that Vall-E significantly outperforms the state-of-the-art zero-shot TTS system in terms of speech naturalness and speaker similarity. In addition, we find Vall-E could preserve the speaker's emotion and acoustic environment of the acoustic prompt in synthesis. See this https URL for demos of our work.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2301.02111",
                "caption": "Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers",
                "credit": "Chengyi Wang, Sanyuan Chen, Yu Wu, Ziqiang Zhang, Long Zhou, Shujie Liu, Zhuo Chen, Yanqing Liu, Huaming Wang, Jinyu Li, Lei He, Sheng Zhao, Furu Wei"
            }
        },
        {
            "start_date": {
                "year": 2022,
                "month": 12,
                "day": 28
            },
            "text": {
                "headline": "Hybrid H3-2.7B",
                "text": "<p>State space models (SSMs) have demonstrated state-of-the-art sequence modeling performance in some modalities, but underperform attention in language modeling. Moreover, despite scaling nearly linearly in sequence length instead of quadratically, SSMs are still slower than Transformers due to poor hardware utilization. In this paper, we make progress on understanding the expressivity gap between SSMs and attention in language modeling, and on reducing the hardware barrier between SSMs and attention. First, we use synthetic language modeling tasks to understand the gap between SSMs and attention. We find that existing SSMs struggle with two capabilities: recalling earlier tokens in the sequence and comparing tokens across the sequence. To understand the impact on language modeling, we propose a new SSM layer, H3, that is explicitly designed for these abilities. H3 matches attention on the synthetic languages and comes within 0.4 PPL of Transformers on OpenWebText. Furthermore, a hybrid 125M-parameter H3-attention model that retains two attention layers surprisingly outperforms Transformers on OpenWebText by 1.0 PPL. Next, to improve the efficiency of training SSMs on modern hardware, we propose FlashConv. FlashConv uses a fused block FFT algorithm to improve efficiency on sequences up to 8K, and introduces a novel state passing algorithm that exploits the recurrent properties of SSMs to scale to longer sequences. FlashConv yields 2× speedup on the long-range arena benchmark and allows hybrid language models to generate text 2.4× faster than Transformers. Using FlashConv, we scale hybrid H3-attention language models up to 2.7B parameters on the Pile and find promising initial results, achieving lower perplexity than Transformers and outperforming Transformers in zero- and few-shot learning on a majority of tasks in the SuperGLUE benchmark.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2212.14052",
                "caption": "Hungry Hungry Hippos: Towards Language Modeling with State Space Models",
                "credit": "Daniel Y. Fu, Tri Dao, Khaled K. Saab, Armin W. Thomas, Atri Rudra, Christopher Ré"
            }
        },
        {
            "start_date": {
                "year": 2022,
                "month": 12,
                "day": 19
            },
            "text": {
                "headline": "CaLM",
                "text": "<p>Protein representations from deep language models have yielded state-of-the-art performance across many tasks in computational protein engineering. In recent years, progress has primarily focused on parameter count, with recent models’ capacities surpassing the size of the very datasets they were trained on. Here, we propose an alternative direction. We show that large language models trained on codons, instead of amino acid sequences, provide high-quality representations that outperform comparable state-of-the-art models across a variety of tasks.\nIn some tasks, like species recognition, prediction of protein and transcript abundance, or melting point estimation, we show that a language model trained on codons outperforms every other published protein language model, including some that contain over 50 times more parameters. These results suggest that, in addition to commonly studied scale and model complexity, the information content of biological data provides an orthogonal direction to improve the power of machine learning in biology.</p>"
            },
            "media": {
                "url": "https://www.biorxiv.org/content/10.1101/2022.12.15.519894v1.full.pdf",
                "caption": "Codon language embeddings provide strong signals for protein engineering",
                "credit": "Carlos Outeiral, Charlotte M. Deane"
            }
        },
        {
            "start_date": {
                "year": 2022,
                "month": 12,
                "day": 13
            },
            "text": {
                "headline": "RT-1",
                "text": "<p>By transferring knowledge from large, diverse, task-agnostic datasets, modern machine learning models can solve specific downstream tasks either zero-shot or with small task-specific datasets to a high level of performance. While this capability has been demonstrated in other fields such as computer vision, natural language processing or speech recognition, it remains to be shown in robotics, where the generalization capabilities of the models are particularly critical due to the difficulty of collecting real-world robotic data. We argue that one of the keys to the success of such general robotic models lies with open-ended task-agnostic training, combined with high-capacity architectures that can absorb all of the diverse, robotic data. In this paper, we present a model class, dubbed Robotics Transformer, that exhibits promising scalable model properties. We verify our conclusions in a study of different model classes and their ability to generalize as a function of the data size, model size, and data diversity based on a large-scale data collection on real robots performing real-world tasks. The project's website and videos can be found at this http URL</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2212.06817",
                "caption": "RT-1: Robotics Transformer for Real-World Control at Scale",
                "credit": "Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Tomas Jackson, Sally Jesmonth, Nikhil J Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Isabel Leal, Kuang-Huei Lee, Sergey Levine, Yao Lu, Utsav Malla, Deeksha Manjunath, Igor Mordatch, Ofir Nachum, Carolina Parada, Jodilyn Peralta, Emily Perez, Karl Pertsch, Jornell Quiambao, Kanishka Rao, Michael Ryoo, Grecia Salazar, Pannag Sanketi, Kevin Sayed, Jaspiar Singh, Sumedh Sontakke, Austin Stone, Clayton Tan, Huong Tran, Vincent Vanhoucke, Steve Vega, Quan Vuong, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Tianhe Yu, Brianna Zitkovich"
            }
        },
        {
            "start_date": {
                "year": 2022,
                "month": 12,
                "day": 10
            },
            "text": {
                "headline": "TranceptEve",
                "text": "<p>Modeling the fitness landscape of protein sequences has historically relied on training models on family-specific sets of homologous sequences called Multiple Sequence Alignments. Many proteins are however difficult to align or have shallow alignments which limits the potential scope of alignment-based methods. Not subject to these limitations, large protein language models trained on non-aligned sequences across protein families have achieved increasingly high predictive performance – but have not yet fully bridged the gap with their alignment-based counterparts. In this work, we introduce TranceptEVE – a hybrid method between family-specific and family-agnostic models that seeks to build on the relative strengths from each approach. Our method gracefully adapts to the depth of the alignment, fully relying on its autoregressive transformer when dealing with shallow alignments and leaning more heavily on the family-specifc models for proteins with deeper alignments. Besides its broader application scope, it achieves state-of-the-art performance for mutation effects prediction, both in terms of correlation with experimental assays and with clinical annotations from ClinVar.</p>"
            },
            "media": {
                "url": "https://www.biorxiv.org/content/10.1101/2022.12.07.519495v1",
                "caption": "TranceptEVE: Combining Family-specific and Family-agnostic Models of Protein Sequences for Improved Fitness Prediction",
                "credit": "Pascal Notin, Lood Van Niekerk, Aaron W Kollasch, Daniel Ritter, Yarin Gal, Debora S. Marks"
            }
        },
        {
            "start_date": {
                "year": 2022,
                "month": 12,
                "day": 4
            },
            "text": {
                "headline": "Vega v2",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://arxiv.org/pdf/2212.01853",
                "caption": "Toward Efficient Language Model Pretraining and Downstream Adaptation via Self-Evolution: A Case Study on SuperGLUE",
                "credit": "Qihuang Zhong, Liang Ding, Yibing Zhan, Yu Qiao, Yonggang Wen, Li Shen, Juhua Liu, Baosheng Yu, Bo Du, Yixin Chen, Xinbo Gao, Chunyan Miao, Xiaoou Tang, Dacheng Tao"
            }
        },
        {
            "start_date": {
                "year": 2022,
                "month": 12,
                "day": 1
            },
            "text": {
                "headline": "DeepNash",
                "text": "<p>We introduce DeepNash, an autonomous agent that plays the imperfect information game Stratego at a human expert level. Stratego is one of the few iconic board games that artificial intelligence (AI) has not yet mastered. It is a game characterized by a twin challenge: It requires long-term strategic thinking as in chess, but it also requires dealing with imperfect information as in poker. The technique underpinning DeepNash uses a game-theoretic, model-free deep reinforcement learning method, without search, that learns to master Stratego through self-play from scratch. DeepNash beat existing state-of-the-art AI methods in Stratego and achieved a year-to-date (2022) and all-time top-three ranking on the Gravon games platform, competing with human expert players.</p>"
            },
            "media": {
                "url": "https://www.science.org/stoken/author-tokens/ST-887/full",
                "caption": "Mastering the game of Stratego with model-free multiagent reinforcement learning",
                "credit": "Julien Perolat, Bart de Vylder, Daniel Hennes, Eugene Tarassov, Florian Strub, Vincent de Boer, Paul Muller, Jerome T. Connor, Neil Burch, Thomas Anthony, Stephen McAleer, Romuald Elie, Sarah H. Cen, Zhe Wang, Audrunas Gruslys, Aleksandra Malysheva, Mina Khan, Sherjil Ozair, Finbarr Timbers, Toby Pohlen, Tom Eccles, Mark Rowland, Marc Lanctot, Jean-Baptiste Lespiau, Bilal Piot, Shayegan Omidshafiei, Edward Lockhart, Laurent Sifre, Nathalie Beauguerlange, Remi Munos, David Silver, Satinder Singh, Demis Hassabis, Karl Tuyls"
            }
        },
        {
            "start_date": {
                "year": 2022,
                "month": 11,
                "day": 30
            },
            "text": {
                "headline": "GPT-3.5 Turbo",
                "text": "<p>GPT-3.5 Turbo models can understand and generate natural language or code and have been optimized for chat using the Chat Completions API but work well for non-chat tasks as well. As of July 2024, use gpt-4o-mini in place of GPT-3.5 Turbo, as it is cheaper, more capable, multimodal, and just as fast. GPT-3.5 Turbo is still available for use in the API.</p>"
            },
            "media": {
                "url": "https://platform.openai.com/docs/models/gpt-3.5-turbo",
                "caption": "A fast, inexpensive model for simple tasks",
                "credit": "John Schulman, Barret Zoph, Christina Kim, Jacob Hilton, Jacob Menick, Jiayi Weng, Juan Felipe Ceron Uribe, Liam Fedus, Luke Metz, Michael Pokorny, Rapha Gontijo Lopes, Shengjia Zhao, Arun Vijayvergiya, Eric Sigler, Adam Perelman, Chelsea Voss, Mike Heaton, Joel Parish, Dave Cummings, Rajeev Nayak, Valerie Balcom, David Schnurr, Tomer Kaftan, Chris Hallacy, Nicholas Turley, Noah Deutsch, Vik Goel, Jonathan Ward, Aris Konstantinidis, Wojciech Zaremba, Long Ouyang, Leonard Bogdonoff, Joshua Gross, David Medina, Sarah Yoo, Teddy Lee, Ryan Lowe, Dan Mossing, Joost Huizinga, Roger Jiang, Carroll Wainwright, Diogo Almeida, Steph Lin, Marvin Zhang, Kai Xiao, Katarina Slama, Steven Bills, Alex Gray, Jan Leike, Jakub Pachocki, Phil Tillet, Shantanu Jain, Greg Brockman, Nick Ryder, Alex Paino, Qiming Yuan, Clemens Winter, Ben Wang, Mo Bavarian, Igor Babuschkin, Szymon Sidor, Ingmar Kanitscheider, Mikhail Pavlov, Matthias Plappert, Nik Tezak, Heewoo Jun, William Zhuk, Vitchyr Pong, Lukasz Kaiser, Jerry Tworek, Andrew Carr, Lilian Weng, Sandhini Agarwal, Karl Cobbe, Vineet Kosaraju, Alethea Power, Stanislas Polu, Jesse Han, Raul Puri, Shawn Jain, Benjamin Chess, Christian Gibson, Oleg Boiko, Emy Parparita, Amin Tootoonchian, Kyle Kosic, Christopher Hesse"
            }
        },
        {
            "start_date": {
                "year": 2022,
                "month": 11,
                "day": 28
            },
            "text": {
                "headline": "GPT-3.5",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://platform.openai.com/docs/models/gpt-3-5",
                "caption": "",
                "credit": ""
            }
        },
        {
            "start_date": {
                "year": 2022,
                "month": 11,
                "day": 28
            },
            "text": {
                "headline": "DiT-XL/2 + Discriminator Guidance",
                "text": "<p>The proposed method, Discriminator Guidance, aims to improve sample generation of pre-trained diffusion models. The approach introduces a discriminator that gives explicit supervision to a denoising sample path whether it is realistic or not. Unlike GANs, our approach does not require joint training of score and discriminator networks. Instead, we train the discriminator after score training, making discriminator training stable and fast to converge. In sample generation, we add an auxiliary term to the pre-trained score to deceive the discriminator. This term corrects the model score to the data score at the optimal discriminator, which implies that the discriminator helps better score estimation in a complementary way. Using our algorithm, we achive state-of-the-art results on ImageNet 256x256 with FID 1.83 and recall 0.64, similar to the validation data's FID (1.68) and recall (0.66). We release the code at this https URL.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2211.17091v4",
                "caption": "Refining Generative Process with Discriminator Guidance in Score-based Diffusion Models",
                "credit": "Dongjun Kim, Yeongmin Kim, Se Jung Kwon, Wanmo Kang, Il-Chul Moon"
            }
        },
        {
            "start_date": {
                "year": 2022,
                "month": 11,
                "day": 28
            },
            "text": {
                "headline": "Discriminator Guidance",
                "text": "<p>The proposed method, Discriminator Guidance, aims to improve sample generation of pre-trained diffusion models. The approach introduces a discriminator that gives explicit supervision to a denoising sample path whether it is realistic or not. Unlike GANs, our approach does not require joint training of score and discriminator networks. Instead, we train the discriminator after score training, making discriminator training stable and fast to converge. In sample generation, we add an auxiliary term to the pre-trained score to deceive the discriminator. This term corrects the model score to the data score at the optimal discriminator, which implies that the discriminator helps better score estimation in a complementary way. Using our algorithm, we achive state-of-the-art results on ImageNet 256x256 with FID 1.83 and recall 0.64, similar to the validation data's FID (1.68) and recall (0.66).</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2211.17091v4",
                "caption": "Refining Generative Process with Discriminator Guidance in Score-based Diffusion Models",
                "credit": "Dongjun Kim, Yeongmin Kim, Se Jung Kwon, Wanmo Kang, Il-Chul Moon"
            }
        },
        {
            "start_date": {
                "year": 2022,
                "month": 11,
                "day": 28
            },
            "text": {
                "headline": "ALM 1.0",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://github.com/FlagAI-Open/FlagAI/blob/master/examples/ALM/README.md",
                "caption": "ALM 1.0",
                "credit": ""
            }
        },
        {
            "start_date": {
                "year": 2022,
                "month": 11,
                "day": 22
            },
            "text": {
                "headline": "CICERO",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://www.science.org/doi/10.1126/science.ade9097",
                "caption": "Human-level play in the game of Diplomacy by combining language models with strategic reasoning",
                "credit": "Anton Bakhtin, Noam Brown, Emily Dinan, Gabriele Farina, Colin Flaherty, Daniel Fried, Andrew Goff, Jonathan Gray, Hengyuan Hu, Athul Paul Jacob, Mojtaba Komeili, Karthik Konath, Minae Kwon, Adam Lerer, Mike Lewis, Alexander H. Miller, Sasha Mitts, Adithya Renduchintala, Stephen Roller, Dirk Rowe, Weiyan Shi, Joe Spisak, Alexander Wei, David Wu, Hugh Zhang, Markus Zijlstra"
            }
        },
        {
            "start_date": {
                "year": 2022,
                "month": 11,
                "day": 20
            },
            "text": {
                "headline": "AR-LDM",
                "text": "<p>Conditioned diffusion models have demonstrated state-of-the-art text-to-image synthesis capacity. Recently, most works focus on synthesizing independent images; While for real-world applications, it is common and necessary to generate a series of coherent images for story-stelling. In this work, we mainly focus on story visualization and continuation tasks and propose AR-LDM, a latent diffusion model auto-regressively conditioned on history captions and generated images. Moreover, AR-LDM can generalize to new characters through adaptation. To our best knowledge, this is the first work successfully leveraging diffusion models for coherent visual story synthesizing. Quantitative results show that AR-LDM achieves SoTA FID scores on PororoSV, FlintstonesSV, and the newly introduced challenging dataset VIST containing natural images. Large-scale human evaluations show that AR-LDM has superior performance in terms of quality, relevance, and consistency.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2211.10950",
                "caption": "Synthesizing Coherent Story with Auto-Regressive Latent Diffusion Models",
                "credit": "Xichen Pan, Pengda Qin, Yuhong Li, Hui Xue, Wenhu Chen"
            }
        },
        {
            "start_date": {
                "year": 2022,
                "month": 11,
                "day": 18
            },
            "text": {
                "headline": "Fusion in Encoder",
                "text": "<p>Generative models have recently started to outperform extractive models in Open Domain Question Answering, largely by leveraging their decoder to attend over multiple encoded passages and combining their information. However, generative models tend to be larger than extractive models due to the need for a decoder, run slower during inference due to auto-regressive decoder beam search, and their generated output often suffers from hallucinations. We propose to extend transformer encoders with the ability to fuse information from multiple passages, using global representation to provide cross-sample attention over all tokens across samples. Furthermore, we propose an alternative answer span probability calculation to better aggregate answer scores in the global space of all samples. Using our proposed method, we outperform the current state-of-the-art method by 2.5 Exact Match score on the Natural Question dataset while using only 25% of parameters and 35% of the latency during inference, and 4.4 Exact Match on WebQuestions dataset. When coupled with synthetic data augmentation, we outperform larger models on the TriviaQA dataset as well. The latency and parameter savings of our method make it particularly attractive for open-domain question answering, as these models are often compute-intensive.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2211.10147",
                "caption": "FiE: Building a Global Probability Space by Leveraging Early Fusion in Encoder for Open-Domain Question Answering",
                "credit": "Akhil Kedia, Mohd Abbas Zaidi, Haejun Lee"
            }
        },
        {
            "start_date": {
                "year": 2022,
                "month": 11,
                "day": 16
            },
            "text": {
                "headline": "Galactica",
                "text": "<p>Information overload is a major obstacle to scientific progress. The explosive growth in scientific literature and data has made it ever harder to discover useful insights in a large mass of information. Today scientific knowledge is accessed through search engines, but they are unable to organize scientific knowledge alone. In this paper we introduce Galactica: a large language model that can store, combine and reason about scientific knowledge. We train on a large scientific corpus of papers, reference material, knowledge bases and many other sources. We outperform existing models on a range of scientific tasks. On technical knowledge probes such as LaTeX equations, Galactica outperforms the latest GPT-3 by 68.2% versus 49.0%. Galactica also performs well on reasoning, outperforming Chinchilla on mathematical MMLU by 41.3% to 35.7%, and PaLM 540B on MATH with a score of 20.4% versus 8.8%. It also sets a new state-of-the-art on downstream tasks such as PubMedQA and MedMCQA dev of 77.6% and 52.9%. And despite not being trained on a general corpus, Galactica outperforms BLOOM and OPT-175B on BIG-bench. We believe these results demonstrate the potential for language models as a new interface for science. We open source the model for the benefit of the scientific community.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2211.09085",
                "caption": "Galactica: A Large Language Model for Science",
                "credit": "Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, Robert Stojnic"
            }
        },
        {
            "start_date": {
                "year": 2022,
                "month": 11,
                "day": 14
            },
            "text": {
                "headline": "EVA-01",
                "text": "<p>We launch EVA, a vision-centric foundation model to explore the limits of visual representation at scale using only publicly accessible data. EVA is a vanilla ViT pre-trained to reconstruct the masked out image-text aligned vision features conditioned on visible image patches. Via this pretext task, we can efficiently scale up EVA to one billion parameters, and sets new records on a broad range of representative vision downstream tasks, such as image recognition, video action recognition, object detection, instance segmentation and semantic segmentation without heavy supervised training. Moreover, we observe quantitative changes in scaling EVA result in qualitative changes in transfer learning performance that are not present in other models. For instance, EVA takes a great leap in the challenging large vocabulary instance segmentation task: our model achieves almost the same state-of-the-art performance on LVISv1.0 dataset with over a thousand categories and COCO dataset with only eighty categories. Beyond a pure vision encoder, EVA can also serve as a vision-centric, multi-modal pivot to connect images and text. We find initializing the vision tower of a giant CLIP from EVA can greatly stabilize the training and outperform the training from scratch counterpart with much fewer samples and less compute, providing a new direction for scaling up and accelerating the costly training of multi-modal foundation models. To facilitate future research, we release all the code and billion-scale model.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2211.07636",
                "caption": "EVA: Exploring the Limits of Masked Visual Representation Learning at Scale",
                "credit": "Yuxin Fang, Wen Wang, Binhui Xie, Quan Sun, Ledell Wu, Xinggang Wang, Tiejun Huang, Xinlong Wang, Yue Cao"
            }
        },
        {
            "start_date": {
                "year": 2022,
                "month": 11,
                "day": 12
            },
            "text": {
                "headline": "AltCLIP_M9",
                "text": "<p>In this work, we present a conceptually simple and effective method to train a strong bilingual/multilingual multimodal representation model. Starting from the pre-trained multimodal representation model CLIP released by OpenAI, we altered its text encoder with a pre-trained multilingual text encoder XLM-R, and aligned both languages and image representations by a two-stage training schema consisting of teacher learning and contrastive learning. We validate our method through evaluations of a wide range of tasks. We set new state-of-the-art performances on a bunch of tasks including ImageNet-CN, Flicker30k-CN, COCO-CN and XTD. Further, we obtain very close performances with CLIP on almost all tasks, suggesting that one can simply alter the text encoder in CLIP for extended capabilities such as multilingual understanding. Our models and code are available at this https URL.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2211.06679",
                "caption": "AltCLIP: Altering the Language Encoder in CLIP for Extended Language Capabilities",
                "credit": "Zhongzhi Chen, Guang Liu, Bo-Wen Zhang, Fulong Ye, Qinghong Yang, Ledell Wu"
            }
        },
        {
            "start_date": {
                "year": 2022,
                "month": 11,
                "day": 10
            },
            "text": {
                "headline": "InternImage",
                "text": "<p>Compared to the great progress of large-scale vision transformers (ViTs) in recent years, large-scale models based on convolutional neural networks (CNNs) are still in an early state. This work presents a new large-scale CNN-based foundation model, termed InternImage, which can obtain the gain from increasing parameters and training data like ViTs. Different from the recent CNNs that focus on large dense kernels, InternImage takes deformable convolution as the core operator, so that our model not only has the large effective receptive field required for downstream tasks such as detection and segmentation, but also has the adaptive spatial aggregation conditioned by input and task information. As a result, the proposed InternImage reduces the strict inductive bias of traditional CNNs and makes it possible to learn stronger and more robust patterns with large-scale parameters from massive data like ViTs. The effectiveness of our model is proven on challenging benchmarks including ImageNet, COCO, and ADE20K. It is worth mentioning that InternImage-H achieved a new record 65.4 mAP on COCO test-dev and 62.9 mIoU on ADE20K, outperforming current leading CNNs and ViTs. The code will be released at this https URL.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2211.05778",
                "caption": "InternImage: Exploring Large-Scale Vision Foundation Models with Deformable Convolutions",
                "credit": "Wenhai Wang, Jifeng Dai, Zhe Chen, Zhenhang Huang, Zhiqi Li, Xizhou Zhu, Xiaowei Hu, Tong Lu, Lewei Lu, Hongsheng Li, Xiaogang Wang, Yu Qiao"
            }
        },
        {
            "start_date": {
                "year": 2022,
                "month": 11,
                "day": 3
            },
            "text": {
                "headline": "mT0-13B",
                "text": "<p>Multitask prompted finetuning (MTF) has been shown to help large language models generalize to new tasks in a zero-shot setting, but so far explorations of MTF have focused on English data and models. We apply MTF to the pretrained multilingual BLOOM and mT5 model families to produce finetuned variants called BLOOMZ and mT0. We find finetuning large multilingual language models on English tasks with English prompts allows for task generalization to non-English languages that appear only in the pretraining corpus. Finetuning on multilingual tasks with English prompts further improves performance on English and non-English tasks leading to various state-of-the-art zero-shot results. We also investigate finetuning on multilingual tasks with prompts that have been machine-translated from English to match the language of each dataset. We find training on these machine-translated prompts leads to better performance on human-written prompts in the respective languages. Surprisingly, we find models are capable of zero-shot generalization to tasks in languages they have never intentionally seen. We conjecture that the models are learning higher-level capabilities that are both task- and language-agnostic. In addition, we introduce xP3, a composite of supervised datasets in 46 languages with English and machine-translated prompts. Our code, datasets and models are freely available at this https URL.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2211.01786",
                "caption": "Crosslingual Generalization through Multitask Finetuning",
                "credit": "Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao, M Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf, Xiangru Tang, Dragomir Radev, Alham Fikri Aji, Khalid Almubarak, Samuel Albanie, Zaid Alyafeai, Albert Webson, Edward Raff, Colin Raffel"
            }
        },
        {
            "start_date": {
                "year": 2022,
                "month": 11,
                "day": 3
            },
            "text": {
                "headline": "Mogrifier RLSTM (WT2)",
                "text": "<p>Just because some purely recurrent models suffer from being hard to optimize and inefficient on today's hardware, they are not necessarily bad models of language. We demonstrate this by the extent to which these models can still be improved by a combination of a slightly better recurrent cell, architecture, objective, as well as optimization. In the process, we establish a new state of the art for language modelling on small datasets and on Enwik8 with dynamic evaluation.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2211.01848",
                "caption": "Circling Back to Recurrent Models of Language",
                "credit": "Gábor Melis"
            }
        },
        {
            "start_date": {
                "year": 2022,
                "month": 11,
                "day": 3
            },
            "text": {
                "headline": "BLOOMZ-176B",
                "text": "<p>Multitask prompted finetuning (MTF) has been shown to help large language models generalize to new tasks in a zero-shot setting, but so far explorations of MTF have focused on English data and models. We apply MTF to the pretrained multilingual BLOOM and mT5 model families to produce finetuned variants called BLOOMZ and mT0. We find finetuning large multilingual language models on English tasks with English prompts allows for task generalization to non-English languages that appear only in the pretraining corpus. Finetuning on multilingual tasks with English prompts further improves performance on English and non-English tasks leading to various state-of-the-art zero-shot results. We also investigate finetuning on multilingual tasks with prompts that have been machine-translated from English to match the language of each dataset. We find training on these machine-translated prompts leads to better performance on human-written prompts in the respective languages. Surprisingly, we find models are capable of zero-shot generalization to tasks in languages they have never intentionally seen. We conjecture that the models are learning higher-level capabilities that are both task- and language-agnostic. In addition, we introduce xP3, a composite of supervised datasets in 46 languages with English and machine-translated prompts. Our code, datasets and models are freely available at this https URL.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2211.01786, https://huggingface.co/bigscience/bloomz",
                "caption": "Crosslingual Generalization through Multitask Finetuning",
                "credit": "Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao, M Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf, Xiangru Tang, Dragomir Radev, Alham Fikri Aji, Khalid Almubarak, Samuel Albanie, Zaid Alyafeai, Albert Webson, Edward Raff, Colin Raffel"
            }
        },
        {
            "start_date": {
                "year": 2022,
                "month": 11,
                "day": 2
            },
            "text": {
                "headline": "eDiff-I",
                "text": "<p>Large-scale diffusion-based generative models have led to breakthroughs in text-conditioned high-resolution image synthesis. Starting from random noise, such text-to-image diffusion models gradually synthesize images in an iterative fashion while conditioning on text prompts. We find that their synthesis behavior qualitatively changes throughout this process: Early in sampling, generation strongly relies on the text prompt to generate text-aligned content, while later, the text conditioning is almost entirely ignored. This suggests that sharing model parameters throughout the entire generation process may not be ideal. Therefore, in contrast to existing works, we propose to train an ensemble of text-to-image diffusion models specialized for different synthesis stages. To maintain training efficiency, we initially train a single model, which is then split into specialized models that are trained for the specific stages of the iterative generation process. Our ensemble of diffusion models, called eDiff-I, results in improved text alignment while maintaining the same inference computation cost and preserving high visual quality, outperforming previous large-scale text-to-image diffusion models on the standard benchmark. In addition, we train our model to exploit a variety of embeddings for conditioning, including the T5 text, CLIP text, and CLIP image embeddings. We show that these different embeddings lead to different behaviors. Notably, the CLIP image embedding allows an intuitive way of transferring the style of a reference image to the target text-to-image output. Lastly, we show a technique that enables eDiff-I's \"paint-with-words\" capability. A user can select the word in the input text and paint it in a canvas to control the output, which is very handy for crafting the desired image in mind. The project page is available at this https URL</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2211.01324",
                "caption": "eDiff-I: Text-to-Image Diffusion Models with an Ensemble of Expert Denoisers",
                "credit": "Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Qinsheng Zhang, Karsten Kreis, Miika Aittala, Timo Aila, Samuli Laine, Bryan Catanzaro, Tero Karras, Ming-Yu Liu"
            }
        },
        {
            "start_date": {
                "year": 2022,
                "month": 10,
                "day": 24
            },
            "text": {
                "headline": "EnCodec",
                "text": "<p>We introduce a state-of-the-art real-time, high-fidelity, audio codec leveraging neural networks. It consists in a streaming encoder-decoder architecture with quantized latent space trained in an end-to-end fashion. We simplify and speed-up the training by using a single multiscale spectrogram adversary that efficiently reduces artifacts and produce high-quality samples. We introduce a novel loss balancer mechanism to stabilize training: the weight of a loss now defines the fraction of the overall gradient it should represent, thus decoupling the choice of this hyper-parameter from the typical scale of the loss. Finally, we study how lightweight Transformer models can be used to further compress the obtained representation by up to 40%, while staying faster than real time. We provide a detailed description of the key design choices of the proposed model including: training objective, architectural changes and a study of various perceptual loss functions. We present an extensive subjective evaluation (MUSHRA tests) together with an ablation study for a range of bandwidths and audio domains, including speech, noisy-reverberant speech, and music. Our approach is superior to the baselines methods across all evaluated settings, considering both 24 kHz monophonic and 48 kHz stereophonic audio. Code and models are available at this http URL.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2210.13438, ",
                "caption": "High Fidelity Neural Audio Compression",
                "credit": "Alexandre Défossez, Jade Copet, Gabriel Synnaeve, Yossi Adi"
            }
        },
        {
            "start_date": {
                "year": 2022,
                "month": 10,
                "day": 20
            },
            "text": {
                "headline": "U-PaLM (540B)",
                "text": "<p>Scaling language models improves performance but comes with significant computational costs. This paper proposes UL2R, a method that substantially improves existing language models and their scaling curves with a relatively tiny amount of extra compute. The key idea is to continue training a state-of-the-art large language model (e.g., PaLM) on a few more steps with UL2's mixture-of-denoiser objective. We show that, with almost negligible extra computational costs and no new sources of data, we are able to substantially improve the scaling properties of large language models on downstream metrics. In this paper, we continue training PaLM with UL2R, introducing a new set of models at 8B, 62B, and 540B scale which we call U-PaLM. Impressively, at 540B scale, we show an approximately 2x computational savings rate where U-PaLM achieves the same performance as the final PaLM 540B model at around half its computational budget (i.e., saving ∼4.4 million TPUv4 hours). We further show that this improved scaling curve leads to 'emergent abilities' on challenging BIG-Bench tasks -- for instance, U-PaLM does much better than PaLM on some tasks or demonstrates better quality at much smaller scale (62B as opposed to 540B). Overall, we show that U-PaLM outperforms PaLM on many few-shot setups, i.e., English NLP tasks (e.g., commonsense reasoning, question answering), reasoning tasks with chain-of-thought (e.g., GSM8K), multilingual tasks (MGSM, TydiQA), MMLU and challenging BIG-Bench tasks. Finally, we provide qualitative examples showing the new capabilities of U-PaLM for single and multi-span infilling.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2210.11399",
                "caption": "Transcending Scaling Laws with 0.1% Extra Compute",
                "credit": "Yi Tay, Jason Wei, Hyung Won Chung, Vinh Q. Tran, David R. So, Siamak Shakeri, Xavier Garcia, Huaixiu Steven Zheng, Jinfeng Rao, Aakanksha Chowdhery, Denny Zhou, Donald Metzler, Slav Petrov, Neil Houlsby, Quoc V. Le, Mostafa Dehghani"
            }
        },
        {
            "start_date": {
                "year": 2022,
                "month": 10,
                "day": 20
            },
            "text": {
                "headline": "LMSI-Palm",
                "text": "<p>Large Language Models (LLMs) have achieved excellent performances in various tasks. However, fine-tuning an LLM requires extensive supervision. Human, on the other hand, may improve their reasoning abilities by self-thinking without external inputs. In this work, we demonstrate that an LLM is also capable of self-improving with only unlabeled datasets. We use a pre-trained LLM to generate \"high-confidence\" rationale-augmented answers for unlabeled questions using Chain-of-Thought prompting and self-consistency, and fine-tune the LLM using those self-generated solutions as target outputs. We show that our approach improves the general reasoning ability of a 540B-parameter LLM (74.4%->82.1% on GSM8K, 78.2%->83.0% on DROP, 90.0%->94.4% on OpenBookQA, and 63.4%->67.9% on ANLI-A3) and achieves state-of-the-art-level performance, without any ground truth label. We conduct ablation studies and show that fine-tuning on reasoning is critical for self-improvement.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2210.11610",
                "caption": "Large Language Models Can Self-Improve",
                "credit": "Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, Jiawei Han"
            }
        },
        {
            "start_date": {
                "year": 2022,
                "month": 10,
                "day": 20
            },
            "text": {
                "headline": "Flan-T5 11B",
                "text": "<p>Finetuning language models on a collection of datasets phrased as instructions has been shown to improve model performance and generalization to unseen tasks. In this paper we explore instruction finetuning with a particular focus on (1) scaling the number of tasks, (2) scaling the model size, and (3) finetuning on chain-of-thought data. We find that instruction finetuning with the above aspects dramatically improves performance on a variety of model classes (PaLM, T5, U-PaLM), prompting setups (zero-shot, few-shot, CoT), and evaluation benchmarks (MMLU, BBH, TyDiQA, MGSM, open-ended generation). For instance, Flan-PaLM 540B instruction-finetuned on 1.8K tasks outperforms PALM 540B by a large margin (+9.4% on average). Flan-PaLM 540B achieves state-of-the-art performance on several benchmarks, such as 75.2% on five-shot MMLU. We also publicly release Flan-T5 checkpoints, which achieve strong few-shot performance even compared to much larger models, such as PaLM 62B. Overall, instruction finetuning is a general method for improving the performance and usability of pretrained language models.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2210.11416, https://huggingface.co/google/flan-t5-xxl",
                "caption": "Scaling Instruction-Finetuned Language Models",
                "credit": "Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, Jason Wei"
            }
        },
        {
            "start_date": {
                "year": 2022,
                "month": 10,
                "day": 20
            },
            "text": {
                "headline": "Flan-PaLM 540B",
                "text": "<p>Finetuning language models on a collection of datasets phrased as instructions has been shown to improve model performance and generalization to unseen tasks. In this paper we explore instruction finetuning with a particular focus on (1) scaling the number of tasks, (2) scaling the model size, and (3) finetuning on chain-of-thought data. We find that instruction finetuning with the above aspects dramatically improves performance on a variety of model classes (PaLM, T5, U-PaLM), prompting setups (zero-shot, few-shot, CoT), and evaluation benchmarks (MMLU, BBH, TyDiQA, MGSM, open-ended generation). For instance, Flan-PaLM 540B instruction-finetuned on 1.8K tasks outperforms PALM 540B by a large margin (+9.4% on average). Flan-PaLM 540B achieves state-of-the-art performance on several benchmarks, such as 75.2% on five-shot MMLU. We also publicly release Flan-T5 checkpoints, which achieve strong few-shot performance even compared to much larger models, such as PaLM 62B. Overall, instruction finetuning is a general method for improving the performance and usability of pretrained language models.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2210.11416",
                "caption": "Scaling Instruction-Finetuned Language Models",
                "credit": "Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, Jason Wei"
            }
        },
        {
            "start_date": {
                "year": 2022,
                "month": 10,
                "day": 11
            },
            "text": {
                "headline": "GenSLM",
                "text": "<p>Our work seeks to transform how new and emergent variants of pandemic causing viruses, specially SARS-CoV-2, are identified and classified. By adapting large language models (LLMs) for genomic data, we build genome-scale language models (GenSLMs) which can learn the evolutionary landscape of SARS-CoV-2 genomes. By pretraining on over 10 million prokaryotic gene sequences, and then finetuning a SARS-CoV-2 specific model on 1.5 million genomes, we show that GenSLM can accurately and rapidly identify variants of concern. Thus, to our knowledge, GenSLM represents one of the first whole genome scale foundation models which can generalize to other prediction tasks. We demonstrate the scaling of GenSLMs on both GPU-based supercomputers and AI-hardware accelerators, achieving over 1.54 zettaflops in training runs. We present initial scientific insights gleaned from examining GenSLMs in tracking the evolutionary dynamics of SARS-CoV-2, noting that its full potential on large biological data is yet to be realized.</p>"
            },
            "media": {
                "url": "https://www.biorxiv.org/content/biorxiv/early/2022/10/11/2022.10.10.511571.full.pdf",
                "caption": "GenSLMs: Genome-scale language models reveal SARS-CoV-2 evolutionary dynamics",
                "credit": "Maxim Zvyagin, Alexander Brace, Kyle Hippe, Yuntian Deng, Bin Zhang, Cindy Orozco Bohorquez, Austin Clyde, Bharat Kale, Danilo Perez-Rivera, Heng Ma, Carla M. Mann, Michael Irvin, J. Gregory Pauloski, Logan Ward, Valerie Hayot, Murali Emani, Sam Foreman, Zhen Xie, Diangen Lin, Maulik Shukla, Weili Nie, Josh Romero, Christian Dallago, Arash Vahdat, Chaowei Xiao, Thomas Gibbs, Ian Foster, James J. Davis, Michael E. Papka, Thomas Brettin, Rick Stevens, Anima Anandkumar, Venkatram Vishwanath, Arvind Ramanathan"
            }
        },
        {
            "start_date": {
                "year": 2022,
                "month": 10,
                "day": 11
            },
            "text": {
                "headline": "Diplodocus",
                "text": "<p>No-press Diplomacy is a complex strategy game involving both cooperation and competition that has served as a benchmark for multi-agent AI research. While self-play reinforcement learning has resulted in numerous successes in purely adversarial games like chess, Go, and poker, self-play alone is insufficient for achieving optimal performance in domains involving cooperation with humans. We address this shortcoming by first introducing a planning algorithm we call DiL-piKL that regularizes a reward-maximizing policy toward a human imitation-learned policy. We prove that this is a no-regret learning algorithm under a modified utility function. We then show that DiL-piKL can be extended into a self-play reinforcement learning algorithm we call RL-DiL-piKL that provides a model of human play while simultaneously training an agent that responds well to this human model. We used RL-DiL-piKL to train an agent we name Diplodocus. In a 200-game no-press Diplomacy tournament involving 62 human participants spanning skill levels from beginner to expert, two Diplodocus agents both achieved a higher average score than all other participants who played more than two games, and ranked first and third according to an Elo ratings model.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2210.05492",
                "caption": "Mastering the Game of No-Press Diplomacy via Human-Regularized Reinforcement Learning and Planning",
                "credit": "Anton Bakhtin, David J Wu, Adam Lerer, Jonathan Gray, Athul Paul Jacob, Gabriele Farina, Alexander H Miller, Noam Brown"
            }
        },
        {
            "start_date": {
                "year": 2022,
                "month": 10,
                "day": 5
            },
            "text": {
                "headline": "Phenaki",
                "text": "<p>We present Phenaki, a model capable of realistic video synthesis, given a sequence of textual prompts. Generating videos from text is particularly challenging due to the computational cost, limited quantities of high quality text-video data and variable length of videos. To address these issues, we introduce a new model for learning video representation which compresses the video to a small representation of discrete tokens. This tokenizer uses causal attention in time, which allows it to work with variable-length videos. To generate video tokens from text we are using a bidirectional masked transformer conditioned on pre-computed text tokens. The generated video tokens are subsequently de-tokenized to create the actual video. To address data issues, we demonstrate how joint training on a large corpus of image-text pairs as well as a smaller number of video-text examples can result in generalization beyond what is available in the video datasets. Compared to the previous video generation methods, Phenaki can generate arbitrary long videos conditioned on a sequence of prompts (i.e. time variable text or a story) in open domain. To the best of our knowledge, this is the first time a paper studies generating videos from time variable prompts. In addition, compared to the per-frame baselines, the proposed video encoder-decoder computes fewer tokens per video but results in better spatio-temporal consistency.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2210.02399",
                "caption": "Phenaki: Variable Length Video Generation From Open Domain Textual Description",
                "credit": "Ruben Villegas, Mohammad Babaeizadeh, Pieter-Jan Kindermans, Hernan Moraldo, Han Zhang, Mohammad Taghi Saffar, Santiago Castro, Julius Kunze, Dumitru Erhan"
            }
        },
        {
            "start_date": {
                "year": 2022,
                "month": 10,
                "day": 4
            },
            "text": {
                "headline": "DiffDock",
                "text": "<p>Predicting the binding structure of a small molecule ligand to a protein -- a task known as molecular docking -- is critical to drug design. Recent deep learning methods that treat docking as a regression problem have decreased runtime compared to traditional search-based methods but have yet to offer substantial improvements in accuracy. We instead frame molecular docking as a generative modeling problem and develop DiffDock, a diffusion generative model over the non-Euclidean manifold of ligand poses. To do so, we map this manifold to the product space of the degrees of freedom (translational, rotational, and torsional) involved in docking and develop an efficient diffusion process on this space. Empirically, DiffDock obtains a 38% top-1 success rate (RMSD<2A) on PDBBind, significantly outperforming the previous state-of-the-art of traditional docking (23%) and deep learning (20%) methods. Moreover, while previous methods are not able to dock on computationally folded structures (maximum accuracy 10.4%), DiffDock maintains significantly higher precision (21.7%). Finally, DiffDock has fast inference times and provides confidence estimates with high selective accuracy.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2210.01776, https://docs.nvidia.com/bionemo-framework/latest/models/diffdock.html",
                "caption": "DiffDock: Diffusion Steps, Twists, and Turns for Molecular Docking",
                "credit": "Gabriele Corso, Hannes Stärk, Bowen Jing, Regina Barzilay, Tommi Jaakkola"
            }
        },
        {
            "start_date": {
                "year": 2022,
                "month": 9,
                "day": 29
            },
            "text": {
                "headline": "Make-A-Video",
                "text": "<p>We propose Make-A-Video -- an approach for directly translating the tremendous recent progress in Text-to-Image (T2I) generation to Text-to-Video (T2V). Our intuition is simple: learn what the world looks like and how it is described from paired text-image data, and learn how the world moves from unsupervised video footage. Make-A-Video has three advantages: (1) it accelerates training of the T2V model (it does not need to learn visual and multimodal representations from scratch), (2) it does not require paired text-video data, and (3) the generated videos inherit the vastness (diversity in aesthetic, fantastical depictions, etc.) of today's image generation models. We design a simple yet effective way to build on T2I models with novel and effective spatial-temporal modules. First, we decompose the full temporal U-Net and attention tensors and approximate them in space and time. Second, we design a spatial temporal pipeline to generate high resolution and frame rate videos with a video decoder, interpolation model and two super resolution models that can enable various applications besides T2V. In all aspects, spatial and temporal resolution, faithfulness to text, and quality, Make-A-Video sets the new state-of-the-art in text-to-video generation, as determined by both qualitative and quantitative measures.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2209.14792",
                "caption": "Make-A-Video: Text-to-Video Generation without Text-Video Data",
                "credit": "Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, Devi Parikh, Sonal Gupta, Yaniv Taigman"
            }
        },
        {
            "start_date": {
                "year": 2022,
                "month": 9,
                "day": 21
            },
            "text": {
                "headline": "Whisper",
                "text": "<p>We study the capabilities of speech processing systems trained simply to predict large amounts of transcripts of audio on the internet. When scaled to 680,000 hours of multilingual and multitask supervision, the resulting models generalize well to standard benchmarks and are often competitive with prior fully supervised results but in a zeroshot transfer setting without the need for any finetuning. When compared to humans, the models approach their accuracy and robustness. We are releasing models and inference code to serve as a foundation for further work on robust speech processing.</p>"
            },
            "media": {
                "url": "https://cdn.openai.com/papers/whisper.pdf\n\nhttps://arxiv.org/abs/2212.04356",
                "caption": "Robust Speech Recognition via Large-Scale Weak Supervision",
                "credit": "Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, Ilya Sutskever"
            }
        },
        {
            "start_date": {
                "year": 2022,
                "month": 9,
                "day": 14
            },
            "text": {
                "headline": "PaLI",
                "text": "<p>Effective scaling and a flexible task interface enable large language models to excel at many tasks. We present PaLI (Pathways Language and Image model), a model that extends this approach to the joint modeling of language and vision. PaLI generates text based on visual and textual inputs, and with this interface performs many vision, language, and multimodal tasks, in many languages. To train PaLI, we make use of large pre-trained encoder-decoder language models and Vision Transformers (ViTs). This allows us to capitalize on their existing capabilities and leverage the substantial cost of training them. We find that joint scaling of the vision and language components is important. Since existing Transformers for language are much larger than their vision counterparts, we train a large, 4-billion parameter ViT (ViT-e) to quantify the benefits from even larger-capacity vision models. To train PaLI, we create a large multilingual mix of pretraining tasks, based on a new image-text training set containing 10B images and texts in over 100 languages. PaLI achieves state-of-the-art in multiple vision and language tasks (such as captioning, visual question-answering, scene-text understanding), while retaining a simple, modular, and scalable design.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2209.06794v4",
                "caption": "PaLI: A Jointly-Scaled Multilingual Language-Image Model",
                "credit": "Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer, Alexander Kolesnikov, Joan Puigcerver, Nan Ding, Keran Rong, Hassan Akbari, Gaurav Mishra, Linting Xue, Ashish Thapliyal, James Bradbury, Weicheng Kuo, Mojtaba Seyedhosseini, Chao Jia, Burcu Karagol Ayan, Carlos Riquelme, Andreas Steiner, Anelia Angelova, Xiaohua Zhai, Neil Houlsby, Radu Soricut"
            }
        },
        {
            "start_date": {
                "year": 2022,
                "month": 8,
                "day": 22
            },
            "text": {
                "headline": "BEIT-3",
                "text": "<p>A big convergence of language, vision, and multimodal pretraining is emerging. In this work, we introduce a general-purpose multimodal foundation model BEiT-3, which achieves state-of-the-art transfer performance on both vision and vision-language tasks. Specifically, we advance the big convergence from three aspects: backbone architecture, pretraining task, and model scaling up. We introduce Multiway Transformers for general-purpose modeling, where the modular architecture enables both deep fusion and modality-specific encoding. Based on the shared backbone, we perform masked \"language\" modeling on images (Imglish), texts (English), and image-text pairs (\"parallel sentences\") in a unified manner. Experimental results show that BEiT-3 obtains state-of-the-art performance on object detection (COCO), semantic segmentation (ADE20K), image classification (ImageNet), visual reasoning (NLVR2), visual question answering (VQAv2), image captioning (COCO), and cross-modal retrieval (Flickr30K, COCO). </p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2208.10442",
                "caption": "Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks",
                "credit": "Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks"
            }
        },
        {
            "start_date": {
                "year": 2022,
                "month": 8,
                "day": 10
            },
            "text": {
                "headline": "BlenderBot 3",
                "text": "<p>We present BlenderBot 3, a 175B parameter dialogue model capable of open-domain conversation with access to the internet and a long-term memory, and having been trained on a large number of user defined tasks. We release both the model weights and code, and have also deployed the model on a public web page to interact with organic users. This technical report describes how the model was built (architecture, model and training scheme), and details of its deployment, including safety mechanisms. Human evaluations show its superiority to existing open-domain dialogue agents, including its predecessors (Roller et al., 2021; Komeili et al., 2022). Finally, we detail our plan for continual learning using the data collected from deployment, which will also be publicly released. The goal of this research program is thus to enable the community to study ever-improving responsible agents that learn through interaction.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2208.03188, https://github.com/facebookresearch/ParlAI/blob/main/parlai/zoo/bb3/model_card.md\n\ntraining code: https://parl.ai/projects/bb3/ ",
                "caption": "BlenderBot 3: a deployed conversational agent that continually learns to responsibly engage",
                "credit": "Kurt Shuster, Jing Xu, Mojtaba Komeili, Da Ju, Eric Michael Smith, Stephen Roller, Megan Ung, Moya Chen, Kushal Arora, Joshua Lane, Morteza Behrooz, William Ngan, Spencer Poff, Naman Goyal, Arthur Szlam, Y-Lan Boureau, Melanie Kambadur, Jason Weston"
            }
        },
        {
            "start_date": {
                "year": 2022,
                "month": 8,
                "day": 4
            },
            "text": {
                "headline": "GLM-130B",
                "text": "<p>GLM-130B (ICLR 2023) is an open bilingual (English & Chinese) bidirectional dense model with 130 billion parameters, pre-trained using the General Language Model (GLM) algorithm1. It is designed to support inference tasks with the 130B parameters on a single A100 (40G * 8) or V100 (32G * 8) server. As of July 3rd, 2022, GLM-130B has been trained on over 400 billion text tokens (200B each for Chinese and English) </p>"
            },
            "media": {
                "url": "https://keg.cs.tsinghua.edu.cn/glm-130b/posts/glm-130b/",
                "caption": "GLM-130B: An Open Bilingual Pre-trained Model",
                "credit": "Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, Weng Lam Tam, Zixuan Ma, Yufei Xue, Jidong Zhai, Wenguang Chen, Peng Zhang, Yuxiao Dong, Jie Tang"
            }
        },
        {
            "start_date": {
                "year": 2022,
                "month": 8,
                "day": 2
            },
            "text": {
                "headline": "AlexaTM 20B",
                "text": "<p>In this work, we demonstrate that multilingual large-scale sequence-to-sequence (seq2seq) models, pre-trained on a mixture of denoising and Causal Language Modeling (CLM) tasks, are more efficient few-shot learners than decoder-only models on various tasks. In particular, we train a 20 billion parameter multilingual seq2seq model called Alexa Teacher Model (AlexaTM 20B) and show that it achieves state-of-the-art (SOTA) performance on 1-shot summarization tasks, outperforming a much larger 540B PaLM decoder model. AlexaTM 20B also achieves SOTA in 1-shot machine translation, especially for low-resource languages, across almost all language pairs supported by the model (Arabic, English, French, German, Hindi, Italian, Japanese, Marathi, Portuguese, Spanish, Tamil, and Telugu) on Flores-101 dataset. We also show in zero-shot setting, AlexaTM 20B outperforms GPT3 (175B) on SuperGLUE and SQuADv2 datasets and provides SOTA performance on multilingual tasks such as XNLI, XCOPA, Paws-X, and XWinograd. Overall, our results present a compelling case for seq2seq models as a powerful alternative to decoder-only models for Large-scale Language Model (LLM) training. </p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2208.01448",
                "caption": "AlexaTM 20B: Few-Shot Learning Using a Large-Scale Multilingual Seq2Seq Model",
                "credit": "Saleh Soltan, Shankar Ananthakrishnan, Jack FitzGerald, Rahul Gupta, Wael Hamza, Haidar Khan, Charith Peris, Stephen Rawls, Andy Rosenbaum, Anna Rumshisky, Chandana Satya Prakash, Mukund Sridhar, Fabian Triefenbach, Apurv Verma, Gokhan Tur, Prem Natarajan"
            }
        },
        {
            "start_date": {
                "year": 2022,
                "month": 7,
                "day": 22
            },
            "text": {
                "headline": "OmegaPLM",
                "text": "<p>Recent breakthroughs have used deep learning to exploit evolutionary information in multiple sequence alignments (MSAs) to accurately predict protein structures. However, MSAs of homologous proteins are not always available, such as with orphan proteins or fast-evolving proteins like antibodies, and a protein typically folds in a natural setting from its primary amino acid sequence into its three-dimensional structure, suggesting that evolutionary information and MSAs should not be necessary to predict a protein’s folded form. Here, we introduce OmegaFold, the first computational method to successfully predict high-resolution protein structure from a single primary sequence alone. Using a new combination of a protein language model that allows us to make predictions from single sequences and a geometry-inspired transformer model trained on protein structures, OmegaFold outperforms RoseTTAFold and achieves similar prediction accuracy to AlphaFold2 on recently released structures. OmegaFold enables accurate predictions on orphan proteins that do not belong to any functionally characterized protein family and antibodies that tend to have noisy MSAs due to fast evolution. Our study fills a much-encountered gap in structure prediction and brings us a step closer to understanding protein folding in nature.</p>"
            },
            "media": {
                "url": "https://www.biorxiv.org/content/10.1101/2022.07.21.500999v1",
                "caption": "High-resolution de novo structure prediction from primary sequence",
                "credit": "Ruidong Wu, Fan Ding, Rui Wang, Rui Shen, Xiwen Zhang, Shitong Luo, Chenpeng Su, Zuofan Wu, Qi Xie, Bonnie Berger, Jianzhu Ma, Jian Peng"
            }
        },
        {
            "start_date": {
                "year": 2022,
                "month": 7,
                "day": 21
            },
            "text": {
                "headline": "ESM2-15B",
                "text": "<p>\"Recent advances in machine learning have leveraged evolutionary information in multiple sequence alignments to predict protein structure. We demonstrate direct inference of full atomic-level protein structure from primary sequence using a large language model. As language models of protein sequences are scaled up to 15 billion parameters, an atomic-resolution picture of protein structure emerges in the learned representations. This results in an order-of-magnitude acceleration of high-resolution structure prediction, which enables large-scale structural characterization of metagenomic proteins. We apply this capability to construct the ESM Metagenomic Atlas by predicting structures for >617 million metagenomic protein sequences, including >225 million that are predicted with high confidence, which gives a view into the vast breadth and diversity of natural proteins.\"</p>"
            },
            "media": {
                "url": "https://www.science.org/doi/abs/10.1126/science.ade2574\nhttps://www.biorxiv.org/content/10.1101/2022.07.20.500902v2",
                "caption": "Evolutionary-scale prediction of atomic-level protein structure with a language model",
                "credit": "Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita Smetanin, Robert Verkuil, Ori Kabeli, Yaniv Shmueli, Allan dos Santos Costa, Maryam Fazel-Zarandi, Tom Sercu, Salvatore Candido, Alexander Rives"
            }
        },
        {
            "start_date": {
                "year": 2022,
                "month": 7,
                "day": 11
            },
            "text": {
                "headline": "BLOOM-176B",
                "text": "<p>Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2211.05100",
                "caption": "BLOOM: A 176B-Parameter Open-Access Multilingual Language Model",
                "credit": "Margaret Mitchell, Giada Pistilli, Yacine Jernite, Ezinwanne Ozoani, Marissa Gerchick, Nazneen Rajani, Sasha Luccioni, Irene Solaiman, Maraim Masoud, Somaieh Nikpoor, Carlos Muñoz Ferrandis, Stas Bekman, Christopher Akiki, Danish Contractor, David Lansky, Angelina McMillan-Major, Tristan Thrush, Suzana Ilić, Gérard Dupont, Shayne Longpre, Manan Dey, Stella Biderman, Douwe Kiela, Emi Baylor, Teven Le Scao, Aaron Gokaslan, Julien Launay, Niklas Muennighoff"
            }
        },
        {
            "start_date": {
                "year": 2022,
                "month": 7,
                "day": 6
            },
            "text": {
                "headline": "NLLB",
                "text": "<p>Driven by the goal of eradicating language barriers on a global scale, machine translation has solidified itself as a key focus of artificial intelligence research today. However, such efforts have coalesced around a small subset of languages, leaving behind the vast majority of mostly low-resource languages. What does it take to break the 200 language barrier while ensuring safe, high quality results, all while keeping ethical considerations in mind? In No Language Left Behind, we took on this challenge by first contextualizing the need for low-resource language translation support through exploratory interviews with native speakers. Then, we created datasets and models aimed at narrowing the performance gap between low and high-resource languages. More specifically, we developed a conditional compute model based on Sparsely Gated Mixture of Experts that is trained on data obtained with novel and effective data mining techniques tailored for low-resource languages. We propose multiple architectural and training improvements to counteract overfitting while training on thousands of tasks. Critically, we evaluated the performance of over 40,000 different translation directions using a human-translated benchmark, Flores-200, and combined human evaluation with a novel toxicity benchmark covering all languages in Flores-200 to assess translation safety. Our model achieves an improvement of 44% BLEU relative to the previous state-of-the-art, laying important groundwork towards realizing a universal translation system. Finally, we open source all contributions described in this work, accessible at https://github.com/facebookresearch/fairseq/tree/nllb.</p>"
            },
            "media": {
                "url": "https://research.facebook.com/publications/no-language-left-behind/",
                "caption": "No Language Left Behind: Scaling Human-Centered Machine Translation",
                "credit": "Marta R. Costa-jussà, James Cross, Onur Çelebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, Anna Sun, Skyler Wang, Guillaume Wenzek, Al Youngblood, Bapi Akula, Loic Barrault, Gabriel Mejia Gonzalez, Prangthip Hansanti, John Hoffman, Semarley Jarrett, Kaushik Ram Sadagopan, Dirk Rowe, Shannon Spruit, Chau Tran, Pierre Andrews, Necip Fazil Ayan, Shruti Bhosale, Sergey Edunov, Angela Fan, Cynthia Gao, Vedanuj Goswami, Francisco (Paco) Guzmán, Philipp Koehn, Alexandre Mourachko, Christophe Ropers, Safiyyah Saleem, Holger Schwenk, Jeff Wang"
            }
        },
        {
            "start_date": {
                "year": 2022,
                "month": 7,
                "day": 5
            },
            "text": {
                "headline": "CodeT5-large",
                "text": "<p>\"Program synthesis or code generation aims to generate a program that satisfies a problem specification. Recent approaches using large-scale pretrained language models (LMs) have shown promising results, yet they have some critical limitations. In particular, they often follow a standard supervised fine-tuning procedure to train a code generation model only from the pairs of natural-language problem descriptions and ground-truth programs. Such paradigm largely ignores some important but potentially useful signals in the problem specification such as unit tests, which thus often results in poor performance when solving complex unseen coding tasks. To address the limitations, we propose \"CodeRL\", a new framework for program synthesis tasks through pretrained LMs and deep reinforcement learning (RL). Specifically, during training, we treat the code-generating LM as an actor network, and introduce a critic network that is trained to predict the functional correctness of generated programs and provide dense feedback signals to the actor. During inference, we introduce a new generation procedure with a critical sampling strategy that allows a model to automatically regenerate programs based on feedback from example unit tests and critic scores. For the model backbones, we extended the encoder-decoder architecture of CodeT5 with enhanced learning objectives, larger model sizes, and better pretraining data. Our method not only achieves new SOTA results on the challenging APPS benchmark, but also shows strong zero-shot transfer capability with new SOTA results on the simpler MBPP benchmark.\"</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2207.01780",
                "caption": "CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning",
                "credit": "Hung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio Savarese, Steven C.H. Hoi "
            }
        },
        {
            "start_date": {
                "year": 2022,
                "month": 6,
                "day": 29
            },
            "text": {
                "headline": "Minerva (540B)",
                "text": "<p>Language models have achieved remarkable performance on a wide range of tasks that require natural language understanding. Nevertheless, state-of-the-art models have generally struggled with tasks that require quantitative reasoning, such as solving mathematics, science, and engineering problems at the college level. To help close this gap, we introduce Minerva, a large language model pretrained on general natural language data and further trained on technical content. The model achieves state-of-the-art performance on technical benchmarks without the use of external tools. We also evaluate our model on over two hundred undergraduate-level problems in physics, biology, chemistry, economics, and other sciences that require quantitative reasoning, and find that the model can correctly answer nearly a third of them.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2206.14858",
                "caption": "Solving Quantitative Reasoning Problems with Language Models",
                "credit": "Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, Vedant Misra"
            }
        },
        {
            "start_date": {
                "year": 2022,
                "month": 6,
                "day": 27
            },
            "text": {
                "headline": "ProGen2-xlarge",
                "text": "<p>Attention-based models trained on protein sequences have demonstrated incredible success at classification and generation tasks relevant for artificial intelligence- driven protein design. However, we lack a sufficient understanding of how very large-scale models and data play a role in effective protein model development. We introduce a suite of protein language models, named ProGen2, that are scaled up to 6.4B parameters and trained on different sequence datasets drawn from over a billion proteins from genomic, metagenomic, and immune repertoire databases. ProGen2 models show state-of-the-art performance in capturing the distribution of observed evolutionary sequences, generating novel viable sequences, and predicting protein fitness without additional finetuning. As large model sizes and raw numbers of protein sequences continue to become more widely accessible, our results suggest that a growing emphasis needs to be placed on the data distribution provided to a protein sequence model. We release the ProGen2 models and code at https://github.com/salesforce/progen.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2206.13517",
                "caption": "ProGen2: Exploring the Boundaries of Protein Language Models",
                "credit": "Erik Nijkamp, Jeffrey Ruffolo, Eli N. Weinstein, Nikhil Naik, Ali Madani\n"
            }
        },
        {
            "start_date": {
                "year": 2022,
                "month": 6,
                "day": 22
            },
            "text": {
                "headline": "Parti",
                "text": "<p>We present the Pathways Autoregressive Text-to-Image (Parti) model, which generates high-fidelity photorealistic images and supports content-rich synthesis involving complex compositions and world knowledge. Parti treats text-to-image generation as a sequence-to-sequence modeling problem, akin to machine translation, with sequences of image tokens as the target outputs rather than text tokens in another language. This strategy can naturally tap into the rich body of prior work on large language models, which have seen continued advances in capabilities and performance through scaling data and model sizes. Our approach is simple: First, Parti uses a Transformer-based image tokenizer, ViT-VQGAN, to encode images as sequences of discrete tokens. Second, we achieve consistent quality improvements by scaling the encoder-decoder Transformer model up to 20B parameters, with a new state-of-the-art zero-shot FID score of 7.23 and finetuned FID score of 3.22 on MS-COCO. Our detailed analysis on Localized Narratives as well as PartiPrompts (P2), a new holistic benchmark of over 1600 English prompts, demonstrate the effectiveness of Parti across a wide variety of categories and difficulty aspects. We also explore and highlight limitations of our models in order to define and exemplify key areas of focus for further improvements. See https://parti.research.google/ for high-resolution images.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2206.10789v1",
                "caption": "Scaling Autoregressive Models for Content-Rich Text-to-Image Generation",
                "credit": "Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, Ben Hutchinson, Wei Han, Zarana Parekh, Xin Li, Han Zhang, Jason Baldridge, Yonghui Wu"
            }
        },
        {
            "start_date": {
                "year": 2022,
                "month": 6,
                "day": 14
            },
            "text": {
                "headline": "CoCa",
                "text": "<p>Exploring large-scale pretrained foundation models is of significant interest in computer vision because these models can be quickly transferred to many downstream tasks. This paper presents Contrastive Captioner (CoCa), a minimalist design to pretrain an image-text encoder-decoder foundation model jointly with contrastive loss and captioning loss, thereby subsuming model capabilities from contrastive approaches like CLIP and generative methods like SimVLM. In contrast to standard encoder-decoder transformers where all decoder layers attend to encoder outputs, CoCa omits cross-attention in the first half of decoder layers to encode unimodal text representations, and cascades the remaining decoder layers which cross-attend to the image encoder for multimodal image-text representations. We apply a contrastive loss between unimodal image and text embeddings, in addition to a captioning loss on the multimodal decoder outputs which predicts text tokens autoregressively. By sharing the same computational graph, the two training objectives are computed efficiently with minimal overhead. CoCa is pretrained end-to-end and from scratch on both web-scale alt-text data and annotated images by treating all labels simply as text, seamlessly unifying natural language supervision for representation learning. Empirically, CoCa achieves state-of-the-art performance with zero-shot transfer or minimal task-specific adaptation on a broad range of downstream tasks, spanning visual recognition (ImageNet, Kinetics-400/600/700, Moments-in-Time), crossmodal retrieval (MSCOCO, Flickr30K, MSR-VTT), multimodal understanding (VQA, SNLI-VE, NLVR2), and image captioning (MSCOCO, NoCaps). Notably on ImageNet classification, CoCa obtains 86.3% zero-shot top-1 accuracy, 90.6% with a frozen encoder and learned classification head, and new state-of-the-art 91.0% top-1 accuracy on ImageNet with a finetuned encoder.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2205.01917v2",
                "caption": "CoCa: Contrastive Captioners are Image-Text Foundation Models",
                "credit": "Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, Yonghui Wu"
            }
        },
        {
            "start_date": {
                "year": 2022,
                "month": 6,
                "day": 13
            },
            "text": {
                "headline": "MetaLM",
                "text": "<p>Foundation models have received much attention due to their effectiveness across a broad range of downstream applications. Though there is a big convergence in terms of architecture, most pretrained models are typically still developed for specific tasks or modalities. In this work, we propose to use language models as a general-purpose interface to various foundation models. A collection of pretrained encoders perceive diverse modalities (such as vision, and language), and they dock with a language model that plays the role of a universal task layer. We propose a semi-causal language modeling objective to jointly pretrain the interface and the modular encoders. We subsume the advantages and capabilities from both causal and non-causal modeling, thereby combining the best of two worlds. Specifically, the proposed method not only inherits the capabilities of in-context learning and open-ended generation from causal language modeling, but also is conducive to finetuning because of the bidirectional encoders. More importantly, our approach seamlessly unlocks the combinations of the above capabilities, e.g., enabling in-context learning or instruction following with finetuned encoders. Experimental results across various language-only and vision-language benchmarks show that our model outperforms or is competitive with specialized models on finetuning, zero-shot generalization, and few-shot learning.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2206.06336v1",
                "caption": "Language Models are General-Purpose Interfaces",
                "credit": "Yaru Hao, Haoyu Song, Li Dong, Shaohan Huang, Zewen Chi, Wenhui Wang, Shuming Ma, Furu Wei"
            }
        },
        {
            "start_date": {
                "year": 2022,
                "month": 6,
                "day": 6
            },
            "text": {
                "headline": "DITTO",
                "text": "<p>While large-scale neural language models, such as GPT2 and BART, have achieved impressive results on various text generation tasks, they tend to get stuck in undesirable sentence-level loops with maximization-based decoding algorithms (\\textit{e.g.}, greedy search). This phenomenon is counter-intuitive since there are few consecutive sentence-level repetitions in human corpora (e.g., 0.02\\% in Wikitext-103). To investigate the underlying reasons for generating consecutive sentence-level repetitions, we study the relationship between the probabilities of the repetitive tokens and their previous repetitions in the context. Through our quantitative experiments, we find that 1) Language models have a preference to repeat the previous sentence; 2) The sentence-level repetitions have a \\textit{self-reinforcement effect}: the more times a sentence is repeated in the context, the higher the probability of continuing to generate that sentence; 3) The sentences with higher initial probabilities usually have a stronger self-reinforcement effect. Motivated by our findings, we propose a simple and effective training method \\textbf{DITTO} (Pseu\\underline{D}o-Repet\\underline{IT}ion Penaliza\\underline{T}i\\underline{O}n), where the model learns to penalize probabilities of sentence-level repetitions from pseudo repetitive data. Although our method is motivated by mitigating repetitions, experiments show that DITTO not only mitigates the repetition issue without sacrificing perplexity, but also achieves better generation quality. Extensive experiments on open-ended text generation (Wikitext-103) and text summarization (CNN/DailyMail) demonstrate the generality and effectiveness of our method.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2206.02369",
                "caption": "Learning to Break the Loop: Analyzing and Mitigating Repetitions for Neural Text Generation",
                "credit": "Jin Xu, Xiaojiang Liu, Jianhao Yan, Deng Cai, Huayang Li, Jian Li"
            }
        },
        {
            "start_date": {
                "year": 2022,
                "month": 6,
                "day": 5
            },
            "text": {
                "headline": "Diffusion-GAN",
                "text": "<p>Generative adversarial networks (GANs) are challenging to train stably, and a promising remedy of injecting instance noise into the discriminator input has not been very effective in practice. In this paper, we propose Diffusion-GAN, a novel GAN framework that leverages a forward diffusion chain to generate Gaussianmixture distributed instance noise. Diffusion-GAN consists of three components, including an adaptive diffusion process, a diffusion timestep-dependent discriminator, and a generator. Both the observed and generated data are diffused by the same adaptive diffusion process. At each diffusion timestep, there is a different noise-to-data ratio and the timestep-dependent discriminator learns to distinguish the diffused real data from the diffused generated data. The generator learns from the discriminator’s feedback by backpropagating through the forward diffusion chain, whose length is adaptively adjusted to balance the noise and data levels. We theoretically show that the discriminator’s timestep-dependent strategy gives consistent and helpful guidance to the generator, enabling it to match the true data distribution. We demonstrate the advantages of Diffusion-GAN over strong GAN baselines on various datasets, showing that it can produce more realistic images with higher stability and data efficiency than state-of-the-art GANs.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2206.02262v4",
                "caption": "Diffusion-GAN: Training GANs with Diffusion",
                "credit": "Zhendong Wang, Huangjie Zheng, Pengcheng He, Weizhu Chen, Mingyuan Zhou"
            }
        },
        {
            "start_date": {
                "year": 2022,
                "month": 5,
                "day": 29
            },
            "text": {
                "headline": "CogVideo",
                "text": "<p>Large-scale pretrained transformers have created milestones in text (GPT-3) and text-to-image (DALL-E and CogView) generation. Its application to video generation is still facing many challenges: The potential huge computation cost makes the training from scratch unaffordable; The scarcity and weak relevance of text-video datasets hinder the model understanding complex movement semantics. In this work, we present 9B-parameter transformer CogVideo, trained by inheriting a pretrained text-to-image model, CogView2. We also propose multi-frame-rate hierarchical training strategy to better align text and video clips. As (probably) the first open-source large-scale pretrained text-to-video model, CogVideo outperforms all publicly available models at a large margin in machine and human evaluations.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2205.15868",
                "caption": "CogVideo: Large-scale Pretraining for Text-to-Video Generation via Transformers",
                "credit": "Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, Jie Tang"
            }
        },
        {
            "start_date": {
                "year": 2022,
                "month": 5,
                "day": 27
            },
            "text": {
                "headline": "Tranception",
                "text": "<p>The ability to accurately model the fitness landscape of protein sequences is critical to a wide range of applications, from quantifying the effects of human variants on disease likelihood, to predicting immune-escape mutations in viruses and designing novel biotherapeutic proteins. Deep generative models of protein sequences trained on multiple sequence alignments have been the most successful approaches so far to address these tasks. The performance of these methods is however contingent on the availability of sufficiently deep and diverse alignments for reliable training. Their potential scope is thus limited by the fact many protein families are hard, if not impossible, to align. Large language models trained on massive quantities of non-aligned protein sequences from diverse families address these problems and show potential to eventually bridge the performance gap. We introduce Tranception, a novel transformer architecture leveraging autoregressive predictions and retrieval of homologous sequences at inference to achieve state-of-the-art fitness prediction performance. Given its markedly higher performance on multiple mutants, robustness to shallow alignments and ability to score indels, our approach offers significant gain of scope over existing approaches. To enable more rigorous model testing across a broader range of protein families, we develop ProteinGym -- an extensive set of multiplexed assays of variant effects, substantially increasing both the number and diversity of assays compared to existing benchmarks.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2205.13760",
                "caption": "Tranception: protein fitness prediction with autoregressive transformers and inference-time retrieval",
                "credit": "Pascal Notin, Mafalda Dias, Jonathan Frazer, Javier Marchena-Hurtado, Aidan Gomez, Debora S. Marks, Yarin Gal"
            }
        },
        {
            "start_date": {
                "year": 2022,
                "month": 5,
                "day": 23
            },
            "text": {
                "headline": "Imagen",
                "text": "<p>We present Imagen, a text-to-image diffusion model with an unprecedented degree of photorealism and a deep level of language understanding. Imagen builds on the power of large transformer language models in understanding text and hinges on the strength of diffusion models in high-fidelity image generation. Our key discovery is that generic large language models (e.g. T5), pretrained on text-only corpora, are surprisingly effective at encoding text for image synthesis: increasing the size of the language model in Imagen boosts both sample fidelity and image-text alignment much more than increasing the size of the image diffusion model. Imagen achieves a new state-of-the-art FID score of 7.27 on the COCO dataset, without ever training on COCO, and human raters find Imagen samples to be on par with the COCO data itself in image-text alignment. To assess text-to-image models in greater depth, we introduce DrawBench, a comprehensive and challenging benchmark for text-to-image models. With DrawBench, we compare Imagen with recent methods including VQ-GAN+CLIP, Latent Diffusion Models, and DALL-E 2, and find that human raters prefer Imagen over other models in side-by-side comparisons, both in terms of sample quality and image-text alignment.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2205.11487",
                "caption": "Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding",
                "credit": "Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David J Fleet, Mohammad Norouzi"
            }
        },
        {
            "start_date": {
                "year": 2022,
                "month": 5,
                "day": 18
            },
            "text": {
                "headline": "SimCSE",
                "text": "<p>This paper presents SimCSE, a simple contrastive learning framework that greatly advances state-of-the-art sentence embeddings. We first describe an unsupervised approach, which takes an input sentence and predicts itself in a contrastive objective, with only standard dropout used as noise. This simple method works surprisingly well, performing on par with previous supervised counterparts. We find that dropout acts as minimal data augmentation, and removing it leads to a representation collapse. Then, we propose a supervised approach, which incorporates annotated pairs from natural language inference datasets into our contrastive learning framework by using \"entailment\" pairs as positives and \"contradiction\" pairs as hard negatives. We evaluate SimCSE on standard semantic textual similarity (STS) tasks, and our unsupervised and supervised models using BERT base achieve an average of 76.3% and 81.6% Spearman's correlation respectively, a 4.2% and 2.2% improvement compared to the previous best results. We also show -- both theoretically and empirically -- that the contrastive learning objective regularizes pre-trained embeddings' anisotropic space to be more uniform, and it better aligns positive pairs when supervised signals are available.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2104.08821",
                "caption": "SimCSE: Simple Contrastive Learning of Sentence Embeddings",
                "credit": "Tianyu Gao, Xingcheng Yao, Danqi Chen"
            }
        },
        {
            "start_date": {
                "year": 2022,
                "month": 5,
                "day": 12
            },
            "text": {
                "headline": "Gato",
                "text": "<p>Inspired by progress in large-scale language modeling, we apply a similar approach towards building a single generalist agent beyond the realm of text outputs. The agent, which we refer to as Gato, works as a multi-modal, multi-task, multi-embodiment generalist policy. The same network with the same weights can play Atari, caption images, chat, stack blocks with a real robot arm and much more, deciding based on its context whether to output text, joint torques, button presses, or other tokens. In this report we describe the model and the data, and document the current capabilities of Gato.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2205.06175",
                "caption": "A Generalist Agent",
                "credit": "Scott Reed, Konrad Żołna, Emilio Parisotto, Sergio Gómez Colmenarejo, Alexander Novikov, Gabriel Barth-Maron, Mai Giménez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, Tom Eccles, Jake Bruce, Ali Razavi, Ashley Edwards, Nicolas Heess, Yutian Chen, Raia Hadsell, Oriol Vinyals, Mahyar Bordbar, Nando de Freitas"
            }
        },
        {
            "start_date": {
                "year": 2022,
                "month": 5,
                "day": 10
            },
            "text": {
                "headline": "UL2",
                "text": "<p>Existing pre-trained models are generally geared towards a particular class of problems. To date, there seems to be still no consensus on what the right architecture and pre-training setup should be. This paper presents a unified framework for pre-training models that are universally effective across datasets and setups. We begin by disentangling architectural archetypes with pre-training objectives -- two concepts that are commonly conflated. Next, we present a generalized and unified perspective for self-supervision in NLP and show how different pre-training objectives can be cast as one another and how interpolating between different objectives can be effective. We then propose Mixture-of-Denoisers (MoD), a pre-training objective that combines diverse pre-training paradigms together. We furthermore introduce a notion of mode switching, wherein downstream fine-tuning is associated with specific pre-training schemes. We conduct extensive ablative experiments to compare multiple pre-training objectives and find that our method pushes the Pareto-frontier by outperforming T5 and/or GPT-like models across multiple diverse setups. Finally, by scaling our model up to 20B parameters, we achieve SOTA performance on 50 well-established supervised NLP tasks ranging from language generation (with automated and human evaluation), language understanding, text classification, question answering, commonsense reasoning, long text reasoning, structured knowledge grounding and information retrieval. Our model also achieve strong results at in-context learning, outperforming 175B GPT-3 on zero-shot SuperGLUE and tripling the performance of T5-XXL on one-shot summarization. We release Flax-based T5X model checkpoints for the 20B model at \\url{this https URL}.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2205.05131v1",
                "caption": "Unifying Language Learning Paradigms",
                "credit": "Yi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier Garcia, Dara Bahri, Tal Schuster, Huaixiu Steven Zheng, Neil Houlsby, Donald Metzler"
            }
        },
        {
            "start_date": {
                "year": 2022,
                "month": 5,
                "day": 4
            },
            "text": {
                "headline": "DeBERTaV3large + KEAR",
                "text": "<p>Most of today's AI systems focus on using self-attention mechanisms and transformer architectures on large amounts of diverse data to achieve impressive performance gains. In this paper, we propose to augment the transformer architecture with an external attention mechanism to bring external knowledge and context to bear. By integrating external information into the prediction process, we hope to reduce the need for ever-larger models and increase the democratization of AI systems. We find that the proposed external attention mechanism can significantly improve the performance of existing AI systems, allowing practitioners to easily customize foundation AI models to many diverse downstream applications. In particular, we focus on the task of Commonsense Reasoning, demonstrating that the proposed external attention mechanism can augment existing transformer models and significantly improve the model's reasoning capabilities. The proposed system, Knowledgeable External Attention for commonsense Reasoning (KEAR), reaches human parity on the open CommonsenseQA research benchmark with an accuracy of 89.4\\% in comparison to the human accuracy of 88.9\\%.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2112.03254v3",
                "caption": "Human Parity on CommonsenseQA: Augmenting Self-Attention with External Attention",
                "credit": "Yichong Xu, Chenguang Zhu, Shuohang Wang, Siqi Sun, Hao Cheng, Xiaodong Liu, Jianfeng Gao, Pengcheng He, Michael Zeng, Xuedong Huang"
            }
        },
        {
            "start_date": {
                "year": 2022,
                "month": 5,
                "day": 2
            },
            "text": {
                "headline": "OPT-175B",
                "text": "<p>Large language models, which are often trained for hundreds of thousands of compute days, have shown remarkable capabilities for zero- and few-shot learning. Given their computational cost, these models are difficult to replicate without significant capital. For the few that are available through APIs, no access is granted to the full model weights, making them difficult to study. We present Open Pre-trained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers. We show that OPT-175B is comparable to GPT-3,1 while requiring only 1/7th the carbon footprint to develop. We are also releasing our logbook detailing the infrastructure challenges we faced, along with code for experimenting with all of the released models</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2205.01068",
                "caption": "OPT: Open Pre-trained Transformer Language Models",
                "credit": "Susan Zhang∗ , Stephen Roller∗ , Naman Goyal∗ , Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott† , Sam Shleifer† , Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, Luke Zettlemoyer"
            }
        },
        {
            "start_date": {
                "year": 2022,
                "month": 4,
                "day": 29
            },
            "text": {
                "headline": "Flamingo",
                "text": "<p>Building models that can be rapidly adapted to novel tasks using only a handful of annotated examples is an open challenge for multimodal machine learning research. We introduce Flamingo, a family of Visual Language Models (VLM) with this ability. We propose key architectural innovations to: (i) bridge powerful pretrained vision-only and language-only models, (ii) handle sequences of arbitrarily interleaved visual and textual data, and (iii) seamlessly ingest images or videos as inputs. Thanks to their flexibility, Flamingo models can be trained on large-scale multimodal web corpora containing arbitrarily interleaved text and images, which is key to endow them with in-context few-shot learning capabilities. We perform a thorough evaluation of our models, exploring and measuring their ability to rapidly adapt to a variety of image and video tasks. These include open-ended tasks such as visual question-answering, where the model is prompted with a question which it has to answer; captioning tasks, which evaluate the ability to describe a scene or an event; and close-ended tasks such as multiple-choice visual question-answering. For tasks lying anywhere on this spectrum, a single Flamingo model can achieve a new state of the art with few-shot learning, simply by prompting the model with task-specific examples. On numerous benchmarks, Flamingo outperforms models fine-tuned on thousands of times more task-specific data.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2204.14198",
                "caption": "Flamingo: a Visual Language Model for Few-Shot Learning",
                "credit": "Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andrew Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, Karen Simonyan"
            }
        },
        {
            "start_date": {
                "year": 2022,
                "month": 4,
                "day": 14
            },
            "text": {
                "headline": "Sparse all-MLP",
                "text": "<p>All-MLP architectures have attracted increasing interest as an alternative to attention-based models. In NLP, recent work like gMLP shows that all-MLPs can match Transformers in language modeling, but still lag behind in downstream tasks. In this work, we analyze the limitations of MLPs in expressiveness, and propose sparsely activated MLPs with mixture-of-experts (MoEs) in both feature and input (token) dimensions. Such sparse all-MLPs significantly increase model capacity and expressiveness while keeping the compute constant. We address critical challenges in incorporating conditional computation with two routing strategies. The proposed sparse all-MLP improves language modeling perplexity and obtains up to 2× improvement in training efficiency compared to both Transformer-based MoEs (GShard, Switch Transformer, Base Layers and HASH Layers) as well as dense Transformers and all-MLPs. Finally, we evaluate its zero-shot in-context learning performance on six downstream tasks, and find that it surpasses Transformer-based MoEs and dense Transformers.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2203.06850",
                "caption": "Efficient Language Modeling with Sparse all-MLP",
                "credit": "Ping Yu, Mikel Artexte, Myle Ott, Sam Shleifer, Hongyu Gong, Ves Stoyanov, Xian Li"
            }
        },
        {
            "start_date": {
                "year": 2022,
                "month": 4,
                "day": 13
            },
            "text": {
                "headline": "Stable Diffusion (LDM-KL-8-G)",
                "text": "<p>By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve a new state of the art for image inpainting and highly competitive performance on various tasks, including unconditional image generation, semantic scene synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs. Code is available at this https URL .</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2112.10752",
                "caption": "High-Resolution Image Synthesis with Latent Diffusion Models",
                "credit": "Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Björn Ommer"
            }
        },
        {
            "start_date": {
                "year": 2022,
                "month": 4,
                "day": 7
            },
            "text": {
                "headline": "BERT-RBP",
                "text": "<p>Motivation\nThe accumulation of sequencing data has enabled researchers to predict the interactions between RNA sequences and RNA-binding proteins (RBPs) using novel machine learning techniques. However, existing models are often difficult to interpret and require additional information to sequences. Bidirectional encoder representations from transformer (BERT) is a language-based deep learning model that is highly interpretable. Therefore, a model based on BERT architecture can potentially overcome such limitations.\n\nResults\nHere, we propose BERT-RBP as a model to predict RNA–RBP interactions by adapting the BERT architecture pretrained on a human reference genome. Our model outperformed state-of-the-art prediction models using the eCLIP-seq data of 154 RBPs. The detailed analysis further revealed that BERT-RBP could recognize both the transcript region type and RNA secondary structure only based on sequence information. Overall, the results provide insights into the fine-tuning mechanism of BERT in biological contexts and provide evidence of the applicability of the model to other RNA-related problems.</p>"
            },
            "media": {
                "url": "https://academic.oup.com/bioinformaticsadvances/article/2/1/vbac023/6564689",
                "caption": "Prediction of RNA–protein interactions using a nucleotide language model",
                "credit": "Keisuke Yamada, Michiaki Hamada"
            }
        },
        {
            "start_date": {
                "year": 2022,
                "month": 4,
                "day": 6
            },
            "text": {
                "headline": "DALL·E 2",
                "text": "<p>Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we propose a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter\nare computationally more efficient and produce higher-quality samples.</p>"
            },
            "media": {
                "url": "https://cdn.openai.com/papers/dall-e-2.pdf",
                "caption": "Hierarchical Text-Conditional Image Generation with CLIP Latents",
                "credit": "Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, Mark Chen"
            }
        },
        {
            "start_date": {
                "year": 2022,
                "month": 4,
                "day": 4
            },
            "text": {
                "headline": "PaLM (540B)",
                "text": "<p>Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2204.02311",
                "caption": "PaLM: Scaling Language Modeling with Pathways",
                "credit": "Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev,, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta ,Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, Noah Fiedel"
            }
        },
        {
            "start_date": {
                "year": 2022,
                "month": 3,
                "day": 29
            },
            "text": {
                "headline": "Chinchilla",
                "text": "<p>We investigate the optimal model size and number of tokens for training a transformer language model under a given compute budget. We find that current large language models are significantly undertrained, a consequence of the recent focus on scaling language models whilst keeping the amount of training data constant. By training over \\nummodels language models ranging from 70 million to over 16 billion parameters on 5 to 500 billion tokens, we find that for compute-optimal training, the model size and the number of training tokens should be scaled equally: for every doubling of model size the number of training tokens should also be doubled. We test this hypothesis by training a predicted compute-optimal model, \\chinchilla, that uses the same compute budget as \\gopher but with 70B parameters and 4× more more data. \\chinchilla uniformly and significantly outperforms \\Gopher (280B), GPT-3 (175B), Jurassic-1 (178B), and Megatron-Turing NLG (530B) on a large range of downstream evaluation tasks. This also means that \\chinchilla uses substantially less compute for fine-tuning and inference, greatly facilitating downstream usage. As a highlight, \\chinchilla reaches a state-of-the-art average accuracy of 67.5\\% on the MMLU benchmark, greater than a 7\\% improvement over \\gopher.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2203.15556",
                "caption": "Training Compute-Optimal Large Language Models",
                "credit": "Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, Laurent Sifre"
            }
        },
        {
            "start_date": {
                "year": 2022,
                "month": 3,
                "day": 21
            },
            "text": {
                "headline": "Segatron-XL large, M=384 + HCP",
                "text": "<p>Class-based language models (LMs) have been long devised to address context sparsity in n-gram LMs. In this study, we revisit this approach in the context of neural LMs. We hypothesize that class-based prediction leads to an implicit context aggregation for similar words and thus can improve generalization for rare words. We map words that have a common WordNet hypernym to the same class and train large neural LMs by gradually annealing from predicting the class to token prediction during training. Empirically, this curriculum learning strategy consistently improves perplexity over various large, highly-performant state-of-the-art Transformer-based models on two datasets, WikiText-103 and Arxiv. Our analysis shows that the performance improvement is achieved without sacrificing performance on rare words. Finally, we document other attempts that failed to yield empirical gains, and discuss future directions for the adoption of class-based LMs on a larger scale.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2203.10692",
                "caption": "Better Language Model with Hypernym Class Prediction",
                "credit": "He Bai, Tong Wang, Alessandro Sordoni, Peng Shi"
            }
        },
        {
            "start_date": {
                "year": 2022,
                "month": 3,
                "day": 10
            },
            "text": {
                "headline": "ViT-G (model soup)",
                "text": "<p>The conventional recipe for maximizing model accuracy is to (1) train multiple models with various hyperparameters and (2) pick the individual model which performs best on a held-out validation set, discarding the remainder. In this paper, we revisit the second step of this procedure in the context of fine-tuning large pre-trained models, where fine-tuned models often appear to lie in a single low error basin. We show that averaging the weights of multiple models fine-tuned with different hyperparameter configurations often improves accuracy and robustness. Unlike a conventional ensemble, we may average many models without incurring any additional inference or memory costs -- we call the results \"model soups.\" When fine-tuning large pre-trained models such as CLIP, ALIGN, and a ViT-G pre-trained on JFT, our soup recipe provides significant improvements over the best model in a hyperparameter sweep on ImageNet. The resulting ViT-G model, which attains 90.94% top-1 accuracy on ImageNet, achieved a new state of the art. Furthermore, we show that the model soup approach extends to multiple image classification and natural language processing tasks, improves out-of-distribution performance, and improves zero-shot performance on new downstream tasks. Finally, we analytically relate the performance similarity of weight-averaging and logit-ensembling to flatness of the loss and confidence of the predictions, and validate this relation empirically. Code is available at this https URL.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2203.05482v3",
                "caption": "Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time",
                "credit": "Mitchell Wortsman, Gabriel Ilharco, Samir Yitzhak Gadre, Rebecca Roelofs, Raphael Gontijo-Lopes, Ari S. Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, Ludwig Schmidt"
            }
        },
        {
            "start_date": {
                "year": 2022,
                "month": 3,
                "day": 7
            },
            "text": {
                "headline": "MegaSyn",
                "text": "<p>An international security conference explored how artificial intelligence (AI) technologies for drug discovery could be misused for de novo design of biochemical weapons. A thought experiment evolved into a computational proof.</p>"
            },
            "media": {
                "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9544280/",
                "caption": "Dual Use of Artificial Intelligence-powered Drug Discovery",
                "credit": "Fabio Urbina, Filippa Lentzos, Cédric Invernizzi, Sean Ekins"
            }
        },
        {
            "start_date": {
                "year": 2022,
                "month": 3,
                "day": 2
            },
            "text": {
                "headline": "Statement Curriculum Learning",
                "text": "<p>We explore the use of expert iteration in the context of language modeling applied to formal mathematics. We show that at same compute budget, expert iteration, by which we mean proof search interleaved with learning, dramatically outperforms proof search only. We also observe that when applied to a collection of formal statements of sufficiently varied difficulty, expert iteration is capable of finding and solving a curriculum of increasingly difficult problems, without the need for associated ground-truth proofs. Finally, by applying this expert iteration to a manually curated set of problem statements, we achieve state-of-the-art on the miniF2F benchmark, automatically solving multiple challenging problems drawn from high school olympiads.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2202.01344",
                "caption": "Formal Mathematics Statement Curriculum Learning",
                "credit": "Stanislas Polu, Jesse Michael Han, Kunhao Zheng, Mantas Baksys, Igor Babuschkin, Ilya Sutskever "
            }
        },
        {
            "start_date": {
                "year": 2022,
                "month": 3,
                "day": 1
            },
            "text": {
                "headline": "DeepNet",
                "text": "<p>In this paper, we propose a simple yet effective method to stabilize extremely deep Transformers. Specifically, we introduce a new normalization function (DeepNorm) to modify the residual connection in Transformer, accompanying with theoretically derived initialization. In-depth theoretical analysis shows that model updates can be bounded in a stable way. The proposed method combines the best of two worlds, i.e., good performance of Post-LN and stable training of Pre-LN, making DeepNorm a preferred alternative. We successfully scale Transformers up to 1,000 layers (i.e., 2,500 attention and feed-forward network sublayers) without difficulty, which is one order of magnitude deeper than previous deep Transformers. Remarkably, on a multilingual benchmark with 7,482 translation directions, our 200-layer model with 3.2B parameters significantly outperforms the 48-layer state-of-the-art model with 12B parameters by 5 BLEU points, which indicates a promising scaling direction.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2203.00555",
                "caption": "DeepNet: Scaling Transformers to 1,000 Layers",
                "credit": "Hongyu Wang, Shuming Ma, Li Dong, Shaohan Huang, Dongdong Zhang, Furu Wei"
            }
        },
        {
            "start_date": {
                "year": 2022,
                "month": 2,
                "day": 26
            },
            "text": {
                "headline": "PolyCoder",
                "text": "<p>Large language models (LMs) of code have recently shown tremendous promise in completing code and synthesizing code from natural language descriptions. However, the current state-of-the-art code LMs (e.g., Codex (Chen et al., 2021)) are not publicly available, leaving many questions about their model and data design decisions. We aim to fill in some of these blanks through a systematic evaluation of the largest existing models: Codex, GPT-J, GPT-Neo, GPT-NeoX-20B, and CodeParrot, across various programming languages. Although Codex itself is not open-source, we find that existing open-source models do achieve close results in some programming languages, although targeted mainly for natural language modeling. We further identify an important missing piece in the form of a large open-source model trained exclusively on a multi-lingual corpus of code. We release a new model, PolyCoder, with 2.7B parameters based on the GPT-2 architecture, which was trained on 249GB of code across 12 programming languages on a single machine. In the C programming language, PolyCoder outperforms all models including Codex. Our trained models are open-source and publicly available at this https URL, which enables future research and application in this area.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2202.13169",
                "caption": "A Systematic Evaluation of Large Language Models of Code",
                "credit": "Frank F. Xu, Uri Alon, Graham Neubig, Vincent J. Hellendoorn"
            }
        },
        {
            "start_date": {
                "year": 2022,
                "month": 2,
                "day": 17
            },
            "text": {
                "headline": "ST-MoE",
                "text": "<p>Scale has opened new frontiers in natural language processing -- but at a high cost. In response, Mixture-of-Experts (MoE) and Switch Transformers have been proposed as an energy efficient path to even larger and more capable language models. But advancing the state-of-the-art across a broad set of natural language tasks has been hindered by training instabilities and uncertain quality during fine-tuning. Our work focuses on these issues and acts as a design guide. We conclude by scaling a sparse model to 269B parameters, with a computational cost comparable to a 32B dense encoder-decoder Transformer (Stable and Transferable Mixture-of-Experts or ST-MoE-32B). For the first time, a sparse model achieves state-of-the-art performance in transfer learning, across a diverse set of tasks including reasoning (SuperGLUE, ARC Easy, ARC Challenge), summarization (XSum, CNN-DM), closed book question answering (WebQA, Natural Questions), and adversarially constructed tasks (Winogrande, ANLI R3).</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2202.08906v2",
                "caption": "ST-MoE: Designing Stable and Transferable Sparse Expert Models",
                "credit": "Barret Zoph, Irwan Bello, Sameer Kumar, Nan Du, Yanping Huang, Jeff Dean, Noam Shazeer, William Fedus"
            }
        },
        {
            "start_date": {
                "year": 2022,
                "month": 2,
                "day": 15
            },
            "text": {
                "headline": "Midjourney V1",
                "text": "<p></p>"
            },
            "media": {
                "url": "",
                "caption": "",
                "credit": ""
            }
        },
        {
            "start_date": {
                "year": 2022,
                "month": 2,
                "day": 10
            },
            "text": {
                "headline": "ProteinBERT",
                "text": "<p>Self-supervised deep language modeling has shown unprecedented success across natural language tasks, and has recently been repurposed to biological sequences. However, existing models and pretraining methods are designed and optimized for text analysis. We introduce ProteinBERT, a deep language model specifically designed for proteins. Our pretraining scheme combines language modeling with a novel task of Gene Ontology (GO) annotation prediction. We introduce novel architectural elements that make the model highly efficient and flexible to long sequences. The architecture of ProteinBERT consists of both local and global representations, allowing end-to-end processing of these types of inputs and outputs. ProteinBERT obtains near state-of-the-art performance, and sometimes exceeds it, on multiple benchmarks covering diverse protein properties (including protein structure, post-translational modifications and biophysical attributes), despite using a far smaller and faster model than competing deep-learning methods. Overall, ProteinBERT provides an efficient framework for rapidly training protein predictors, even with limited labeled data.</p>"
            },
            "media": {
                "url": "https://academic.oup.com/bioinformatics/article/38/8/2102/6502274",
                "caption": "ProteinBERT: a universal deep-learning model of protein sequence and function",
                "credit": "Nadav Brandes, Dan Ofer, Yam Peleg, Nadav Rappoport, Michal Linial"
            }
        },
        {
            "start_date": {
                "year": 2022,
                "month": 2,
                "day": 10
            },
            "text": {
                "headline": "LaMDA",
                "text": "<p>We present LaMDA: Language Models for Dialog Applications. LaMDA is a family of Transformer-based neural language models specialized for dialog, which have up to 137B parameters and are pre-trained on 1.56T words of public dialog data and web text. While model scaling alone can improve quality, it shows less improvements on safety and factual grounding. We demonstrate that fine-tuning with annotated data and enabling the model to consult external knowledge sources can lead to significant improvements towards the two key challenges of safety and factual grounding. The first challenge, safety, involves ensuring that the model's responses are consistent with a set of human values, such as preventing harmful suggestions and unfair bias. We quantify safety using a metric based on an illustrative set of human values, and we find that filtering candidate responses using a LaMDA classifier fine-tuned with a small amount of crowdworker-annotated data offers a promising approach to improving model safety. The second challenge, factual grounding, involves enabling the model to consult external knowledge sources, such as an information retrieval system, a language translator, and a calculator. We quantify factuality using a groundedness metric, and we find that our approach enables the model to generate responses grounded in known sources, rather than responses that merely sound plausible. Finally, we explore the use of LaMDA in the domains of education and content recommendations, and analyze their helpfulness and role consistency.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2201.08239",
                "caption": "LaMDA: Language Models for Dialog Applications",
                "credit": "Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, YaGuang Li, Hongrae Lee, Huaixiu Steven Zheng, Amin Ghafouri, Marcelo Menegali, Yanping Huang, Maxim Krikun, Dmitry Lepikhin, James Qin, Dehao Chen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts, Maarten Bosma, Yanqi Zhou, Chung-Ching Chang, Igor Krivokon, Will Rusch, Marc Pickett, Kathleen Meier-Hellstern, Meredith Ringel Morris, Tulsee Doshi, Renelito Delos Santos, Toju Duke, Johnny Soraker, Ben Zevenbergen, Vinodkumar Prabhakaran, Mark Diaz, Ben Hutchinson, Kristen Olson, Alejandra Molina, Erin Hoffman-John, Josh Lee, Lora Aroyo, Ravi Rajakumar, Alena Butryna, Matthew Lamm, Viktoriya Kuzmina, Joe Fenton, Aaron Cohen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera-Arcas, Claire Cui, Marian Croak, Ed Chi, Quoc Le"
            }
        },
        {
            "start_date": {
                "year": 2022,
                "month": 2,
                "day": 9
            },
            "text": {
                "headline": "GPT-NeoX-20B",
                "text": "<p>We introduce GPT-NeoX-20B, a 20 billion parameter autoregressive language model trained on the Pile, whose weights will be made freely and openly available to the public through a permissive license. It is, to the best of our knowledge, the largest dense autoregressive model that has publicly available weights at the time of submission. In this work, we describe \\model{}'s architecture and training and evaluate its performance on a range of language-understanding, mathematics, and knowledge-based tasks. We find that GPT-NeoX-20B is a particularly powerful few-shot reasoner and gains far more in performance when evaluated five-shot than similarly sized GPT-3 and FairSeq models. We open-source the training and evaluation code, as well as the model weights, at this https URL.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2204.06745",
                "caption": "GPT-NeoX-20B: An Open-Source Autoregressive Language Model",
                "credit": "Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, USVSN Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, Samuel Weinbach"
            }
        },
        {
            "start_date": {
                "year": 2022,
                "month": 2,
                "day": 7
            },
            "text": {
                "headline": "RETRO-7B",
                "text": "<p>We enhance auto-regressive language models by conditioning on document chunks retrieved from a large corpus, based on local similarity with preceding tokens. With a 2 trillion token database, our Retrieval-Enhanced Transformer (RETRO) obtains comparable performance to GPT-3 and Jurassic-1 on the Pile, despite using 25× fewer parameters. After fine-tuning, RETRO performance translates to downstream knowledge-intensive tasks such as question answering. RETRO combines a frozen Bert retriever, a differentiable encoder and a chunked cross-attention mechanism to predict tokens based on an order of magnitude more data than what is typically consumed during training. We typically train RETRO from scratch, yet can also rapidly RETROfit pre-trained transformers with retrieval and still achieve good performance. Our work opens up new avenues for improving language models through explicit memory at unprecedented scale.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2112.04426",
                "caption": "Improving language models by retrieving from trillions of tokens",
                "credit": "Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George van den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego de Las Casas, Aurelia Guy, Jacob Menick, Roman Ring, Tom Hennigan, Saffron Huang, Loren Maggiore, Chris Jones, Albin Cassirer, Andy Brock, Michela Paganini, Geoffrey Irving, Oriol Vinyals, Simon Osindero,Karen Simonyan, Jack W. Rae‡, Erich Elsen‡ and Laurent Sifre"
            }
        },
        {
            "start_date": {
                "year": 2022,
                "month": 2,
                "day": 2
            },
            "text": {
                "headline": "AlphaCode",
                "text": "<p>Programming is a powerful and ubiquitous problem-solving tool. Developing systems that can assist programmers or even generate programs independently could make programming more productive and\naccessible, yet so far incorporating innovations in AI has proven challenging. Recent large-scale language models have demonstrated an impressive ability to generate code, and are now able to complete\nsimple programming tasks. However, these models still perform poorly when evaluated on more complex, unseen problems that require problem-solving skills beyond simply translating instructions into\ncode. For example, competitive programming problems which require an understanding of algorithms\nand complex natural language remain extremely challenging. To address this gap, we introduce AlphaCode, a system for code generation that can create novel solutions to these problems that require deeper\nreasoning. In simulated evaluations on recent programming competitions on the Codeforces platform,\nAlphaCode achieved on average a ranking of top 54.3% in competitions with more than 5,000 participants. We found that three key components were critical to achieve good and reliable performance:\n(1) an extensive and clean competitive programming dataset for training and evaluation, (2) large and\nefficient-to-sample transformer-based architectures, and (3) large-scale model sampling to explore the\nsearch space, followed by filtering based on program behavior to a small set of submissions.\n</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2203.07814",
                "caption": "Competition-Level Code Generation with AlphaCode",
                "credit": "Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien de Masson d'Autume, Igor Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, James Molloy, Daniel J. Mankowitz, Esme Sutherland Robson, Pushmeet Kohli, Nando de Freitas, Koray Kavukcuoglu, Oriol Vinyals"
            }
        },
        {
            "start_date": {
                "year": 2022,
                "month": 1,
                "day": 27
            },
            "text": {
                "headline": "InstructGPT 175B",
                "text": "<p>Making language models bigger does not inherently make them better at following a user’s intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback (RLHF). We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.</p>"
            },
            "media": {
                "url": "https://arxiv.org/pdf/2203.02155",
                "caption": "Training language models to follow instructions with human feedback",
                "credit": "Long Ouyang, Pamela Mishkin, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright,John Schulman Amanda Askell, Fraser Kelton Peter Welinder, Luke Miller Maddie Simens Paul Christiano,Ryan Lowe,Chong Zhang Jacob Hilton, Sandhini Agarwal Katarina Slama Alex Ray, Jan Leike"
            }
        },
        {
            "start_date": {
                "year": 2022,
                "month": 1,
                "day": 27
            },
            "text": {
                "headline": "InstructGPT 6B",
                "text": "<p>Making language models bigger does not inherently make them better at following a user’s intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback (RLHF). We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.</p>"
            },
            "media": {
                "url": "https://arxiv.org/pdf/2203.02155",
                "caption": "Training language models to follow instructions with human feedback",
                "credit": "Long Ouyang, Pamela Mishkin, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright,John Schulman Amanda Askell, Fraser Kelton Peter Welinder, Luke Miller Maddie Simens Paul Christiano,Ryan Lowe,Chong Zhang Jacob Hilton, Sandhini Agarwal Katarina Slama Alex Ray, Jan Leike"
            }
        },
        {
            "start_date": {
                "year": 2022,
                "month": 1,
                "day": 27
            },
            "text": {
                "headline": "InstructGPT 1.3B",
                "text": "<p>Making language models bigger does not inherently make them better at following a user’s intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback (RLHF). We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.</p>"
            },
            "media": {
                "url": "https://arxiv.org/pdf/2203.02155",
                "caption": "Training language models to follow instructions with human feedback",
                "credit": "Long Ouyang, Pamela Mishkin, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright,John Schulman Amanda Askell, Fraser Kelton Peter Welinder, Luke Miller Maddie Simens Paul Christiano,Ryan Lowe,Chong Zhang Jacob Hilton, Sandhini Agarwal Katarina Slama Alex Ray, Jan Leike"
            }
        },
        {
            "start_date": {
                "year": 2022,
                "month": 1,
                "day": 23
            },
            "text": {
                "headline": "OntoProtein",
                "text": "<p>Self-supervised protein language models have proved their effectiveness in learn- ing the proteins representations. With the increasing computational power, cur- rent protein language models pre-trained with millions of diverse sequences can advance the parameter scale from million-level to billion-level and achieve re- markable improvement. However, those prevailing approaches rarely consider incorporating knowledge graphs (KGs), which can provide rich structured knowl- edge facts for better protein representations. We argue that informative biology knowledge in KGs can enhance protein representation with external knowledge. In this work, we propose OntoProtein, the first general framework that makes use of structure in GO (Gene Ontology) into protein pre-training models. We construct a novel large-scale knowledge graph that consists of GO and its related proteins, and gene annotation texts or protein sequences describe all nodes in the graph. We propose novel contrastive learning with knowledge-aware negative sampling to jointly optimize the knowledge graph and protein embedding during pre-training. Experimental results show that OntoProtein can surpass state-of-the-art methods with pre-trained protein language models in TAPE benchmark and yield better performance compared with baselines in protein-protein interaction and protein function prediction1.</p>"
            },
            "media": {
                "url": "https://openreview.net/pdf?id=yfe1VMYAXa4",
                "caption": "ONTOPROTEIN: PROTEIN PRETRAINING WITH GENE ONTOLOGY EMBEDDING",
                "credit": "Ningyu Zhang, Zhen Bi, Xiaozhuan Liang, Siyuan Cheng, Shumin Deng, Qiang Zhang, Jiazhang Lian, Huajun Chen, Haosen Hong"
            }
        },
        {
            "start_date": {
                "year": 2022,
                "month": 1,
                "day": 22
            },
            "text": {
                "headline": "AbLang (heavy sequences)",
                "text": "<p>Motivation\nGeneral protein language models have been shown to summarize the semantics of protein sequences into representations that are useful for state-of-the-art predictive methods. However, for antibody specific problems, such as restoring residues lost due to sequencing errors, a model trained solely on antibodies may be more powerful. Antibodies are one of the few protein types where the volume of sequence data needed for such language models is available, e.g. in the Observed Antibody Space (OAS) database.\n\nResults\nHere, we introduce AbLang, a language model trained on the antibody sequences in the OAS database. We demonstrate the power of AbLang by using it to restore missing residues in antibody sequence data, a key issue with B-cell receptor repertoire sequencing, e.g. over 40% of OAS sequences are missing the first 15 amino acids. AbLang restores the missing residues of antibody sequences better than using IMGT germlines or the general protein language model ESM-1b. Further, AbLang does not require knowledge of the germline of the antibody and is seven times faster than ESM-1b.</p>"
            },
            "media": {
                "url": "https://academic.oup.com/bioinformaticsadvances/article/2/1/vbac046/6609807",
                "caption": "AbLang: an antibody language model for completing antibody sequences",
                "credit": "Tobias H Olsen, Iain H Moal, Charlotte M Deane"
            }
        },
        {
            "start_date": {
                "year": 2022,
                "month": 1,
                "day": 20
            },
            "text": {
                "headline": "data2vec (vision)",
                "text": "<p>While the general idea of self-supervised learning is identical across modalities, the actual algorithms and objectives differ widely because they were developed with a single modality in mind. To get us closer to general self-supervised learning, we present data2vec, a framework that uses the same learning method for either speech,NLP or computer vision. The core idea is to predict latent representations of the full input data based on a masked view of the input in a selfdistillation setup using a standard Transformer architecture. Instead of predicting modality-specific targets such as words, visual tokens or units of human speech which are local in nature, data2vec predicts contextualized latent representations that contain information from the entire input. Experiments on the major benchmarks of speech recognition, image classification, and natural language understanding demonstrate a new state of the art or competitive performance to predominant approaches.</p>"
            },
            "media": {
                "url": "https://ai.facebook.com/research/data2vec-a-general-framework-for-self-supervised-learning-in-speech-vision-and-language/",
                "caption": "Data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language",
                "credit": "Alexei Baevski,  Wei-Ning Hsu,  Qiantong Xu , Arun Babu,  Jiatao Gu,  Michael Auli"
            }
        },
        {
            "start_date": {
                "year": 2022,
                "month": 1,
                "day": 20
            },
            "text": {
                "headline": "data2vec (speech)",
                "text": "<p>While the general idea of self-supervised learning is identical across modalities, the actual algorithms and objectives differ widely because they were developed with a single modality in mind. To get us closer to general self-supervised learning, we present data2vec, a framework that uses the same learning method for either speech,NLP or computer vision. The core idea is to predict latent representations of the full input data based on a masked view of the input in a selfdistillation setup using a standard Transformer architecture. Instead of predicting modality-specific targets such as words, visual tokens or units of human speech which are local in nature, data2vec predicts contextualized latent representations that contain information from the entire input. Experiments on the major benchmarks of speech recognition, image classification, and natural language understanding demonstrate a new state of the art or competitive performance to predominant approaches.</p>"
            },
            "media": {
                "url": "https://ai.facebook.com/research/data2vec-a-general-framework-for-self-supervised-learning-in-speech-vision-and-language/",
                "caption": "Data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language",
                "credit": "Alexei Baevski,  Wei-Ning Hsu,  Qiantong Xu , Arun Babu,  Jiatao Gu,  Michael Auli"
            }
        },
        {
            "start_date": {
                "year": 2022,
                "month": 1,
                "day": 20
            },
            "text": {
                "headline": "data2vec (language)",
                "text": "<p>While the general idea of self-supervised learning is identical across modalities, the actual algorithms and objectives differ widely because\nthey were developed with a single modality in mind. To get us closer to general self-supervised learning, we present data2vec, a framework that\nuses the same learning method for either speech, NLP or computer vision. The core idea is to predict latent representations of the full input data based on a masked view of the input in a selfdistillation setup using a standard transformer architecture. Instead of predicting modality-specific\ntargets such as words, visual tokens or units of human speech which are local in nature, data2vecpredicts contextualized latent representations that\ncontain information from the entire input. Experiments on the major benchmarks of speech recognition, image classification, and natural language understanding demonstrate a new state of the art or competitive performance to predominant approaches. Models and code are available at www.github.com/pytorch/fairseq/tree/master/examples/data2vec.\n</p>"
            },
            "media": {
                "url": "https://ai.facebook.com/research/data2vec-a-general-framework-for-self-supervised-learning-in-speech-vision-and-language/",
                "caption": "Data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language",
                "credit": "Alexei Baevski,  Wei-Ning Hsu,  Qiantong Xu , Arun Babu,  Jiatao Gu,  Michael Auli"
            }
        },
        {
            "start_date": {
                "year": 2022,
                "month": 1,
                "day": 7
            },
            "text": {
                "headline": "Detic",
                "text": "<p> Current object detectors are limited in vocabulary size due to the small scale of detection datasets. Image classifiers, on the other hand, reason about much larger vocabularies, as their datasets are larger and easier to collect. We propose Detic, which simply trains the classifiers of a detector on image classification data and thus expands the vocabulary of detectors to tens of thousands of concepts. Unlike prior work, Detic does not need complex assignment schemes to assign image labels to boxes based on model predictions, making it much easier to implement and compatible with a range of detection architectures and backbones. Our results show that Detic yields excellent detectors even for classes without box annotations. It outperforms prior work on both open-vocabulary and long-tail detection benchmarks. Detic provides a gain of 2.4 mAP for all classes and 8.3 mAP for novel classes on the open-vocabulary LVIS benchmark. On the standard LVIS benchmark, Detic obtains 41.7 mAP when evaluated on all classes, or only rare classes, hence closing the gap in performance for object categories with few samples. For the first time, we train a detector with all the twenty-one-thousand classes of the ImageNet dataset and show that it generalizes to new datasets without finetuning. Code is available at \\url{this https URL}. </p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2201.02605",
                "caption": "Detecting Twenty-thousand Classes using Image-level Supervision",
                "credit": "Xingyi Zhou, Rohit Girdhar, Armand Joulin, Philipp Krähenbühl, Ishan Misra"
            }
        },
        {
            "start_date": {
                "year": 2021,
                "month": 12,
                "day": 31
            },
            "text": {
                "headline": "ERNIE-ViLG",
                "text": "<p>Conventional methods for the image-text generation tasks mainly tackle the naturally bidirectional generation tasks separately, focusing on designing task-specific frameworks to improve the quality and fidelity of the generated samples. Recently, Vision-Language Pre-training models have greatly improved the performance of the image-to-text generation tasks, but large-scale pre-training models for text-to-image synthesis task are still under-developed. In this paper, we propose ERNIE-ViLG, a unified generative pre-training framework for bidirectional image-text generation with transformer model. Based on the image quantization models, we formulate both image generation and text generation as autoregressive generative tasks conditioned on the text/image input. The bidirectional image-text generative modeling eases the semantic alignments across vision and language. For the text-to-image generation process, we further propose an end-to-end training method to jointly learn the visual sequence generator and the image reconstructor. To explore the landscape of large-scale pre-training for bidirectional text-image generation, we train a 10-billion parameter ERNIE-ViLG model on a large-scale dataset of 145 million (Chinese) image-text pairs which achieves state-of-the-art performance for both text-to-image and image-to-text tasks, obtaining an FID of 7.9 on MS-COCO for text-to-image synthesis and best results on COCO-CN and AIC-ICC for image captioning.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2112.15283",
                "caption": "ERNIE-ViLG: Unified Generative Pre-training for Bidirectional Vision-Language Generation",
                "credit": "Han Zhang, Weichong Yin, Yewei Fang, Lanxin Li, Boqiang Duan, Zhihua Wu, Yu Sun, Hao Tian, Hua Wu, Haifeng Wang"
            }
        },
        {
            "start_date": {
                "year": 2021,
                "month": 12,
                "day": 23
            },
            "text": {
                "headline": "ERNIE 3.0 Titan",
                "text": "<p>Pre-trained language models have achieved state-of-the-art results in various Natural Language Processing (NLP) tasks. GPT-3 has shown that scaling up pre-trained language models can further exploit their enormous potential. A unified framework named ERNIE 3.0 was recently proposed for pre-training large-scale knowledge enhanced models and trained a model with 10 billion parameters. ERNIE 3.0 outperformed the state-of-the-art models on various NLP tasks. In order to explore the performance of scaling up ERNIE 3.0, we train a hundred-billion-parameter model called ERNIE 3.0 Titan with up to 260 billion parameters on the PaddlePaddle platform. Furthermore, we design a self-supervised adversarial loss and a controllable language modeling loss to make ERNIE 3.0 Titan generate credible and controllable texts. To reduce the computation overhead and carbon emission, we propose an online distillation framework for ERNIE 3.0 Titan, where the teacher model will teach students and train itself simultaneously. ERNIE 3.0 Titan is the largest Chinese dense pre-trained model so far. Empirical results show that the ERNIE 3.0 Titan outperforms the state-of-the-art models on 68 NLP datasets.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2112.12731",
                "caption": "ERNIE 3.0 Titan: Exploring Larger-scale Knowledge Enhanced Pre-training for Language Understanding and Generation",
                "credit": "Shuohuan Wang, Yu Sun, Yang Xiang, Zhihua Wu, Siyu Ding, Weibao Gong, Shikun Feng"
            }
        },
        {
            "start_date": {
                "year": 2021,
                "month": 12,
                "day": 20
            },
            "text": {
                "headline": "XGLM-7.5B",
                "text": "<p>Large-scale generative language models such as GPT-3 are competitive few-shot learners. While these models are known to be able to jointly represent many different languages, their training data is dominated by English, potentially limiting their cross-lingual generalization. In this work, we train multilingual generative language models on a corpus covering a diverse set of languages, and study their few- and zero-shot learning capabilities in a wide range of tasks. Our largest model with 7.5 billion parameters sets new state of the art in few-shot learning in more than 20 representative languages, outperforming GPT-3 of comparable size in multilingual commonsense reasoning (with +7.4% absolute accuracy improvement in 0-shot settings and +9.4% in 4-shot settings) and natural language inference (+5.4% in each of 0-shot and 4-shot settings). On the FLORES-101 machine translation benchmark, our model outperforms GPT-3 on 171 out of 182 directions with 32 training examples, while surpassing the official supervised baseline in 45 directions. We conduct an in-depth analysis of different multilingual prompting approaches, showing in particular that strong few-shot learning performance across languages can be achieved via cross-lingual transfer through both templates and demonstration examples. Finally, we evaluate our models in social value tasks such as hate speech detection in five languages and find it has limitations similar to comparable sized GPT-3 models. </p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2112.10668",
                "caption": "Few-shot Learning with Multilingual Language Models",
                "credit": "Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth Pasunuru, Sam Shleifer, Punit Singh Koura, Vishrav Chaudhary, Brian O'Horo, Jeff Wang, Luke Zettlemoyer, Zornitsa Kozareva, Mona Diab, Veselin Stoyanov, Xian Li"
            }
        },
        {
            "start_date": {
                "year": 2021,
                "month": 12,
                "day": 20
            },
            "text": {
                "headline": "LDM-1.45B",
                "text": "<p>By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve a new state of the art for image inpainting and highly competitive performance on various tasks, including unconditional image generation, semantic scene synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs. Code is available at this https URL.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2112.10752",
                "caption": "High-Resolution Image Synthesis with Latent Diffusion Models",
                "credit": "Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Björn Ommer"
            }
        },
        {
            "start_date": {
                "year": 2021,
                "month": 12,
                "day": 20
            },
            "text": {
                "headline": "GLIDE",
                "text": "<p>Diffusion models have recently been shown to generate high-quality synthetic images, especially when paired with a guidance technique to trade off diversity for fidelity. We explore diffusion models for the problem of text-conditional image synthesis and compare two different guidance strategies: CLIP guidance and classifier-free guidance. We find that the latter is preferred by human evaluators for both photorealism and caption similarity, and often produces photorealistic samples. Samples from a 3.5 billion parameter text-conditional diffusion model using classifier-free guidance are favored by human evaluators to those from DALL-E, even when the latter uses expensive CLIP reranking. Additionally, we find that our models can be fine-tuned to perform image inpainting, enabling powerful text-driven image editing. We train a smaller model on a filtered dataset and release the code and weights at this https URL.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2112.10741",
                "caption": "GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models",
                "credit": "Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, Mark Chen"
            }
        },
        {
            "start_date": {
                "year": 2021,
                "month": 12,
                "day": 16
            },
            "text": {
                "headline": "Contriever",
                "text": "<p>Recently, information retrieval has seen the emergence of dense retrievers, using neural networks, as an alternative to classical sparse methods based on term-frequency. These models have obtained state-of-the-art results on datasets and tasks where large training sets are available. However, they do not transfer well to new applications with no training data, and are outperformed by unsupervised term-frequency methods such as BM25. In this work, we explore the limits of contrastive learning as a way to train unsupervised dense retrievers and show that it leads to strong performance in various retrieval settings. On the BEIR benchmark our unsupervised model outperforms BM25 on 11 out of 15 datasets for the Recall@100. When used as pre-training before fine-tuning, either on a few thousands in-domain examples or on the large MS~MARCO dataset, our contrastive model leads to improvements on the BEIR benchmark. Finally, we evaluate our approach for multi-lingual retrieval, where training data is even scarcer than for English, and show that our approach leads to strong unsupervised performance. Our model also exhibits strong cross-lingual transfer when fine-tuned on supervised English data only and evaluated on low resources language such as Swahili. We show that our unsupervised models can perform cross-lingual retrieval between different scripts, such as retrieving English documents from Arabic queries, which would not be possible with term matching methods.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2112.09118",
                "caption": "Unsupervised Dense Information Retrieval with Contrastive Learning",
                "credit": "Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, Edouard Grave"
            }
        },
        {
            "start_date": {
                "year": 2021,
                "month": 12,
                "day": 15
            },
            "text": {
                "headline": "LongT5",
                "text": "<p>Recent work has shown that either (1) increasing the input length or (2) increasing model size can improve the performance of Transformer-based neural models. In this paper, we present a new model, called LongT5, with which we explore the effects of scaling both the input length and model size at the same time. Specifically, we integrated attention ideas from long-input transformers (ETC), and adopted pre-training strategies from summarization pre-training (PEGASUS) into the scalable T5 architecture. The result is a new attention mechanism we call {\\em Transient Global} (TGlobal), which mimics ETC's local/global attention mechanism, but without requiring additional side-inputs. We are able to achieve state-of-the-art results on several summarization tasks and outperform the original T5 models on question answering tasks.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2112.07916",
                "caption": "LongT5: Efficient Text-To-Text Transformer for Long Sequences",
                "credit": "Mandy Guo, Joshua Ainslie, David Uthus, Santiago Ontanon, Jianmo Ni, Yun-Hsuan Sung, Yinfei Yang"
            }
        },
        {
            "start_date": {
                "year": 2021,
                "month": 12,
                "day": 13
            },
            "text": {
                "headline": "GLaM",
                "text": "<p>Scaling language models with more data, compute and parameters has driven significant progress in natural language processing. For example, thanks to scaling, GPT-3 was able to achieve strong results on in-context learning tasks. However, training these large dense models requires significant amounts of computing resources. In this paper, we propose and develop a family of language models named GLaM (Generalist Language Model), which uses a sparsely activated mixture-of-experts architecture to scale the model capacity while also incurring substantially less training cost compared to dense variants. The largest GLaM has 1.2 trillion parameters, which is approximately 7x larger than GPT-3. It consumes only 1/3 of the energy used to train GPT-3 and requires half of the computation flops for inference, while still achieving better overall zero-shot and one-shot performance across 29 NLP tasks.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2112.06905",
                "caption": "GLaM: Efficient Scaling of Language Models with Mixture-of-Experts",
                "credit": "Nan Du, Yanping Huang, Andrew M. Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, Barret Zoph, Liam Fedus, Maarten Bosma, Zongwei Zhou, Tao Wang, Yu Emma Wang, Kellie Webster, Marie Pellat, Kevin Robinson, Kathleen Meier-Hellstern, Toju Duke, Lucas Dixon, Kun Zhang, Quoc V Le, Yonghui Wu, Zhifeng Chen, Claire Cui"
            }
        },
        {
            "start_date": {
                "year": 2021,
                "month": 12,
                "day": 8
            },
            "text": {
                "headline": "Gopher (280B)",
                "text": "<p>We enhance auto-regressive language models by conditioning on document chunks retrieved from a large corpus, based on local similarity with preceding tokens. With a 2 trillion token database, our Retrieval-Enhanced Transformer (RETRO) obtains comparable performance to GPT-3 and Jurassic-1 on the Pile, despite using 25× fewer parameters. After fine-tuning, RETRO performance translates to downstream knowledge-intensive tasks such as question answering. RETRO combines a frozen Bert retriever, a differentiable encoder and a chunked cross-attention mechanism to predict tokens based on an order of magnitude more data than what is typically consumed during training. We typically train RETRO from scratch, yet can also rapidly RETROfit pre-trained transformers with retrieval and still achieve good performance. Our work opens up new avenues for improving language models through explicit memory at unprecedented scale.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2112.11446",
                "caption": "\"Scaling Language Models: Methods, Analysis & Insights from Training Gopher\"",
                "credit": "Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell, George van den Driessche, Lisa Anne Hendricks, Maribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan Uesato, John Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu, Erich Elsen, Siddhant Jayakumar, Elena Buchatskaya, David Budden, Esme Sutherland, Karen Simonyan, Michela Paganini, Laurent Sifre, Lena Martens, Xiang Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau, Maria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sottiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de Masson d'Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de Las Casas, Aurelia Guy, Chris Jones, James Bradbury, Matthew Johnson, Blake Hechtman, Laura Weidinger, Iason Gabriel, William Isaac, Ed Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub, Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray Kavukcuoglu, Geoffrey Irving"
            }
        },
        {
            "start_date": {
                "year": 2021,
                "month": 12,
                "day": 6
            },
            "text": {
                "headline": "Student of Games",
                "text": "<p>Games have a long history as benchmarks for progress in artificial intelligence. Approaches using search and learning produced strong performance across many perfect information games, and approaches using game-theoretic reasoning and learning demonstrated strong performance for specific imperfect information poker variants. We introduce Student of Games, a general-purpose algorithm that unifies previous approaches, combining guided search, self-play learning, and game-theoretic reasoning. Student of Games achieves strong empirical performance in large perfect and imperfect information games -- an important step towards truly general algorithms for arbitrary environments. We prove that Student of Games is sound, converging to perfect play as available computation and approximation capacity increases. Student of Games reaches strong performance in chess and Go, beats the strongest openly available agent in heads-up no-limit Texas hold'em poker, and defeats the state-of-the-art agent in Scotland Yard, an imperfect information game that illustrates the value of guided search, learning, and game-theoretic reasoning.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2112.03178",
                "caption": "Player of Games",
                "credit": "Martin Schmid, Matej Moravcik, Neil Burch, Rudolf Kadlec, Josh Davidson, Kevin Waugh, Nolan Bard, Finbarr Timbers, Marc Lanctot, Zach Holland, Elnaz Davoodi, Alden Christianson, Michael Bowling"
            }
        },
        {
            "start_date": {
                "year": 2021,
                "month": 12,
                "day": 3
            },
            "text": {
                "headline": "T-NLRv5 XXL",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://www.marketscreener.com/quote/stock/MICROSOFT-CORPORATION-4835/news/Microsoft-Turing-NLRv5-achieves-new-performance-milestones-37207301/\nhttps://www.microsoft.com/en-us/research/blog/efficiently-and-effectively-scaling-up-language-model-pretraining-for-best-language-representation-model-on-glue-and-superglue/",
                "caption": "Microsoft : Turing-NLRv5 achieves new performance milestones",
                "credit": ""
            }
        },
        {
            "start_date": {
                "year": 2021,
                "month": 11,
                "day": 24
            },
            "text": {
                "headline": "NÜWA",
                "text": "<p>This paper presents a unified multimodal pre-trained model called NÜWA that can generate new or manipulate existing visual data (i.e., images and videos) for various visual synthesis tasks. To cover language, image, and video at the same time for different scenarios, a 3D transformer encoder-decoder framework is designed, which can not only deal with videos as 3D data but also adapt to texts and images as 1D and 2D data, respectively. A 3D Nearby Attention (3DNA) mechanism is also proposed to consider the nature of the visual data and reduce the computational complexity. We evaluate NÜWA on 8 downstream tasks. Compared to several strong baselines, NÜWA achieves state-of-the-art results on text-to-image generation, text-to-video generation, video prediction, etc. Furthermore, it also shows surprisingly good zero-shot capabilities on text-guided image and video manipulation tasks. Project repo is this https URL.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2111.12417",
                "caption": "NÜWA: Visual Synthesis Pre-training for Neural visUal World creAtion",
                "credit": "Chenfei Wu, Jian Liang, Lei Ji, Fan Yang, Yuejian Fang, Daxin Jiang, Nan Duan"
            }
        },
        {
            "start_date": {
                "year": 2021,
                "month": 11,
                "day": 22
            },
            "text": {
                "headline": "Florence",
                "text": "<p>Automated visual understanding of our diverse and open world demands computer vision models to generalize well with minimal customization for specific tasks, similar to human vision. Computer vision foundation models, which are trained on diverse, large-scale dataset and can be adapted to a wide range of downstream tasks, are critical for this mission to solve real-world computer vision applications. While existing vision foundation models such as CLIP, ALIGN, and Wu Dao 2.0 focus mainly on mapping images and textual representations to a cross-modal shared representation, we introduce a new computer vision foundation model, Florence, to expand the representations from coarse (scene) to fine (object), from static (images) to dynamic (videos), and from RGB to multiple modalities (caption, depth). By incorporating universal visual-language representations from Web-scale image-text data, our Florence model can be easily adapted for various computer vision tasks, such as classification, retrieval, object detection, VQA, image caption, video retrieval and action recognition. Moreover, Florence demonstrates outstanding performance in many types of transfer learning: fully sampled fine-tuning, linear probing, few-shot transfer and zero-shot transfer for novel images and objects. All of these properties are critical for our vision foundation model to serve general purpose vision tasks. Florence achieves new state-of-the-art results in majority of 44 representative benchmarks, e.g., ImageNet-1K zero-shot classification with top-1 accuracy of 83.74 and the top-5 accuracy of 97.18, 62.4 mAP on COCO fine tuning, 80.36 on VQA, and 87.8 on Kinetics-600.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2111.11432v1",
                "caption": "Florence: A New Foundation Model for Computer Vision",
                "credit": "Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella, Xiyang Dai, Jianfeng Gao, Houdong Hu, Xuedong Huang, Boxin Li, Chunyuan Li, Ce Liu, Mengchen Liu, Zicheng Liu, Yumao Lu, Yu Shi, Lijuan Wang, JianFeng Wang, Bin Xiao, Zhen Xiao, Jianwei Yang, Michael Zeng, Luowei Zhou, Pengchuan Zhang"
            }
        },
        {
            "start_date": {
                "year": 2021,
                "month": 11,
                "day": 19
            },
            "text": {
                "headline": "BASIC-L",
                "text": "<p>We present a combined scaling method - named BASIC - that achieves 85.7% top-1 accuracy on the ImageNet ILSVRC-2012 validation set without learning from any labeled ImageNet example. This accuracy surpasses best published similar models - CLIP and ALIGN - by 9.3%. Our BASIC model also shows significant improvements in robustness benchmarks. For instance, on 5 test sets with natural distribution shifts such as ImageNet-{A,R,V2,Sketch} and ObjectNet, our model achieves 84.3% top-1 average accuracy, only a small drop from its original ImageNet accuracy. To achieve these results, we scale up the contrastive learning framework of CLIP and ALIGN in three dimensions: data size, model size, and batch size. Our dataset has 6.6B noisy image-text pairs, which is 4x larger than ALIGN, and 16x larger than CLIP. Our largest model has 3B weights, which is 3.75x larger in parameters and 8x larger in FLOPs than ALIGN and CLIP. Finally, our batch size is 65536 which is 2x more than CLIP and 4x more than ALIGN. We encountered two main challenges with the scaling rules of BASIC. First, the main challenge with implementing the combined scaling rules of BASIC is the limited memory of accelerators, such as GPUs and TPUs. To overcome the memory limit, we propose two simple methods which make use of gradient checkpointing and model parallelism. Second, while increasing the dataset size and the model size has been the defacto method to improve the performance of deep learning models like BASIC, the effect of a large contrastive batch size on such contrastive-trained image-text models is not well-understood. To shed light on the benefits of large contrastive batch sizes, we develop a theoretical framework which shows that larger contrastive batch sizes lead to smaller generalization gaps for image-text models such as BASIC.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2111.10050",
                "caption": "Combined Scaling for Zero-shot Transfer Learning",
                "credit": "Hieu Pham, Zihang Dai, Golnaz Ghiasi, Kenji Kawaguchi, Hanxiao Liu, Adams Wei Yu, Jiahui Yu, Yi-Ting Chen, Minh-Thang Luong, Yonghui Wu, Mingxing Tan, Quoc V. Le"
            }
        },
        {
            "start_date": {
                "year": 2021,
                "month": 11,
                "day": 18
            },
            "text": {
                "headline": "Swin Transformer V2 (SwinV2-G)",
                "text": "<p>Large-scale NLP models have been shown to significantly improve the performance on language tasks with no signs of saturation. They also demonstrate amazing few-shot capabilities like that of human beings. This paper aims to explore large-scale models in computer vision. We tackle three major issues in training and application of large vision models, including training instability, resolution gaps between pre-training and fine-tuning, and hunger on labelled data. Three main techniques are proposed: 1) a residual-post-norm method combined with cosine attention to improve training stability; 2) A log-spaced continuous position bias method to effectively transfer models pre-trained using low-resolution images to downstream tasks with high-resolution inputs; 3) A self-supervised pre-training method, SimMIM, to reduce the needs of vast labeled images. Through these techniques, this paper successfully trained a 3 billion-parameter Swin Transformer V2 model, which is the largest dense vision model to date, and makes it capable of training with images of up to 1,536×1,536 resolution. It set new performance records on 4 representative vision tasks, including ImageNet-V2 image classification, COCO object detection, ADE20K semantic segmentation, and Kinetics-400 video action classification. Also note our training is much more efficient than that in Google's billion-level visual models, which consumes 40 times less labelled data and 40 times less training time. Code is available at \\url{this https URL}.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2111.09883v2",
                "caption": "Swin Transformer V2: Scaling Up Capacity and Resolution",
                "credit": "Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, Furu Wei, Baining Guo"
            }
        },
        {
            "start_date": {
                "year": 2021,
                "month": 11,
                "day": 15
            },
            "text": {
                "headline": "ViT-G/14 (LiT)",
                "text": "<p>This paper presents contrastive-tuning, a simple method employing contrastive training to align image and text models while still taking advantage of their pre-training. In our empirical study we find that locked pre-trained image models with unlocked text models work best. We call this instance of contrastive-tuning \"Locked-image Tuning\" (LiT), which just teaches a text model to read out good representations from a pre-trained image model for new tasks. A LiT model gains the capability of zero-shot transfer to new vision tasks, such as image classification or retrieval. The proposed LiT is widely applicable; it works reliably with multiple pre-training methods (supervised and unsupervised) and across diverse architectures (ResNet, Vision Transformers and MLP-Mixer) using three different image-text datasets. With the transformer-based pre-trained ViT-g/14 model, the LiT model achieves 85.2% zero-shot transfer accuracy on the ImageNet test set, and 82.5% on the challenging out-of-distribution ObjectNet test set.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2111.07991v3",
                "caption": "Zero-Shot Transfer with Locked-image Text Tuning",
                "credit": "Xiaohua Zhai, Xiao Wang, Basil Mustafa, Andreas Steiner, Daniel Keysers, Alexander Kolesnikov, Lucas Beyer"
            }
        },
        {
            "start_date": {
                "year": 2021,
                "month": 11,
                "day": 11
            },
            "text": {
                "headline": "Masked Autoencoders ViT-H",
                "text": "<p>This paper shows that masked autoencoders (MAE) are scalable self-supervised learners for computer vision. Our MAE approach is simple: we mask random patches of the input image and reconstruct the missing pixels. It is based on two core designs. First, we develop an asymmetric encoder-decoder architecture, with an encoder that operates only on the visible subset of patches (without mask tokens), along with a lightweight decoder that reconstructs the original image from the latent representation and mask tokens. Second, we find that masking a high proportion of the input image, e.g., 75%, yields a nontrivial and meaningful self-supervisory task. Coupling these two designs enables us to train large models efficiently and effectively: we accelerate training (by 3x or more) and improve accuracy. Our scalable approach allows for learning high-capacity models that generalize well: e.g., a vanilla ViT-Huge model achieves the best accuracy (87.8%) among methods that use only ImageNet-1K data. Transfer performance in downstream tasks outperforms supervised pre-training and shows promising scaling behavior.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2111.06377",
                "caption": "Masked Autoencoders Are Scalable Vision Learners",
                "credit": "Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, Ross Girshick"
            }
        },
        {
            "start_date": {
                "year": 2021,
                "month": 11,
                "day": 1
            },
            "text": {
                "headline": "Projected GAN",
                "text": "<p>Generative Adversarial Networks (GANs) produce high-quality images but are\nchallenging to train. They need careful regularization, vast amounts of compute,\nand expensive hyper-parameter sweeps. We make significant headway on these issues by projecting generated and real samples into a fixed, pretrained feature space.\nMotivated by the finding that the discriminator cannot fully exploit features from\ndeeper layers of the pretrained model, we propose a more effective strategy that\nmixes features across channels and resolutions. Our Projected GAN improves image quality, sample efficiency, and convergence speed. It is further compatible with\nresolutions of up to one Megapixel and advances the state-of-the-art Fréchet Inception Distance (FID) on twenty-two benchmark datasets. Importantly, Projected\nGANs match the previously lowest FIDs up to 40 times faster, cutting the wall-clock\ntime from 5 days to less than 3 hours given the same computational resources.</p>"
            },
            "media": {
                "url": "https://proceedings.neurips.cc/paper/2021/hash/9219adc5c42107c4911e249155320648-Abstract.html",
                "caption": "Projected GANs Converge Faster",
                "credit": "Axel Sauer, Kashyap Chitta, Jens Müller, Andreas Geiger"
            }
        },
        {
            "start_date": {
                "year": 2021,
                "month": 11,
                "day": 1
            },
            "text": {
                "headline": "CodeT5-base",
                "text": "<p>\"Pre-trained models for Natural Languages (NL) like BERT and GPT have been recently shown to transfer well to Programming Languages (PL) and largely benefit a broad set of code-related tasks. Despite their success, most current methods either rely on an encoder-only (or decoder-only) pre-training that is suboptimal for generation (resp. understanding) tasks or process the code snippet in the same way as NL, neglecting the special characteristics of PL such as token types. We present CodeT5, a unified pre-trained encoder-decoder Transformer model that better leverages the code semantics conveyed from the developer-assigned identifiers. Our model employs a unified framework to seamlessly support both code understanding and generation tasks and allows for multi-task learning. Besides, we propose a novel identifier-aware pre-training task that enables the model to distinguish which code tokens are identifiers and to recover them when they are masked. Furthermore, we propose to exploit the user-written code comments with a bimodal dual generation task for better NL-PL alignment. Comprehensive experiments show that CodeT5 significantly outperforms prior methods on understanding tasks such as code defect detection and clone detection, and generation tasks across various directions including PL-NL, NL-PL, and PL-PL. Further analysis reveals that our model can better capture semantic information from code. Our code and pre-trained models are released at https://github.com/salesforce/CodeT5.\"</p>"
            },
            "media": {
                "url": "https://aclanthology.org/2021.emnlp-main.685/",
                "caption": "CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation",
                "credit": "Yue Wang, Weishi Wang, Shafiq Joty, Steven C.H. Hoi"
            }
        },
        {
            "start_date": {
                "year": 2021,
                "month": 10,
                "day": 31
            },
            "text": {
                "headline": "S4",
                "text": "<p>A central goal of sequence modeling is designing a single principled model that can address sequence data across a range of modalities and tasks, particularly on long-range dependencies. Although conventional models including RNNs, CNNs, and Transformers have specialized variants for capturing long dependencies, they still struggle to scale to very long sequences of 10000 or more steps. A promising recent approach proposed modeling sequences by simulating the fundamental state space model (SSM) \\( x'(t) = Ax(t) + Bu(t), y(t) = Cx(t) + Du(t) \\), and showed that for appropriate choices of the state matrix \\( A \\), this system could handle long-range dependencies mathematically and empirically. However, this method has prohibitive computation and memory requirements, rendering it infeasible as a general sequence modeling solution. We propose the Structured State Space sequence model (S4) based on a new parameterization for the SSM, and show that it can be computed much more efficiently than prior approaches while preserving their theoretical strengths. Our technique involves conditioning \\( A \\) with a low-rank correction, allowing it to be diagonalized stably and reducing the SSM to the well-studied computation of a Cauchy kernel. S4 achieves strong empirical results across a diverse range of established benchmarks, including (i) 91\\% accuracy on sequential CIFAR-10 with no data augmentation or auxiliary losses, on par with a larger 2-D ResNet, (ii) substantially closing the gap to Transformers on image and language modeling tasks, while performing generation 60× faster (iii) SoTA on every task from the Long Range Arena benchmark, including solving the challenging Path-X task of length 16k that all prior work fails on, while being as efficient as all competitors.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2111.00396",
                "caption": "Efficiently Modeling Long Sequences with Structured State Spaces",
                "credit": "Albert Gu, Karan Goel, Christopher Ré"
            }
        },
        {
            "start_date": {
                "year": 2021,
                "month": 10,
                "day": 30
            },
            "text": {
                "headline": "EfficientZero",
                "text": "<p>Reinforcement learning has achieved great success in many applications. However, sample efficiency remains a key challenge, with prominent methods requiring millions (or even billions) of environment steps to train. Recently, there has been significant progress in sample efficient image-based RL algorithms; however, consistent human-level performance on the Atari game benchmark remains an elusive goal. We propose a sample efficient model-based visual RL algorithm built on MuZero, which we name EfficientZero. Our method achieves 194.3% mean human performance and 109.0% median performance on the Atari 100k benchmark with only two hours of real-time game experience and outperforms the state SAC in some tasks on the DMControl 100k benchmark. This is the first time an algorithm achieves super-human performance on Atari games with such little data. EfficientZero's performance is also close to DQN's performance at 200 million frames while we consume 500 times less data. EfficientZero's low sample complexity and high performance can bring RL closer to real-world applicability. We implement our algorithm in an easy-to-understand manner and it is available at this https URL. We hope it will accelerate the research of MCTS-based RL algorithms in the wider community.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2111.00210",
                "caption": "Mastering Atari Games with Limited Data",
                "credit": "Weirui Ye, Shaohuai Liu, Thanard Kurutach, Pieter Abbeel, Yang Gao"
            }
        },
        {
            "start_date": {
                "year": 2021,
                "month": 10,
                "day": 27
            },
            "text": {
                "headline": "Eve",
                "text": "<p>Quantifying the pathogenicity of protein variants in human disease-related genes would have a marked effect on clinical decisions, yet the overwhelming majority (over 98%) of these variants still have unknown consequences1–3. In principle, computational methods could support the large-scale interpretation of genetic variants. However, state-of-the-art methods4–10 have relied on training machine learning models on known disease labels. As these labels are sparse, biased and of variable quality, the resulting models have been considered insufficiently reliable11. Here we propose an approach that leverages deep generative models to predict variant pathogenicity without relying on labels. By modelling the distribution of sequence variation across organisms, we implicitly capture constraints on the protein sequences that maintain fitness. Our model EVE (evolutionary model of variant effect) not only outperforms computational approaches that rely on labelled data but also performs on par with, if not better than, predictions from high-throughput experiments, which are increasingly used as evidence for variant classifcation12–16. We predict the pathogenicity of more than 36 million variants across 3,219 disease genes and provide evidence for the classification of more than 256,000 variants of unknown significance. Our work suggests that models of evolutionary information can provide valuable independent evidence for variant interpretation that will be widely useful in research and clinical settings.</p>"
            },
            "media": {
                "url": "https://www.nature.com/articles/s41586-021-04043-8#change-history",
                "caption": "Disease variant prediction with deep generative models of evolutionary data",
                "credit": "Jonathan Frazer, Pascal Notin, Mafalda Dias, Aidan Gomez, Joseph K. Min, Kelly Brock, Yarin Gal and Debora S. Marks"
            }
        },
        {
            "start_date": {
                "year": 2021,
                "month": 10,
                "day": 17
            },
            "text": {
                "headline": "base LM+GNN+kNN",
                "text": "<p>Inspired by the notion that ``{\\it to copy is easier than to memorize}``, in this work, we introduce GNN-LM, which extends the vanilla neural language model (LM) by allowing to reference similar contexts in the entire training corpus. We build a directed heterogeneous graph between an input context and its semantically related neighbors selected from the training corpus, where nodes are tokens in the input context and retrieved neighbor contexts, and edges represent connections between nodes. Graph neural networks (GNNs) are constructed upon the graph to aggregate information from similar contexts to decode the token. This learning paradigm provides direct access to the reference contexts and helps improve a model's generalization ability. We conduct comprehensive experiments to validate the effectiveness of the GNN-LM: GNN-LM achieves a new state-of-the-art perplexity of 14.8 on WikiText-103 (a 3.9 point improvement over its counterpart of the vanilla LM model), and shows substantial improvement on One Billion Word and Enwiki8 datasets against strong baselines. In-depth ablation studies are performed to understand the mechanics of GNN-LM. \\footnote{The code can be found at this https URL</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2110.08743",
                "caption": "GNN-LM: Language Modeling based on Global Contexts via GNN",
                "credit": "Yuxian Meng, Shi Zong, Xiaoya Li, Xiaofei Sun, Tianwei Zhang, Fei Wu, Jiwei Li"
            }
        },
        {
            "start_date": {
                "year": 2021,
                "month": 10,
                "day": 15
            },
            "text": {
                "headline": "T0-XXL",
                "text": "<p>Large language models have recently been shown to attain reasonable zero-shot generalization on a diverse set of tasks (Brown et al., 2020). It has been hypothesized that this is a consequence of implicit multitask learning in language models' pretraining (Radford et al., 2019). Can zero-shot generalization instead be directly induced by explicit multitask learning? To test this question at scale, we develop a system for easily mapping any natural language tasks into a human-readable prompted form. We convert a large set of supervised datasets, each with multiple prompts with diverse wording. These prompted datasets allow for benchmarking the ability of a model to perform completely held-out tasks. We fine-tune a pretrained encoder-decoder model (Raffel et al., 2020; Lester et al., 2021) on this multitask mixture covering a wide variety of tasks. The model attains strong zero-shot performance on several standard datasets, often outperforming models up to 16x its size. Further, our approach attains strong performance on a subset of tasks from the BIG-bench benchmark, outperforming models up to 6x its size. All trained models are available at this https URL and all prompts are available at this https URL.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2110.08207",
                "caption": "Multitask Prompted Training Enables Zero-Shot Task Generalization",
                "credit": "Victor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao,  Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal V. Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng-Xin Yong, Harshit Pandey, Michael McKenna, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Tali Bers, Stella Biderman, Leo Gao, Thomas Wolf, Alexander M. Rush"
            }
        },
        {
            "start_date": {
                "year": 2021,
                "month": 10,
                "day": 12
            },
            "text": {
                "headline": "Yuan 1.0",
                "text": "<p>Recent work like GPT-3 has demonstrated excellent performance of Zero-Shot and Few-Shot learning on many natural language processing (NLP) tasks by scaling up model size, dataset size and the amount of computation. However, training a model like GPT-3 requires huge amount of computational resources which makes it challengeable to researchers. In this work, we propose a method that incorporates large-scale distributed training performance into model architecture design. With this method, Yuan 1.0, the current largest singleton language model with 245B parameters, achieves excellent performance on thousands GPUs during training, and the state-of-the-art results on NLP tasks. A data processing method is designed to efficiently filter massive amount of raw data. The current largest high-quality Chinese corpus with 5TB high quality texts is built based on this method. In addition, a calibration and label expansion method is proposed to improve the Zero-Shot and Few-Shot performance, and steady improvement is observed on the accuracy of various tasks. Yuan 1.0 presents strong capacity of natural language generation, and the generated articles are difficult to distinguish from the human-written ones.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2110.04725",
                "caption": "Yuan 1.0: Large-Scale Pre-trained Language Model in Zero-Shot and Few-Shot Learning",
                "credit": "Shaohua Wu, Xudong Zhao, Tong Yu, Rongguo Zhang, Chong Shen, Hongli Liu, Feng Li, Hong Zhu, Jiangang Luo, Liang Xu, Xuanwei Zhang, Jun Liu"
            }
        },
        {
            "start_date": {
                "year": 2021,
                "month": 10,
                "day": 11
            },
            "text": {
                "headline": "Megatron-Turing NLG 530B",
                "text": "<p>Pretrained general-purpose language models can achieve state-of-the-art accuracies in various natural language processing domains by adapting to downstream tasks via zero-shot, few-shot and fine-tuning techniques. Because of their success, the size of these models has increased rapidly, requiring high-performance hardware, software, and algorithmic techniques to enable training such large models. As the result of a joint effort between Microsoft and NVIDIA, we present details on the training of the largest monolithic transformer based language model, Megatron-Turing NLG 530B (MT-NLG), with 530 billion parameters. In this paper, we first focus on the infrastructure as well as the 3D parallelism methodology used to train this model using DeepSpeed and Megatron. Next, we detail the training process, the design of our training corpus, and our data curation techniques, which we believe is a key ingredient to the success of the model. Finally, we discuss various evaluation results, as well as other interesting observations and new properties exhibited by MT-NLG. We demonstrate that MT-NLG achieves superior zero-, one-, and few-shot learning accuracies on several NLP benchmarks and establishes new state-of-the-art results. We believe that our contributions will help further the development of large-scale training infrastructures, large-scale language models, and natural language generations.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2201.11990",
                "caption": "Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model",
                "credit": "Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, Elton Zhang, Rewon Child, Reza Yazdani Aminabadi, Julie Bernauer, Xia Song, Mohammad Shoeybi, Yuxiong He, Michael Houston, Saurabh Tiwary, Bryan Catanzaro"
            }
        },
        {
            "start_date": {
                "year": 2021,
                "month": 10,
                "day": 4
            },
            "text": {
                "headline": "AlphaFold-Multimer",
                "text": "<p>While the vast majority of well-structured single protein chains can now be predicted to high accuracy due to the recent AlphaFold [1] model, the prediction of multi-chain protein complexes remains a challenge in many cases. In this work, we demonstrate that an AlphaFold model trained specifically for multimeric inputs of known stoichiometry, which we call AlphaFold-Multimer, significantly increases accuracy of predicted multimeric interfaces over input-adapted single-chain AlphaFold while maintaining high intra-chain accuracy. On a benchmark dataset of 17 heterodimer proteins without templates (introduced in [2]) we achieve at least medium accuracy (DockQ [3] ≥ 0.49) on 14 targets and high accuracy (DockQ ≥ 0.8) on 6 targets, compared to 9 targets of at least medium accuracy and 4 of high accuracy for the previous state of the art system (an AlphaFold-based system from [2]). We also predict structures for a large dataset of 4,433 recent protein complexes, from which we score all non-redundant interfaces with low template identity. For heteromeric interfaces we successfully predict the interface (DockQ ≥ 0.23) in 67% of cases, and produce high accuracy predictions (DockQ ≥ 0.8) in 23% of cases, an improvement of +25 and +11 percentage points over the flexible linker modification of AlphaFold [4] respectively. For homomeric interfaces we successfully predict the interface in 69% of cases, and produce high accuracy predictions in 34% of cases, an improvement of +5 percentage points in both instances.</p>"
            },
            "media": {
                "url": "https://www.biorxiv.org/content/10.1101/2021.10.04.463034v1",
                "caption": "Protein complex prediction with AlphaFold-Multimer",
                "credit": "Richard Evans, Michael O’Neill, Alexander Pritzel, Natasha Antropova, Andrew Senior, Tim Green, Augustin Žídek, Russ Bates, Sam Blackwell, Jason Yim, Olaf Ronneberger, Sebastian Bodenstein, Michal Zielinski, Alex Bridgland, Anna Potapenko, Andrew Cowie, Kathryn Tunyasuvunakool, Rishub Jain, Ellen Clancy, Pushmeet Kohli, John Jumper and Demis Hassabis"
            }
        },
        {
            "start_date": {
                "year": 2021,
                "month": 9,
                "day": 21
            },
            "text": {
                "headline": "TrOCR",
                "text": "<p>Text recognition is a long-standing research problem for document digitalization. Existing approaches are usually built based on CNN for image understanding and RNN for char-level text generation. In addition, another language model is usually needed to improve the overall accuracy as a post-processing step. In this paper, we propose an end-to-end text recognition approach with pre-trained image Transformer and text Transformer models, namely TrOCR, which leverages the Transformer architecture for both image understanding and wordpiece-level text generation. The TrOCR model is simple but effective, and can be pre-trained with large-scale synthetic data and fine-tuned with human-labeled datasets. Experiments show that the TrOCR model outperforms the current state-of-the-art models on the printed, handwritten and scene text recognition tasks. The TrOCR models and code are publicly available at https://aka.ms/trocr</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2109.10282",
                "caption": "TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models",
                "credit": "Minghao Li, Tengchao Lv, Jingye Chen, Lei Cui, Yijuan Lu, Dinei Florencio, Cha Zhang, Zhoujun Li, Furu Wei"
            }
        },
        {
            "start_date": {
                "year": 2021,
                "month": 9,
                "day": 20
            },
            "text": {
                "headline": "PLATO-XL",
                "text": "<p>To explore the limit of dialogue generation pre-training, we present the models of PLATO-XL with up to 11 billion parameters, trained on both Chinese and English social media conversations. To train such large models, we adopt the architecture of unified transformer with high computation and parameter efficiency. In addition, we carry out multi-party aware pre-training to better distinguish the characteristic information in social media conversations. With such designs, PLATO-XL successfully achieves superior performances as compared to other approaches in both Chinese and English chitchat. We further explore the capacity of PLATO-XL on other conversational tasks, such as knowledge grounded dialogue and task-oriented conversation. The experimental results indicate that PLATO-XL obtains state-of-the-art results across multiple conversational tasks, verifying its potential as a foundation model of conversational AI.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2109.09519",
                "caption": "PLATO-XL: Exploring the Large-scale Pre-training of Dialogue Generation",
                "credit": "Siqi Bao, Huang He, Fan Wang, Hua Wu, Haifeng Wang, Wenquan Wu, Zhihua Wu, Zhen Guo, Hua Lu, Xinxian Huang, Xin Tian, Xinchao Xu, Yingzhan Lin, Zheng-Yu Niu"
            }
        },
        {
            "start_date": {
                "year": 2021,
                "month": 9,
                "day": 10
            },
            "text": {
                "headline": "HyperCLOVA 204B",
                "text": "<p></p>"
            },
            "media": {
                "url": "",
                "caption": "",
                "credit": ""
            }
        },
        {
            "start_date": {
                "year": 2021,
                "month": 9,
                "day": 6
            },
            "text": {
                "headline": "PermuteFormer",
                "text": "<p>A recent variation of Transformer, Performer, scales Transformer to longer sequences with a linear attention mechanism. However, it is not compatible with relative position encoding, which has advantages over absolute position encoding. In this paper, we discuss possible ways to add relative position encoding to Performer. Based on the analysis, we propose PermuteFormer, a Performer-based model with relative position encoding that scales linearly on long sequences. PermuteFormer applies position-dependent transformation on queries and keys to encode positional information into the attention module. This transformation is carefully crafted so that the final output of self-attention is not affected by absolute positions of tokens. PermuteFormer introduces negligible computational overhead by design that it runs as fast as Performer. We evaluate PermuteFormer on Long-Range Arena, a dataset for long sequences, as well as WikiText-103, a language modeling dataset. The experiments show that PermuteFormer uniformly improves the performance of Performer with almost no computational overhead and outperforms vanilla Transformer on most of the tasks.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2109.02377",
                "caption": "PermuteFormer: Efficient Relative Position Encoding for Long Sequences",
                "credit": "Peng Chen"
            }
        },
        {
            "start_date": {
                "year": 2021,
                "month": 9,
                "day": 4
            },
            "text": {
                "headline": "MEB",
                "text": "<p>Recently, Transformer-based deep learning models like GPT-3 have been getting a lot of attention in the machine learning world. These models excel at understanding semantic relationships, and they have contributed to large improvements in Microsoft Bing’s search experience and surpassing human performance on the SuperGLUE academic benchmark. However, these models can fail to capture more nuanced relationships between query and document terms beyond pure semantics.\n\nIn this blog post, we are introducing “Make Every feature Binary” (MEB), a large-scale sparse model that complements our production Transformer models to improve search relevance for Microsoft customers using AI at Scale. To make search more accurate and dynamic, MEB better harnesses the power of large data and allows for an input feature space with over 200 billion binary features that reflect the subtle relationships between search queries and documents.</p>"
            },
            "media": {
                "url": "https://www.microsoft.com/en-us/research/blog/make-every-feature-binary-a-135b-parameter-sparse-neural-network-for-massively-improved-search-relevance/",
                "caption": "Make Every feature Binary: A 135B parameter sparse neural network for massively improved search relevance",
                "credit": "W Liu, Z Wang, X Liu, N Zeng, Y Liu, FE Alsaadi"
            }
        },
        {
            "start_date": {
                "year": 2021,
                "month": 9,
                "day": 3
            },
            "text": {
                "headline": "FLAN 137B",
                "text": "<p>This paper explores a simple method for improving the zero-shot learning abilities of language models. We show that instruction tuning—finetuning language models on a collection of datasets described via instructions—substantially improves zeroshot performance on unseen tasks. We take a 137B parameter pretrained language model and instruction tune it on over 60 NLP datasets verbalized via natural language instruction templates. We evaluate this instruction-tuned model, which we call FLAN, on unseen task types. FLAN substantially improves the performance of its unmodified counterpart and surpasses zero-shot 175B GPT-3 on 20 of 25 datasets that we evaluate. FLAN even outperforms few-shot GPT-3 by a large margin on ANLI, RTE, BoolQ, AI2-ARC, OpenbookQA, and StoryCloze. Ablation studies reveal that number of finetuning datasets, model scale, and natural language instructions are key to the success of instruction tuning.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2109.01652",
                "caption": "Finetuned Language Models Are Zero-Shot Learners",
                "credit": "Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V. Le"
            }
        },
        {
            "start_date": {
                "year": 2021,
                "month": 8,
                "day": 17
            },
            "text": {
                "headline": "XLMR-XXL",
                "text": "<p>Recent work has demonstrated the effectiveness of cross-lingual language model pretraining for cross-lingual understanding. In this study, we present the results of two larger multilingual masked language models, with 3.5B and 10.7B parameters. Our two new models dubbed XLM-R XL and XLM-R XXL outperform XLM-R by 1.8% and 2.4% average accuracy on XNLI. Our model also outperforms the RoBERTa-Large model on several English tasks of the GLUE benchmark by 0.3% on average while handling 99 more languages. This suggests pretrained models with larger capacity may obtain both strong performance on high-resource languages while greatly improving low-resource languages. We make our code and models publicly available.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2105.00572",
                "caption": "Larger-Scale Transformers for Multilingual Masked Language Modeling",
                "credit": "Naman Goyal, Jingfei Du, Myle Ott, Giri Anantharaman, Alexis Conneau"
            }
        },
        {
            "start_date": {
                "year": 2021,
                "month": 8,
                "day": 15
            },
            "text": {
                "headline": "DNABERT",
                "text": "<p>Motivation\nDeciphering the language of non-coding DNA is one of the fundamental problems in genome research. Gene regulatory code is highly complex due to the existence of polysemy and distant semantic relationship, which previous informatics methods often fail to capture especially in data-scarce scenarios.\n\nResults\nTo address this challenge, we developed a novel pre-trained bidirectional encoder representation, named DNABERT, to capture global and transferrable understanding of genomic DNA sequences based on up and downstream nucleotide contexts. We compared DNABERT to the most widely used programs for genome-wide regulatory elements prediction and demonstrate its ease of use, accuracy and efficiency. We show that the single pre-trained transformers model can simultaneously achieve state-of-the-art performance on prediction of promoters, splice sites and transcription factor binding sites, after easy fine-tuning using small task-specific labeled data. Further, DNABERT enables direct visualization of nucleotide-level importance and semantic relationship within input sequences for better interpretability and accurate identification of conserved sequence motifs and functional genetic variant candidates. Finally, we demonstrate that pre-trained DNABERT with human genome can even be readily applied to other organisms with exceptional performance. We anticipate that the pre-trained DNABERT model can be fined tuned to many other sequence analyses tasks.</p>"
            },
            "media": {
                "url": "https://academic.oup.com/bioinformatics/article/37/15/2112/6128680",
                "caption": "DNABERT: pre-trained Bidirectional Encoder Representations from Transformers model for DNA-language in genome",
                "credit": "Yanrong Ji, Zhihan Zhou, Han Liu, Ramana V Davuluri"
            }
        },
        {
            "start_date": {
                "year": 2021,
                "month": 8,
                "day": 11
            },
            "text": {
                "headline": "Zidong Taichu",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://gitee.com/zidongtaichu/multi-modal-models",
                "caption": "Zidong Ancestral multi-modal large model",
                "credit": ""
            }
        },
        {
            "start_date": {
                "year": 2021,
                "month": 8,
                "day": 11
            },
            "text": {
                "headline": "Jurassic-1-Jumbo",
                "text": "<p>Jurassic-1 is a pair of auto-regressive language models recently released by AI21 Labs, consisting of J1-Jumbo, a 178B-parameter model, and J1-Large, a 7B-parameter model. We describe their architecture and training, and evaluate their performance relative to GPT-3. The evaluation is in terms of perplexity, as well as zero-shot and few-shot learning. To that end, we developed a zeroshot and few-shot test suite, which we made publicly available (https://github.com/ai21labs/ lm-evaluation) as a shared resource for the evaluation of mega language models.</p>"
            },
            "media": {
                "url": "https://uploads-ssl.webflow.com/60fd4503684b466578c0d307/61138924626a6981ee09caf6_jurassic_tech_paper.pdf",
                "caption": "Jurassic-1: Technical Details and Evaluation",
                "credit": "Opher Lieber, Or Sharir, Barak Lenz, Yoav Shoham"
            }
        },
        {
            "start_date": {
                "year": 2021,
                "month": 8,
                "day": 7
            },
            "text": {
                "headline": "W2v-BERT",
                "text": "<p>Motivated by the success of masked language modeling~(MLM) in pre-training natural language processing models, we propose w2v-BERT that explores MLM for self-supervised speech representation learning. w2v-BERT is a framework that combines contrastive learning and MLM, where the former trains the model to discretize input continuous speech signals into a finite set of discriminative speech tokens, and the latter trains the model to learn contextualized speech representations via solving a masked prediction task consuming the discretized tokens. In contrast to existing MLM-based speech pre-training frameworks such as HuBERT, which relies on an iterative re-clustering and re-training process, or vq-wav2vec, which concatenates two separately trained modules, w2v-BERT can be optimized in an end-to-end fashion by solving the two self-supervised tasks~(the contrastive task and MLM) simultaneously. Our experiments show that w2v-BERT achieves competitive results compared to current state-of-the-art pre-trained models on the LibriSpeech benchmarks when using the Libri-Light~60k corpus as the unsupervised data. In particular, when compared to published models such as conformer-based wav2vec~2.0 and HuBERT, our model shows~5\\% to~10\\% relative WER reduction on the test-clean and test-other subsets. When applied to the Google's Voice Search traffic dataset, w2v-BERT outperforms our internal conformer-based wav2vec~2.0 by more than~30\\% relatively.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2108.06209v2",
                "caption": "W2v-BERT: Combining Contrastive Learning and Masked Language Modeling for Self-Supervised Speech Pre-Training",
                "credit": "Yu-An Chung, Yu Zhang, Wei Han, Chung-Cheng Chiu, James Qin, Ruoming Pang, Yonghui Wu"
            }
        },
        {
            "start_date": {
                "year": 2021,
                "month": 8,
                "day": 6
            },
            "text": {
                "headline": "YOLOX-X",
                "text": "<p>In this report, we present some experienced improvements to YOLO series, forming a new high-performance detector -- YOLOX. We switch the YOLO detector to an anchor-free manner and conduct other advanced detection techniques, i.e., a decoupled head and the leading label assignment strategy SimOTA to achieve state-of-the-art results across a large scale range of models: For YOLO-Nano with only 0.91M parameters and 1.08G FLOPs, we get 25.3% AP on COCO, surpassing NanoDet by 1.8% AP; for YOLOv3, one of the most widely used detectors in industry, we boost it to 47.3% AP on COCO, outperforming the current best practice by 3.0% AP; for YOLOX-L with roughly the same amount of parameters as YOLOv4-CSP, YOLOv5-L, we achieve 50.0% AP on COCO at a speed of 68.9 FPS on Tesla V100, exceeding YOLOv5-L by 1.8% AP. Further, we won the 1st Place on Streaming Perception Challenge (Workshop on Autonomous Driving at CVPR 2021) using a single YOLOX-L model. We hope this report can provide useful experience for developers and researchers in practical scenes, and we also provide deploy versions with ONNX, TensorRT, NCNN, and Openvino supported. Source code is at this https URL.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2107.08430",
                "caption": "YOLOX: Exceeding YOLO Series in 2021",
                "credit": "Zheng Ge, Songtao Liu, Feng Wang, Zeming Li, Jian Sun"
            }
        },
        {
            "start_date": {
                "year": 2021,
                "month": 8,
                "day": 3
            },
            "text": {
                "headline": "6-Act Tether",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://openaccess.thecvf.com/content/ICCV2021/html/Ye_Auxiliary_Tasks_and_Exploration_Enable_ObjectGoal_Navigation_ICCV_2021_paper.html",
                "caption": "Auxiliary Tasks and Exploration Enable ObjectGoal Navigation",
                "credit": "Joel Ye, Dhruv Batra, Abhishek Das, Erik Wijmans"
            }
        },
        {
            "start_date": {
                "year": 2021,
                "month": 7,
                "day": 29
            },
            "text": {
                "headline": "SEER",
                "text": "<p>Recently, self-supervised learning methods like MoCo, SimCLR, BYOL and SwAV have reduced the gap with supervised methods. These results have been achieved in a control environment, that is the highly curated ImageNet dataset. However, the premise of self-supervised learning is that it can learn from any random image and from any unbounded dataset. In this work, we explore if self-supervision lives to its expectation by training large models on random, uncurated images with no supervision. Our final SElf-supERvised (SEER) model, a RegNetY with 1.3B parameters trained on 1B random images with 512 GPUs achieves 84.2% top-1 accuracy, surpassing the best self-supervised pretrained model by 1% and confirming that self-supervised learning works in a real world setting. Interestingly, we also observe that self-supervised models are good few-shot learners achieving 77.9% top-1 with access to only 10% of ImageNet. Code: this https URL</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2103.01988",
                "caption": "Self-supervised Pretraining of Visual Features in the Wild",
                "credit": "Priya Goyal, Mathilde Caron, Benjamin Lefaudeux, Min Xu, Pengchao Wang, Vivek Pai, Mannat Singh, Vitaliy Liptchinsky, Ishan Misra, Armand Joulin, Piotr Bojanowski"
            }
        },
        {
            "start_date": {
                "year": 2021,
                "month": 7,
                "day": 27
            },
            "text": {
                "headline": "HuBERT",
                "text": "<p>Self-supervised approaches for speech representation learning are challenged by three unique problems: (1) there are multiple sound units in each input utterance, (2) there is no lexicon of input sound units during the pre-training phase, and (3) sound units have variable lengths with no explicit segmentation. To deal with these three problems, we propose the Hidden-Unit BERT (HuBERT) approach for self-supervised speech representation learning, which utilizes an offline clustering step to provide aligned target labels for a BERT-like prediction loss. A key ingredient of our approach is applying the prediction loss over the masked regions only, which forces the model to learn a combined acoustic and language model over the continuous inputs. HuBERT relies primarily on the consistency of the unsupervised clustering step rather than the intrinsic quality of the assigned cluster labels. Starting with a simple k-means teacher of 100 clusters, and using two iterations of clustering, the HuBERT model either matches or improves upon the state-of-the-art wav2vec 2.0 performance on the Librispeech (960h) and Libri-light (60,000h) benchmarks with 10min, 1h, 10h, 100h, and 960h fine-tuning subsets. Using a 1B parameter model, HuBERT shows up to 19% and 13% relative WER reduction on the more challenging dev-other and test-other evaluation subsets.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2106.07447",
                "caption": "HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units",
                "credit": "Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, Abdelrahman Mohamed"
            }
        },
        {
            "start_date": {
                "year": 2021,
                "month": 7,
                "day": 27
            },
            "text": {
                "headline": "GOAT",
                "text": "<p>In this work we create agents that can perform well beyond a single, individual task, that exhibit much wider generalisation of behaviour to a massive, rich space of challenges. We define a universe of tasks within an environment domain and demonstrate the ability to train agents that are generally capable across this vast space and beyond. The environment is natively multi-agent, spanning the continuum of competitive, cooperative, and independent games, which are situated within procedurally generated physical 3D worlds. The resulting space is exceptionally diverse in terms of the challenges posed to agents, and as such, even measuring the learning progress of an agent is an open research problem. We propose an iterative notion of improvement between successive generations of agents, rather than seeking to maximise a singular objective, allowing us to quantify progress despite tasks being incomparable in terms of achievable rewards. We show that through constructing an open-ended learning process, which dynamically changes the training task distributions and training objectives such that the agent never stops learning, we achieve consistent learning of new behaviours. The resulting agent is able to score reward in every one of our humanly solvable evaluation levels, with behaviour generalising to many held-out points in the universe of tasks. Examples of this zero-shot generalisation include good performance on Hide and Seek, Capture the Flag, and Tag. Through analysis and hand-authored probe tasks we characterise the behaviour of our agent, and find interesting emergent heuristic behaviours such as trial-and-error experimentation, simple tool use, option switching, and cooperation. Finally, we demonstrate that the general capabilities of this agent could unlock larger scale transfer of behaviour through cheap finetuning.</p>"
            },
            "media": {
                "url": "https://deepmind.com/blog/article/generally-capable-agents-emerge-from-open-ended-play\n\nhttps://arxiv.org/abs/2107.12808",
                "caption": "Open-Ended Learning Leads to Generally Capable Agents",
                "credit": "Open-Ended Learning Team*, Adam Stooke, Anuj Mahajan, Catarina Barros, Charlie Deck, Jakob Bauer, Jakub Sygnowski, Maja Trebacz, Max Jaderberg, Michael Mathieu, Nat McAleese, Nathalie Bradley-Schmieg, Nathaniel Wong, Nicolas Porcel, Roberta Raileanu, Steph Hughes-Fitt, Valentin Dalibard and Wojciech Marian Czarnecki"
            }
        },
        {
            "start_date": {
                "year": 2021,
                "month": 7,
                "day": 7
            },
            "text": {
                "headline": "Codex",
                "text": "<p>We introduce Codex, a GPT language model fine-tuned on publicly available code from GitHub, and study its Python code-writing capabilities. A distinct production version of Codex powers GitHub Copilot. On HumanEval, a new evaluation set we release to measure functional correctness for synthesizing programs from docstrings, our model solves 28.8% of the problems, while GPT-3 solves 0% and GPT-J solves 11.4%. Furthermore, we find that repeated sampling from the model is a surprisingly effective strategy for producing working solutions to difficult prompts. Using this method, we solve 70.2% of our problems with 100 samples per problem. Careful investigation of our model reveals its limitations, including difficulty with docstrings describing long chains of operations and with binding operations to variables. Finally, we discuss the potential broader impacts of deploying powerful code generation technologies, covering safety, security, and economics.</p>"
            },
            "media": {
                "url": "https://openai.com/blog/openai-codex/\nhttps://arxiv.org/abs/2107.03374",
                "caption": "Evaluating Large Language Models Trained on Code",
                "credit": "Mark Chen , Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger,  Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji,  Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, Wojciech Zaremba "
            }
        },
        {
            "start_date": {
                "year": 2021,
                "month": 7,
                "day": 5
            },
            "text": {
                "headline": "ERNIE 3.0",
                "text": "<p></p>"
            },
            "media": {
                "url": "http://research.baidu.com/Blog/index-view?id=160",
                "caption": "ERNIE 3.0: Large-scale Knowledge Enhanced Pre-training for Language Understanding and Generation",
                "credit": "Y Sun, S Wang, S Feng, S Ding, C Pang"
            }
        },
        {
            "start_date": {
                "year": 2021,
                "month": 6,
                "day": 28
            },
            "text": {
                "headline": "Adaptive Input Transformer + RD",
                "text": "<p>Dropout is a powerful and widely used technique to regularize the training of deep neural networks. In this paper, we introduce a simple regularization strategy upon dropout in model training, namely R-Drop, which forces the output distributions of different sub models generated by dropout to be consistent with each other. Specifically, for each training sample, R-Drop minimizes the bidirectional KL-divergence between the output distributions of two sub models sampled by dropout. Theoretical analysis reveals that R-Drop reduces the freedom of the model parameters and complements dropout. Experiments on 5 widely used deep learning tasks (18 datasets in total), including neural machine translation, abstractive summarization, language understanding, language modeling, and image classification, show that R-Drop is universally effective. In particular, it yields substantial improvements when applied to fine-tune large-scale pre-trained models, e.g., ViT, RoBERTa-large, and BART, and achieves state-of-the-art (SOTA) performances with the vanilla Transformer model on WMT14 English→German translation (30.91 BLEU) and WMT14 English→French translation (43.95 BLEU), even surpassing models trained with extra large-scale data and expert-designed advanced variants of Transformer models. Our code is available at GitHub{\\url{this https URL}}.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2106.14448",
                "caption": "R-Drop: Regularized Dropout for Neural Networks",
                "credit": "Xiaobo Liang, Lijun Wu, Juntao Li, Yue Wang, Qi Meng, Tao Qin, Wei Chen, Min Zhang, Tie-Yan Liu"
            }
        },
        {
            "start_date": {
                "year": 2021,
                "month": 6,
                "day": 23
            },
            "text": {
                "headline": "EfficientNetV2-XL",
                "text": "<p>This paper introduces EfficientNetV2, a new family of convolutional networks that have faster training speed and better parameter efficiency than previous models. To develop this family of models, we use a combination of training-aware neural architecture search and scaling, to jointly optimize training speed and parameter efficiency. The models were searched from the search space enriched with new ops such as Fused-MBConv. Our experiments show that EfficientNetV2 models train much faster than state-of-the-art models while being up to 6.8x smaller.\nOur training can be further sped up by progressively increasing the image size during training, but it often causes a drop in accuracy. To compensate for this accuracy drop, we propose to adaptively adjust regularization (e.g., dropout and data augmentation) as well, such that we can achieve both fast training and good accuracy.\nWith progressive learning, our EfficientNetV2 significantly outperforms previous models on ImageNet and CIFAR/Cars/Flowers datasets. By pretraining on the same ImageNet21k, our EfficientNetV2 achieves 87.3% top-1 accuracy on ImageNet ILSVRC2012, outperforming the recent ViT by 2.0% accuracy while training 5x-11x faster using the same computing resources. Code will be available at this https URL.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2104.00298",
                "caption": "EfficientNetV2: Smaller Models and Faster Training",
                "credit": "Mingxing Tan, Quoc V. Le"
            }
        },
        {
            "start_date": {
                "year": 2021,
                "month": 6,
                "day": 11
            },
            "text": {
                "headline": "Denoising Diffusion Probabilistic Models (LSUN Bedroom)",
                "text": "<p>We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at this https URL</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2006.11239",
                "caption": "Denoising Diffusion Probabilistic Models",
                "credit": "Jonathan Ho, Ajay Jain, Pieter Abbeel"
            }
        },
        {
            "start_date": {
                "year": 2021,
                "month": 6,
                "day": 11
            },
            "text": {
                "headline": "ALIGN",
                "text": "<p>Pre-trained representations are becoming crucial for many NLP and perception tasks. While representation learning in NLP has transitioned to training on raw text without human annotations, visual and vision-language representations still rely heavily on curated training datasets that are expensive or require expert knowledge. For vision applications, representations are mostly learned using datasets with explicit class labels such as ImageNet or OpenImages. For vision-language, popular datasets like Conceptual Captions, MSCOCO, or CLIP all involve a non-trivial data collection (and cleaning) process. This costly curation process limits the size of datasets and hence hinders the scaling of trained models. In this paper, we leverage a noisy dataset of over one billion image alt-text pairs, obtained without expensive filtering or post-processing steps in the Conceptual Captions dataset. A simple dual-encoder architecture learns to align visual and language representations of the image and text pairs using a contrastive loss. We show that the scale of our corpus can make up for its noise and leads to state-of-the-art representations even with such a simple learning scheme. Our visual representation achieves strong performance when transferred to classification tasks such as ImageNet and VTAB. The aligned visual and language representations enables zero-shot image classification and also set new state-of-the-art results on Flickr30K and MSCOCO image-text retrieval benchmarks, even when compared with more sophisticated cross-attention models. The representations also enable cross-modality search with complex text and text + image queries.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2102.05918",
                "caption": "Scaling up visual and vision-language representation learning with noisy text supervision",
                "credit": "Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V. Le, Yunhsuan Sung, Zhen Li, Tom Duerig"
            }
        },
        {
            "start_date": {
                "year": 2021,
                "month": 6,
                "day": 10
            },
            "text": {
                "headline": "DeBERTa",
                "text": "<p>Recent progress in pre-trained neural language models has significantly improved the performance of many natural language processing (NLP) tasks. In this paper we propose a new model architecture DeBERTa (Decoding-enhanced BERT with disentangled attention) that improves the BERT and RoBERTa models using two novel techniques. The first is the disentangled attention mechanism, where each word is represented using two vectors that encode its content and position, respectively, and the attention weights among words are computed using disentangled matrices on their contents and relative positions, respectively. Second, an enhanced mask decoder is used to incorporate absolute positions in the decoding layer to predict the masked tokens in model pre-training. In addition, a new virtual adversarial training method is used for fine-tuning to improve models' generalization. We show that these techniques significantly improve the efficiency of model pre-training and the performance of both natural language understanding (NLU) and natural langauge generation (NLG) downstream tasks. Compared to RoBERTa-Large, a DeBERTa model trained on half of the training data performs consistently better on a wide range of NLP tasks, achieving improvements on MNLI by +0.9% (90.2% vs. 91.1%), on SQuAD v2.0 by +2.3% (88.4% vs. 90.7%) and RACE by +3.6% (83.2% vs. 86.8%). Notably, we scale up DeBERTa by training a larger version that consists of 48 Transform layers with 1.5 billion parameters. The significant performance boost makes the single DeBERTa model surpass the human performance on the SuperGLUE benchmark (Wang et al., 2019a) for the first time in terms of macro-average score (89.9 versus 89.8), and the ensemble DeBERTa model sits atop the SuperGLUE leaderboard as of January 6, 2021, out performing the human baseline by a decent margin (90.3 versus 89.8).</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2006.03654",
                "caption": "DeBERTa: Decoding-enhanced BERT with Disentangled Attention",
                "credit": "Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen"
            }
        },
        {
            "start_date": {
                "year": 2021,
                "month": 6,
                "day": 9
            },
            "text": {
                "headline": "EMDR",
                "text": "<p>We present an end-to-end differentiable training method for retrieval-augmented open-domain question answering systems that combine information from multiple retrieved documents when generating answers. We model retrieval decisions as latent variables over sets of relevant documents. Since marginalizing over sets of retrieved documents is computationally hard, we approximate this using an expectation-maximization algorithm. We iteratively estimate the value of our latent variable (the set of relevant documents for a given question) and then use this estimate to update the retriever and reader parameters. We hypothesize that such end-to-end training allows training signals to flow to the reader and then to the retriever better than staged-wise training. This results in a retriever that is able to select more relevant documents for a question and a reader that is trained on more accurate documents to generate an answer. Experiments on three benchmark datasets demonstrate that our proposed method outperforms all existing approaches of comparable size by 2-3% absolute exact match points, achieving new state-of-the-art results. Our results also demonstrate the feasibility of learning to retrieve to improve answer generation without explicit supervision of retrieval decisions.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2106.05346v2",
                "caption": "End-to-End Training of Multi-Document Reader and Retriever for Open-Domain Question Answering",
                "credit": "Devendra Singh Sachan, Siva Reddy, William Hamilton, Chris Dyer, Dani Yogatama"
            }
        },
        {
            "start_date": {
                "year": 2021,
                "month": 6,
                "day": 9
            },
            "text": {
                "headline": "CoAtNet",
                "text": "<p>Transformers have attracted increasing interests in computer vision, but they still fall behind state-of-the-art convolutional networks. In this work, we show that while Transformers tend to have larger model capacity, their generalization can be worse than convolutional networks due to the lack of the right inductive bias. To effectively combine the strengths from both architectures, we present CoAtNets (pronounced “coat” nets), a family of hybrid models built from two key insights: (1) depthwise Convolution and self-Attention can be naturally unified via simple relative attention; (2) vertically stacking convolution layers and attention layers in a principled way is surprisingly effective in improving generalization, capacity and efficiency. Experiments show that our CoAtNets achieve state-of-the-art performance under different resource constraints across various datasets: Without extra data, CoAtNet achieves 86.0% ImageNet top-1 accuracy; When pre-trained with 13M images from ImageNet-21K, our CoAtNet achieves 88.56% top-1 accuracy, matching ViT-huge pre-trained with 300M images from JFT-300M while using 23x less data; Notably, when we further scale up CoAtNet with JFT-3B, it achieves 90.88% top-1 accuracy on ImageNet, establishing a new state-of-the-art result.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2106.04803v2",
                "caption": "CoAtNet: Marrying Convolution and Attention\nfor All Data Sizes",
                "credit": "Zihang Dai, Hanxiao Liu, Quoc V. Le, Mingxing Tan"
            }
        },
        {
            "start_date": {
                "year": 2021,
                "month": 6,
                "day": 8
            },
            "text": {
                "headline": "ViT-G/14",
                "text": "<p>Attention-based neural networks such as the Vision Transformer (ViT) have recently attained state-of-the-art results on many computer vision benchmarks. Scale is a primary ingredient in attaining excellent results, therefore, understanding a model's scaling properties is a key to designing future generations effectively. While the laws for scaling Transformer language models have been studied, it is unknown how Vision Transformers scale. To address this, we scale ViT models and data, both up and down, and characterize the relationships between error rate, data, and compute. Along the way, we refine the architecture and training of ViT, reducing memory consumption and increasing accuracy of the resulting models. As a result, we successfully train a ViT model with two billion parameters, which attains a new state-of-the-art on ImageNet of 90.45% top-1 accuracy. The model also performs well for few-shot transfer, for example, reaching 84.86% top-1 accuracy on ImageNet with only 10 examples per class.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2106.04560",
                "caption": "Scaling Vision Transformers",
                "credit": "X Zhai, A Kolesnikov, N Houlsby, L Beyer"
            }
        },
        {
            "start_date": {
                "year": 2021,
                "month": 5,
                "day": 28
            },
            "text": {
                "headline": "ByT5-XXL",
                "text": "<p>Most widely-used pre-trained language models operate on sequences of tokens corresponding to word or subword units. By comparison, token-free models that operate directly on raw text (bytes or characters) have many benefits: they can process text in any language out of the box, they are more robust to noise, and they minimize technical debt by removing complex and error-prone text preprocessing pipelines. Since byte or character sequences are longer than token sequences, past work on token-free models has often introduced new model architectures designed to amortize the cost of operating directly on raw text. In this paper, we show that a standard Transformer architecture can be used with minimal modifications to process byte sequences. We characterize the trade-offs in terms of parameter count, training FLOPs, and inference speed, and show that byte-level models are competitive with their token-level counterparts. We also demonstrate that byte-level models are significantly more robust to noise and perform better on tasks that are sensitive to spelling and pronunciation. As part of our contribution, we release a new set of pre-trained byte-level Transformer models based on the T5 architecture, as well as all code and data used in our experiments.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2105.13626",
                "caption": "ByT5: Towards a token-free future with pre-trained byte-to-byte models",
                "credit": "Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam Roberts, Colin Raffel"
            }
        },
        {
            "start_date": {
                "year": 2021,
                "month": 5,
                "day": 26
            },
            "text": {
                "headline": "Transformer local-attention (NesT-B)",
                "text": "<p>Hierarchical structures are popular in recent vision transformers, however, they require sophisticated designs and massive datasets to work well. In this paper, we explore the idea of nesting basic local transformers on non-overlapping image blocks and aggregating them in a hierarchical way. We find that the block aggregation function plays a critical role in enabling cross-block non-local information communication. This observation leads us to design a simplified architecture that requires minor code changes upon the original vision transformer. The benefits of the proposed judiciously-selected design are threefold: (1) NesT converges faster and requires much less training data to achieve good generalization on both ImageNet and small datasets like CIFAR; (2) when extending our key ideas to image generation, NesT leads to a strong decoder that is 8× faster than previous transformer-based generators; and (3) we show that decoupling the feature learning and abstraction processes via this nested hierarchy in our design enables constructing a novel method (named GradCAT) for visually interpreting the learned model. Source code is available this https URL.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2105.12723v4",
                "caption": "Nested Hierarchical Transformer: Towards Accurate, Data-Efficient and Interpretable Visual Understanding",
                "credit": "Zizhao Zhang, Han Zhang, Long Zhao, Ting Chen, Sercan Arık, Tomas Pfister"
            }
        },
        {
            "start_date": {
                "year": 2021,
                "month": 5,
                "day": 26
            },
            "text": {
                "headline": "CogView",
                "text": "<p>Text-to-Image generation in the general domain has long been an open problem, which requires both a powerful generative model and cross-modal understanding. We propose CogView, a 4-billion-parameter Transformer with VQ-VAE tokenizer to advance this problem. We also demonstrate the finetuning strategies for various downstream tasks, e.g. style learning, super-resolution, text-image ranking and fashion design, and methods to stabilize pretraining, e.g. eliminating NaN losses. CogView achieves the state-of-the-art FID on the blurred MS COCO dataset, outperforming previous GAN-based models and a recent similar work DALL-E.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2105.13290",
                "caption": "CogView: Mastering Text-to-Image Generation via Transformers",
                "credit": "M Ding, Z Yang, W Hong, W Zheng, C Zhou"
            }
        },
        {
            "start_date": {
                "year": 2021,
                "month": 5,
                "day": 20
            },
            "text": {
                "headline": "MedBERT",
                "text": "<p>Deep learning (DL)-based predictive models from electronic health records (EHRs) deliver impressive performance in many clinical tasks. Large training cohorts, however, are often required by these models to achieve high accuracy, hindering the adoption of DL-based models in scenarios with limited training data. Recently, bidirectional encoder representations from transformers (BERT) and related models have achieved tremendous successes in the natural language processing domain. The pretraining of BERT on a very large training corpus generates contextualized embeddings that can boost the performance of models trained on smaller datasets. Inspired by BERT, we propose Med-BERT, which adapts the BERT framework originally developed for the text domain to the structured EHR domain. Med-BERT is a contextualized embedding model pretrained on a structured EHR dataset of 28,490,650 patients. Fine-tuning experiments showed that Med-BERT substantially improves the prediction accuracy, boosting the area under the receiver operating characteristics curve (AUC) by 1.21–6.14% in two disease prediction tasks from two clinical databases. In particular, pretrained Med-BERT obtains promising performances on tasks with small fine-tuning training sets and can boost the AUC by more than 20% or obtain an AUC as high as a model trained on a training set ten times larger, compared with deep learning models without Med-BERT. We believe that Med-BERT will benefit disease prediction studies with small local training datasets, reduce data collection expenses, and accelerate the pace of artificial intelligence aided healthcare.</p>"
            },
            "media": {
                "url": "https://www.nature.com/articles/s41746-021-00455-y",
                "caption": "Med-BERT: pretrained contextualized embeddings on large-scale structured electronic health records for disease prediction",
                "credit": "Laila Rasmy, Yang Xiang, Ziqian Xie, Cui Tao, Degui Zhi"
            }
        },
        {
            "start_date": {
                "year": 2021,
                "month": 5,
                "day": 11
            },
            "text": {
                "headline": "ADM",
                "text": "<p>We show that diffusion models can achieve image sample quality superior to the current state-of-the-art generative models. We achieve this on unconditional image synthesis by finding a better architecture through a series of ablations. For conditional image synthesis, we further improve sample quality with classifier guidance: a simple, compute-efficient method for trading off diversity for fidelity using gradients from a classifier. We achieve an FID of 2.97 on ImageNet 128×128, 4.59 on ImageNet 256×256, and 7.72 on ImageNet 512×512, and we match BigGAN-deep even with as few as 25 forward passes per sample, all while maintaining better coverage of the distribution. Finally, we find that classifier guidance combines well with upsampling diffusion models, further improving FID to 3.94 on ImageNet 256×256 and 3.85 on ImageNet 512×512.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2105.05233",
                "caption": "Diffusion Models Beat GANs on Image Synthesis",
                "credit": "Prafulla Dhariwal, Alex Nichol"
            }
        },
        {
            "start_date": {
                "year": 2021,
                "month": 5,
                "day": 4
            },
            "text": {
                "headline": "ProtT5-XXL-BFD",
                "text": "<p>Computational biology and bioinformatics provide vast data gold-mines from protein sequences, ideal for Language Models taken from NLP. These LMs reach for new prediction frontiers at low inference costs. Here, we trained two auto-regressive models (Transformer-XL, XLNet) and four auto-encoder models (BERT, Albert, Electra, T5) on data from UniRef and BFD containing up to 393 billion amino acids. The LMs were trained on the Summit supercomputer using 5616 GPUs and TPU Pod up-to 1024 cores.\n\nDimensionality reduction revealed that the raw protein LM-embeddings from unlabeled data captured some biophysical features of protein sequences. We validated the advantage of using the embeddings as exclusive input for several subsequent tasks. The first was a per-residue prediction of protein secondary structure (3-state accuracy Q3=81%-87%); the second were per-protein predictions of protein sub-cellular localization (ten-state accuracy: Q10=81%) and membrane vs. water-soluble (2-state accuracy Q2=91%). For the per-residue predictions the transfer of the most informative embeddings (ProtT5) for the first time outperformed the state-of-the-art without using evolutionary information thereby bypassing expensive database searches. Taken together, the results implied that protein LMs learned some of the grammar of the language of life. To facilitate future work, we released our models at https://github.com/agemagician/ProtTrans.</p>"
            },
            "media": {
                "url": "https://www.biorxiv.org/content/10.1101/2020.07.12.199554v3 or \nhttps://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9477085",
                "caption": "ProtTrans:Towards Cracking the Language of Life's Code Through Self-Supervised Learning",
                "credit": "Ahmed Elnaggar, Michael Heinzinger,  Christian Dallago, Ghalia Rehawi, Yu Wang, Llion Jones, Tom Gibbs, Tamas Feher, Christoph Angerer, Martin Steinegger, Debsindhu Bhowmik, Burkhard Rost"
            }
        },
        {
            "start_date": {
                "year": 2021,
                "month": 5,
                "day": 4
            },
            "text": {
                "headline": "ProtT5-XXL",
                "text": "<p>Computational biology and bioinformatics provide vast data gold-mines from protein sequences, ideal for Language Models taken from NLP. These LMs reach for new prediction frontiers at low inference costs. Here, we trained two auto-regressive models (Transformer-XL, XLNet) and four auto-encoder models (BERT, Albert, Electra, T5) on data from UniRef and BFD containing up to 393 billion amino acids. The LMs were trained on the Summit supercomputer using 5616 GPUs and TPU Pod up-to 1024 cores. Dimensionality reduction revealed that the raw protein LM-embeddings from unlabeled data captured some biophysical features of protein sequences. We validated the advantage of using the embeddings as exclusive input for several subsequent tasks. The first was a per-residue prediction of protein secondary structure (3-state accuracy Q3=81%-87%); the second were per-protein predictions of protein sub-cellular localization (ten-state accuracy: Q10=81%) and membrane vs. water-soluble (2-state accuracy Q2=91%). For the per-residue predictions the transfer of the most informative embeddings (ProtT5) for the first time outperformed the state-of-the-art without using evolutionary information thereby bypassing expensive database searches. Taken together, the results implied that protein LMs learned some of the grammar of the language of life. To facilitate future work, we released our models at https://github.com/agemagician/ProtTrans.</p>"
            },
            "media": {
                "url": "https://www.biorxiv.org/content/10.1101/2020.07.12.199554v3 or \nhttps://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9477085",
                "caption": "ProtTrans: Towards Cracking the Language of Life’s Code Through Self-Supervised Learning",
                "credit": "A Elnaggar, M Heinzinger, C Dallago, G Rihawi"
            }
        },
        {
            "start_date": {
                "year": 2021,
                "month": 5,
                "day": 4
            },
            "text": {
                "headline": "ProtT5-XL-U50",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://www.biorxiv.org/content/10.1101/2020.07.12.199554v3.full.pdf",
                "caption": "",
                "credit": "Ahmed Elnaggar, Michael Heinzinger, View ORCID ProfileChristian Dallago, Ghalia Rehawi, Yu Wang, Llion Jones, Tom Gibbs, Tamas Feher, Christoph Angerer, Martin Steinegger, View ORCID ProfileDebsindhu Bhowmik, Burkhard Rost"
            }
        },
        {
            "start_date": {
                "year": 2021,
                "month": 5,
                "day": 4
            },
            "text": {
                "headline": "ProtBERT-BFD",
                "text": "<p>Computational biology and bioinformatics provide vast data gold-mines from protein sequences, ideal for Language Models taken from NLP. These LMs reach for new prediction frontiers at low inference costs. Here, we trained two auto-regressive models (Transformer-XL, XLNet) and four auto-encoder models (BERT, Albert, Electra, T5) on data from UniRef and BFD containing up to 393 billion amino acids. The LMs were trained on the Summit supercomputer using 5616 GPUs and TPU Pod up-to 1024 cores.\n\nDimensionality reduction revealed that the raw protein LM-embeddings from unlabeled data captured some biophysical features of protein sequences. We validated the advantage of using the embeddings as exclusive input for several subsequent tasks. The first was a per-residue prediction of protein secondary structure (3-state accuracy Q3=81%-87%); the second were per-protein predictions of protein sub-cellular localization (ten-state accuracy: Q10=81%) and membrane vs. water-soluble (2-state accuracy Q2=91%). For the per-residue predictions the transfer of the most informative embeddings (ProtT5) for the first time outperformed the state-of-the-art without using evolutionary information thereby bypassing expensive database searches. Taken together, the results implied that protein LMs learned some of the grammar of the language of life. To facilitate future work, we released our models at https://github.com/agemagician/ProtTrans.</p>"
            },
            "media": {
                "url": "https://www.biorxiv.org/content/10.1101/2020.07.12.199554v3 or \nhttps://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9477085",
                "caption": "ProtTrans:Towards Cracking the Language of Life's Code Through Self-Supervised Learning",
                "credit": "Ahmed Elnaggar, Michael Heinzinger,  Christian Dallago, Ghalia Rehawi, Yu Wang, Llion Jones, Tom Gibbs, Tamas Feher, Christoph Angerer, Martin Steinegger,  Debsindhu Bhowmik, Burkhard Rost"
            }
        },
        {
            "start_date": {
                "year": 2021,
                "month": 4,
                "day": 29
            },
            "text": {
                "headline": "ViT + DINO",
                "text": "<p>In this paper, we question if self-supervised learning provides new properties to Vision Transformer (ViT) that stand out compared to convolutional networks (convnets). Beyond the fact that adapting self-supervised methods to this architecture works particularly well, we make the following observations: first, self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs, nor with convnets. Second, these features are also excellent k-NN classifiers, reaching 78.3% top-1 on ImageNet with a small ViT. Our study also underlines the importance of momentum encoder, multi-crop training, and the use of small patches with ViTs. We implement our findings into a simple self-supervised method, called DINO, which we interpret as a form of self-distillation with no labels. We show the synergy between DINO and ViTs by achieving 80.1% top-1 on ImageNet in linear evaluation with ViT-Base.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2104.14294",
                "caption": "Emerging Properties in Self-Supervised Vision Transformers",
                "credit": "Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, Armand Joulin"
            }
        },
        {
            "start_date": {
                "year": 2021,
                "month": 4,
                "day": 19
            },
            "text": {
                "headline": "PLUG",
                "text": "<p>ExperienceAfter the Great Refining Model, the field of artificial intelligence is entering the era of \"Renging Model\".Since OpenAI released the ultra-large-scale pre-training language model GPT-3 in the English field last year, the training process of similar models in the Chinese field has received much attention.Today, the Aridal Moral Court released 27 billion parameters, 1TB + training data, the world's largest Chinese pre-training language model PLUG, and refreshed the historical record of the CLUE classification list with a score of 80.614.\n\n(autotranslate from Chinese)</p>"
            },
            "media": {
                "url": "https://mp.weixin.qq.com/s/DAQomIkDa52Sef-ruyH5qg",
                "caption": "",
                "credit": ""
            }
        },
        {
            "start_date": {
                "year": 2021,
                "month": 3,
                "day": 5
            },
            "text": {
                "headline": "M6-T",
                "text": "<p>Mixture-of-Experts (MoE) models can achieve promising results with outrageous large amount of parameters but constant computation cost, and thus it has become a trend in model scaling. Still it is a mystery how MoE layers bring quality gains by leveraging the parameters with sparse activation. In this work, we investigate several key factors in sparse expert models. We observe that load imbalance may not be a significant problem affecting model quality, contrary to the perspectives of recent studies, while the number of sparsely activated experts k and expert capacity C in top-k routing can significantly make a difference in this context. Furthermore, we take a step forward to propose a simple method called expert prototyping that splits experts into different prototypes and applies k top-1 routing. This strategy improves the model quality but maintains constant computational costs, and our further exploration on extremely large-scale models reflects that it is more effective in training larger models. We push the model scale to over 1 trillion parameters and implement it on solely 480 NVIDIA V100-32GB GPUs, in comparison with the recent SOTAs on 2048 TPU cores. The proposed giant model achieves substantial speedup in convergence over the same-size baseline.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2105.15082",
                "caption": "M6-T: Exploring Sparse Expert Models and Beyond",
                "credit": "An Yang, Junyang Lin, Rui Men, Chang Zhou, Le Jiang, Xianyan Jia, Ang Wang, Jie Zhang, Jiamang Wang, Yong Li, Di Zhang, Wei Lin, Lin Qu, Jingren Zhou, Hongxia Yang"
            }
        },
        {
            "start_date": {
                "year": 2021,
                "month": 3,
                "day": 5
            },
            "text": {
                "headline": "Generative BST",
                "text": "<p>Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, and displaying knowledge, empathy and personality appropriately, while maintaining a consistent persona. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2004.13637",
                "caption": "Recipes for building an open-domain chatbot",
                "credit": "Stephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson, Yinhan Liu, Jing Xu, Myle Ott, Kurt Shuster, Eric M. Smith, Y-Lan Boureau, Jason Weston"
            }
        },
        {
            "start_date": {
                "year": 2021,
                "month": 3,
                "day": 1
            },
            "text": {
                "headline": "Meta Pseudo Labels",
                "text": "<p>We present Meta Pseudo Labels, a semi-supervised learning method that achieves a new state-of-the-art top-1 accuracy of 90.2% on ImageNet, which is 1.6% better than the existing state-of-the-art. Like Pseudo Labels, Meta Pseudo Labels has a teacher network to generate pseudo labels on unlabeled data to teach a student network. However, unlike Pseudo Labels where the teacher is fixed, the teacher in Meta Pseudo Labels is constantly adapted by the feedback of the student's performance on the labeled dataset. As a result, the teacher generates better pseudo labels to teach the student. Our code will be available at this https URL.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2003.10580",
                "caption": "Meta pseudo labels",
                "credit": "Hieu Pham, Zihang Dai, Qizhe Xie, Minh-Thang Luong, and Quoc V. Le"
            }
        },
        {
            "start_date": {
                "year": 2021,
                "month": 2,
                "day": 24
            },
            "text": {
                "headline": "SRU++ Large",
                "text": "<p>Large language models have become increasingly difficult to train because of the growing computation time and cost. In this work, we present SRU++, a highly-efficient architecture that combines fast recurrence and attention for sequence modeling. SRU++ exhibits strong modeling capacity and training efficiency. On standard language modeling tasks such as Enwik8, Wiki-103 and Billion Word datasets, our model obtains better bits-per-character and perplexity while using 3x-10x less training cost compared to top-performing Transformer models. For instance, our model achieves a state-of-the-art result on the Enwik8 dataset using 1.6 days of training on an 8-GPU machine. We further demonstrate that SRU++ requires minimal attention for near state-of-the-art performance. Our results suggest jointly leveraging fast recurrence with little attention as a promising direction for accelerating model training and inference.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2102.12459",
                "caption": "When Attention Meets Fast Recurrence: Training Language Models with Reduced Compute",
                "credit": "Tao Lei"
            }
        },
        {
            "start_date": {
                "year": 2021,
                "month": 2,
                "day": 18
            },
            "text": {
                "headline": "Rational DQN Average",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://openreview.net/forum?id=gnRmI8TatHV",
                "caption": "Recurrent Rational Networks",
                "credit": "Q Delfosse, P Schramowski, A Molina"
            }
        },
        {
            "start_date": {
                "year": 2021,
                "month": 2,
                "day": 13
            },
            "text": {
                "headline": "MSA Transformer",
                "text": "<p>Unsupervised protein language models trained across millions of diverse sequences learn structure and function of proteins. Protein language models studied to date have been trained to perform inference from individual sequences. The longstanding approach in computational biology has been to make inferences from a family of evolutionarily related sequences by fitting a model to each family independently. In this work we combine the two paradigms. We introduce a protein language model which takes as input a set of sequences in the form of a multiple sequence alignment. The model interleaves row and column attention across the input sequences and is trained with a variant of the masked language modeling objective across many protein families. The performance of the model surpasses current state-of-the-art unsupervised structure learning methods by a wide margin, with far greater parameter efficiency than prior state-of-the-art protein language models. </p>"
            },
            "media": {
                "url": "https://proceedings.mlr.press/v139/rao21a/rao21a.pdf",
                "caption": "MSA Transformer",
                "credit": "Roshan Rao, Jason Liu, Robert Verkuil, Joshua Meier, John F. Canny, Pieter Abbeel, Tom Sercu, Alexander Rives"
            }
        },
        {
            "start_date": {
                "year": 2021,
                "month": 2,
                "day": 9
            },
            "text": {
                "headline": "top-down frozen classifier",
                "text": "<p>Although the lower layers of a deep neural network learn features which are transferable across datasets, these layers are not transferable within the same dataset. That is, in general, freezing the trained feature extractor (the lower layers) and retraining the classifier (the upper layers) on the same dataset leads to worse performance. In this paper, for the first time, we show that the frozen classifier is transferable within the same dataset. We develop a novel top-down training method which can be viewed as an algorithm for searching for high-quality classifiers. We tested this method on automatic speech recognition (ASR) tasks and language modelling tasks. The proposed method consistently improves recurrent neural network ASR models on Wall Street Journal, self-attention ASR models on Switchboard, and AWD-LSTM language models on WikiText-2.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2102.04697",
                "caption": "Train your classifier first: Cascade Neural Networks Training from upper layers to lower layers",
                "credit": "Shucong Zhang, Cong-Thanh Do, Rama Doddipatla, Erfan Loweimi, Peter Bell, Steve Renals"
            }
        },
        {
            "start_date": {
                "year": 2021,
                "month": 1,
                "day": 15
            },
            "text": {
                "headline": "DeiT-B",
                "text": "<p>Recently, neural networks purely based on attention were shown to address image understanding tasks such as image classification. However, these visual transformers are pre-trained with hundreds of millions of images using an expensive infrastructure, thereby limiting their adoption.\nIn this work, we produce a competitive convolution-free transformer by training on Imagenet only. We train them on a single computer in less than 3 days. Our reference vision transformer (86M parameters) achieves top-1 accuracy of 83.1% (single-crop evaluation) on ImageNet with no external data.\nMore importantly, we introduce a teacher-student strategy specific to transformers. It relies on a distillation token ensuring that the student learns from the teacher through attention. We show the interest of this token-based distillation, especially when using a convnet as a teacher. This leads us to report results competitive with convnets for both Imagenet (where we obtain up to 85.2% accuracy) and when transferring to other tasks. We share our code and models.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2012.12877",
                "caption": "Training data-efficient image transformers & distillation through attention",
                "credit": "Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, Hervé Jégou"
            }
        },
        {
            "start_date": {
                "year": 2021,
                "month": 1,
                "day": 11
            },
            "text": {
                "headline": "Switch",
                "text": "<p>In deep learning, models typically reuse the same parameters for all inputs. Mixture of Experts (MoE) defies this and instead selects different parameters for each incoming example. The result is a sparsely-activated model -- with outrageous numbers of parameters -- but a constant computational cost. However, despite several notable successes of MoE, widespread adoption has been hindered by complexity, communication costs and training instability -- we address these with the Switch Transformer. We simplify the MoE routing algorithm and design intuitive improved models with reduced communication and computational costs. Our proposed training techniques help wrangle the instabilities and we show large sparse models may be trained, for the first time, with lower precision (bfloat16) formats. We design models based off T5-Base and T5-Large to obtain up to 7x increases in pre-training speed with the same computational resources. These improvements extend into multilingual settings where we measure gains over the mT5-Base version across all 101 languages. Finally, we advance the current scale of language models by pre-training up to trillion parameter models on the \"Colossal Clean Crawled Corpus\" and achieve a 4x speedup over the T5-XXL model.\n</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2101.03961",
                "caption": "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity",
                "credit": "William Fedus, Barret Zoph, Noam Shazeer"
            }
        },
        {
            "start_date": {
                "year": 2021,
                "month": 1,
                "day": 10
            },
            "text": {
                "headline": "BigSSL",
                "text": "<p>We summarize the results of a host of efforts using giant automatic speech recognition (ASR) models pre-trained using large, diverse unlabeled datasets containing approximately a million hours of audio. We find that the combination of pre-training, self-training and scaling up model size greatly increases data efficiency, even for extremely large tasks with tens of thousands of hours of labeled data. In particular, on an ASR task with 34k hours of labeled data, by fine-tuning an 8 billion parameter pre-trained Conformer model we can match state-of-the-art (SoTA) performance with only 3% of the training data and significantly improve SoTA with the full training set. We also report on the universal benefits gained from using big pre-trained and self-trained models for a large set of downstream tasks that cover a wide range of speech domains and span multiple orders of magnitudes of dataset sizes, including obtaining SoTA performance on many public benchmarks. In addition, we utilize the learned representation of pre-trained networks to achieve SoTA results on non-ASR tasks.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2109.13226",
                "caption": "BigSSL: Exploring the Frontier of Large-Scale Semi-Supervised Learning for Automatic Speech Recognition",
                "credit": "Yu Zhang,  Daniel S. Park, Wei Han,James Qin, Anmol Gulati, Joel Shor, Aren Jansen, Yuanzhong Xu, Yanping Huang, Shibo Wang, Zongwei Zhou, Bo Li, Min Ma, William Chan, Jiahui Yu, Yongqiang Wang, Liangliang Cao, Khe Chai Sim, Bhuvana Ramabhadran, Tara N. Sainath, Françoise Beaufays, Zhifeng Chen, Quoc V. Le, Chung-Cheng Chiu, Ruoming Pang and Yonghui Wu"
            }
        },
        {
            "start_date": {
                "year": 2021,
                "month": 1,
                "day": 5
            },
            "text": {
                "headline": "DALL-E",
                "text": "<p>Text-to-image generation has traditionally focused on finding better modeling assumptions for training on a fixed dataset. These assumptions might involve complex architectures, auxiliary losses, or side information such as object part labels or segmentation masks supplied during training. We describe a simple approach for this task based on a transformer that autoregressively models the text and image tokens as a single stream of data. With sufficient data and scale, our approach is competitive with previous domain-specific models when evaluated in a zero-shot fashion.</p>"
            },
            "media": {
                "url": "https://openai.com/blog/dall-e/\n\nhttps://arxiv.org/abs/2102.12092",
                "caption": "Zero-Shot Text-to-Image Generation",
                "credit": "Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, Ilya Sutskever"
            }
        },
        {
            "start_date": {
                "year": 2021,
                "month": 1,
                "day": 5
            },
            "text": {
                "headline": "CLIP (ViT L/14@336px)",
                "text": "<p>State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at this https URL.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2103.00020",
                "caption": "Learning Transferable Visual Models From Natural Language Supervision",
                "credit": "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever"
            }
        },
        {
            "start_date": {
                "year": 2021,
                "month": 1,
                "day": 5
            },
            "text": {
                "headline": "CLIP (ResNet-50)",
                "text": "<p>State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at this https URL.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2103.00020",
                "caption": "Learning Transferable Visual Models From Natural Language Supervision",
                "credit": "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever"
            }
        },
        {
            "start_date": {
                "year": 2020,
                "month": 12,
                "day": 31
            },
            "text": {
                "headline": "ERNIE-Doc (247M)",
                "text": "<p>Transformers are not suited for processing long documents, due to their quadratically increasing memory and time consumption. Simply truncating a long document or applying the sparse attention mechanism will incur the context fragmentation problem or lead to an inferior modeling capability against comparable model sizes. In this paper, we propose ERNIE-Doc, a document-level language pretraining model based on Recurrence Transformers. Two well-designed techniques, namely the retrospective feed mechanism and the enhanced recurrence mechanism, enable ERNIE-Doc, which has a much longer effective context length, to capture the contextual information of a complete document. We pretrain ERNIE-Doc to explicitly learn the relationships among segments with an additional document-aware segment-reordering objective. Various experiments were conducted on both English and Chinese document-level tasks. ERNIE-Doc improved the state-of-the-art language modeling result of perplexity to 16.8 on WikiText-103. Moreover, it outperformed competitive pretraining models by a large margin on most language understanding tasks, such as text classification and question answering.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2012.15688",
                "caption": "ERNIE-Doc: A Retrospective Long-Document Modeling Transformer",
                "credit": "Siyu Ding, Junyuan Shang, Shuohuan Wang, Yu Sun, Hao Tian, Hua Wu, Haifeng Wang"
            }
        },
        {
            "start_date": {
                "year": 2020,
                "month": 12,
                "day": 25
            },
            "text": {
                "headline": "CT-MoS (WT2)",
                "text": "<p>Temperature scaling has been widely used as an effective approach to control the smoothness of a distribution, which helps the model performance in various tasks. Current practices to apply temperature scaling assume either a fixed, or a manually-crafted dynamically changing schedule. However, our studies indicate that the individual optimal trajectory for each class can change with the context. To this end, we propose contextual temperature, a generalized approach that learns an optimal temperature trajectory for each vocabulary over the context. Experimental results confirm that the proposed method significantly improves state-of-the-art language models, achieving a perplexity of 55.31 and 62.89 on the test set of Penn Treebank and WikiText-2, respectively. In-depth analyses show that the behaviour of the learned temperature schedules varies dramatically by vocabulary, and that the optimal schedules help in controlling the uncertainties. These evidences further justify the need for the proposed method and its advantages over fixed temperature schedules.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2012.13575",
                "caption": "Contextual Temperature for Language Modeling",
                "credit": "Pei-Hsin Wang, Sheng-Iou Hsieh, Shih-Chieh Chang, Yu-Ting Chen, Jia-Yu Pan, Wei Wei, Da-Chang Juan"
            }
        },
        {
            "start_date": {
                "year": 2020,
                "month": 12,
                "day": 23
            },
            "text": {
                "headline": "DensePhrases",
                "text": "<p>Open-domain question answering can be reformulated as a phrase retrieval problem, without the need for processing documents on-demand during inference (Seo et al., 2019). However, current phrase retrieval models heavily depend on sparse representations and still underperform retriever-reader approaches. In this work, we show for the first time that we can learn dense representations of phrases alone that achieve much stronger performance in open-domain QA. We present an effective method to learn phrase representations from the supervision of reading comprehension tasks, coupled with novel negative sampling methods. We also propose a query-side fine-tuning strategy, which can support transfer learning and reduce the discrepancy between training and inference. On five popular open-domain QA datasets, our model DensePhrases improves over previous phrase retrieval models by 15%-25% absolute accuracy and matches the performance of state-of-the-art retriever-reader models. Our model is easy to parallelize due to pure dense representations and processes more than 10 questions per second on CPUs. Finally, we directly use our pre-indexed dense phrase representations for two slot filling tasks, showing the promise of utilizing DensePhrases as a dense knowledge base for downstream tasks. </p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2012.12624v3",
                "caption": "Learning Dense Representations of Phrases at Scale",
                "credit": "Jinhyuk Lee, Mujeen Sung, Jaewoo Kang, Danqi Chen"
            }
        },
        {
            "start_date": {
                "year": 2020,
                "month": 12,
                "day": 17
            },
            "text": {
                "headline": "VQGAN + CLIP",
                "text": "<p>Designed to learn long-range interactions on sequential data, transformers continue to show state-of-the-art results on a wide variety of tasks. In contrast to CNNs, they contain no inductive bias that prioritizes local interactions. This makes them expressive, but also computationally infeasible for long sequences, such as high-resolution images. We demonstrate how combining the effectiveness of the inductive bias of CNNs with the expressivity of transformers enables them to model and thereby synthesize high-resolution images. We show how to (i) use CNNs to learn a context-rich vocabulary of image constituents, and in turn (ii) utilize transformers to efficiently model their composition within high-resolution images. Our approach is readily applied to conditional synthesis tasks, where both non-spatial information, such as object classes, and spatial information, such as segmentations, can control the generated image. In particular, we present the first results on semantically-guided synthesis of megapixel images with transformers and obtain the state of the art among autoregressive models on class-conditional ImageNet. Code and pretrained models can be found at this https URL .</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2012.09841",
                "caption": "Taming Transformers for High-Resolution Image Synthesis",
                "credit": "Patrick Esser, Robin Rombach, Björn Ommer"
            }
        },
        {
            "start_date": {
                "year": 2020,
                "month": 12,
                "day": 15
            },
            "text": {
                "headline": "ESM1b",
                "text": "<p>In the field of artificial intelligence, a combination of scale in data and model capacity enabled by unsupervised learning has led to major advances in representation learning and statistical generation. In the life sciences, the anticipated growth of sequencing promises unprecedented data on natural sequence diversity. Protein language modeling at the scale of evolution is a logical step toward predictive and generative artificial intelligence for biology. To this end, we use unsupervised learning to train a deep contextual language model on 86 billion amino acids across 250 million protein sequences spanning evolutionary diversity. The resulting model contains information about biological properties in its representations. The representations are learned from sequence data alone. The learned representation space has a multiscale organization\nreflecting structure from the level of biochemical properties of amino acids to remote homology of proteins. Information about secondary and tertiary structure is encoded in the representations and can be identified by linear projections. Representation learning\nproduces features that generalize across a range of applications, enabling state-of-the-art supervised prediction of mutational effect and secondary structure and improving state-of-the-art features for long-range contact prediction.</p>"
            },
            "media": {
                "url": "https://www.pnas.org/doi/abs/10.1073/pnas.2016239118",
                "caption": "Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences",
                "credit": "Alexander Rives, Joshua Meier, Tom Sercu, Siddharth Goyal, Zeming Lin, Jason Liu, Demi Guo, Myle Ott, C. Lawrence Zitnick, Jerry Ma, and Rob Fergus"
            }
        },
        {
            "start_date": {
                "year": 2020,
                "month": 12,
                "day": 1
            },
            "text": {
                "headline": "CPM-Large",
                "text": "<p>Pre-trained Language Models (PLMs) have proven to be beneficial for various downstream NLP tasks. Recently, GPT-3, with 175 billion parameters and 570GB training data, drew a lot of attention due to the capacity of few-shot (even zero-shot) learning. However, applying GPT-3 to address Chinese NLP tasks is still challenging, as the training corpus of GPT-3 is primarily English, and the parameters are not publicly available. In this technical report, we release the Chinese Pre-trained Language Model (CPM) with generative pre-training on large-scale Chinese training data. To the best of our knowledge, CPM, with 2.6 billion parameters and 100GB Chinese training data, is the largest Chinese pre-trained language model, which could facilitate several downstream Chinese NLP tasks, such as conversation, essay generation, cloze test, and language understanding. Extensive experiments demonstrate that CPM achieves strong performance on many NLP tasks in the settings of few-shot (even zero-shot) learning. The code and parameters are available at this https URL.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2012.00413",
                "caption": "CPM: A Large-scale Generative Chinese Pre-trained Language Model",
                "credit": "Z Zhang, X Han, H Zhou, P Ke, Y Gu, D Ye, Y Qin, Y Su"
            }
        },
        {
            "start_date": {
                "year": 2020,
                "month": 11,
                "day": 30
            },
            "text": {
                "headline": "AlphaFold 2",
                "text": "<p>Proteins are essential to life, and understanding their structure can facilitate a mechanistic understanding of their function. Through an enormous experimental effort, the structures of around 100,000 unique proteins have been determined, but this represents a small fraction of the billions of known protein sequences. Structural coverage is bottlenecked by the months to years of painstaking effort required to determine a single protein structure. Accurate computational approaches are needed to address this gap and to enable large-scale structural bioinformatics. Predicting the three-dimensional structure that a protein will adopt based solely on its amino acid sequence—the structure prediction component of the ‘protein folding problem’—has been an important open research problem for more than 50 years. Despite recent progress, existing methods fall far short of atomic accuracy, especially when no homologous structure is available. Here we provide the first computational method that can regularly predict protein structures with atomic accuracy even in cases in which no similar structure is known. We validated an entirely redesigned version of our neural network-based model, AlphaFold, in the challenging 14th Critical Assessment of protein Structure Prediction (CASP14), demonstrating accuracy competitive with experimental structures in a majority of cases and greatly outperforming other methods. Underpinning the latest version of AlphaFold is a novel machine learning approach that incorporates physical and biological knowledge about protein structure, leveraging multi-sequence alignments, into the design of the deep learning algorithm.</p>"
            },
            "media": {
                "url": "https://www.nature.com/articles/s41586-021-03819-2",
                "caption": "Highly accurate protein structure prediction with AlphaFold",
                "credit": "John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Kathryn Tunyasuvunakool, Olaf Ronneberger, Russ Bates, Augustin Žídek, Alex Bridgland, Clemens Meyer, Simon A A Kohl, Anna Potapenko, Andrew J Ballard, Andrew Cowie, Bernardino Romera-Paredes, Stanislav Nikolov, Rishub Jain, Jonas Adler, Trevor Back, Stig Petersen, David Reiman, Martin Steinegger, Michalina Pacholska, David Silver, Oriol Vinyals, Andrew W Senior, Koray Kavukcuoglu, Pushmeet Kohli, Demis Hassabis."
            }
        },
        {
            "start_date": {
                "year": 2020,
                "month": 11,
                "day": 23
            },
            "text": {
                "headline": "KEPLER",
                "text": "<p>Pre-trained language representation models (PLMs) cannot well capture factual knowledge from text. In contrast, knowledge embedding (KE) methods can effectively represent the relational facts in knowledge graphs (KGs) with informative entity embeddings, but conventional KE models cannot take full advantage of the abundant textual information. In this paper, we propose a unified model for Knowledge Embedding and Pre-trained LanguagE Representation (KEPLER), which can not only better integrate factual knowledge into PLMs but also produce effective text-enhanced KE with the strong PLMs. In KEPLER, we encode textual entity descriptions with a PLM as their embeddings, and then jointly optimize the KE and language modeling objectives. Experimental results show that KEPLER achieves state-of-the-art performances on various NLP tasks, and also works remarkably well as an inductive KE model on KG link prediction. Furthermore, for pre-training and evaluating KEPLER, we construct Wikidata5M, a large-scale KG dataset with aligned entity descriptions, and benchmark state-of-the-art KE methods on it. It shall serve as a new KE benchmark and facilitate the research on large KG, inductive KE, and KG with text. The source code can be obtained from this https URL.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1911.06136",
                "caption": "KEPLER: A Unified Model for Knowledge Embedding and Pre- trained Language Representation.",
                "credit": "Xiaozhi Wang, Tianyu Gao, Zhaocheng Zhu, Zhiyuan Liu, Juanzi Li, and Jian Tang."
            }
        },
        {
            "start_date": {
                "year": 2020,
                "month": 10,
                "day": 26
            },
            "text": {
                "headline": "SimCLRv2",
                "text": "<p>One paradigm for learning from few labeled examples while making best use of a large amount of unlabeled data is unsupervised pretraining followed by supervised fine-tuning. Although this paradigm uses unlabeled data in a task-agnostic way, in contrast to common approaches to semi-supervised learning for computer vision, we show that it is surprisingly effective for semi-supervised learning on ImageNet. A key ingredient of our approach is the use of big (deep and wide) networks during pretraining and fine-tuning. We find that, the fewer the labels, the more this approach (task-agnostic use of unlabeled data) benefits from a bigger network. After fine-tuning, the big network can be further improved and distilled into a much smaller one with little loss in classification accuracy by using the unlabeled examples for a second time, but in a task-specific way. The proposed semi-supervised learning algorithm can be summarized in three steps: unsupervised pretraining of a big ResNet model using SimCLRv2, supervised fine-tuning on a few labeled examples, and distillation with unlabeled examples for refining and transferring the task-specific knowledge. This procedure achieves 73.9% ImageNet top-1 accuracy with just 1% of the labels (≤13 labeled images per class) using ResNet-50, a 10× improvement in label efficiency over the previous state-of-the-art. With 10% of labels, ResNet-50 trained with our method achieves 77.5% top-1 accuracy, outperforming standard supervised training with all of the labels.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2006.10029",
                "caption": "Big self- supervised models are strong semi-supervised learners.",
                "credit": "Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, and Geoffrey Hinton"
            }
        },
        {
            "start_date": {
                "year": 2020,
                "month": 10,
                "day": 22
            },
            "text": {
                "headline": "wave2vec 2.0 LARGE",
                "text": "<p>We show for the first time that learning powerful representations from speech audio alone followed by fine-tuning on transcribed speech can outperform the best semi-supervised methods while being conceptually simpler. wav2vec 2.0 masks the speech input in the latent space and solves a contrastive task defined over a quantization of the latent representations which are jointly learned. Experiments using all labeled data of Librispeech achieve 1.8/3.3 WER on the clean/other test sets. When lowering the amount of labeled data to one hour, wav2vec 2.0 outperforms the previous state of the art on the 100 hour subset while using 100 times less labeled data. Using just ten minutes of labeled data and pre-training on 53k hours of unlabeled data still achieves 4.8/8.2 WER. This demonstrates the feasibility of speech recognition with limited amounts of labeled data.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2006.11477",
                "caption": "wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations",
                "credit": "Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael Auli"
            }
        },
        {
            "start_date": {
                "year": 2020,
                "month": 10,
                "day": 22
            },
            "text": {
                "headline": "ViT-Huge/14",
                "text": "<p>While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2010.11929",
                "caption": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
                "credit": "Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby"
            }
        },
        {
            "start_date": {
                "year": 2020,
                "month": 10,
                "day": 22
            },
            "text": {
                "headline": "ViT-Base/32",
                "text": "<p>While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2010.11929",
                "caption": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
                "credit": "Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby"
            }
        },
        {
            "start_date": {
                "year": 2020,
                "month": 10,
                "day": 21
            },
            "text": {
                "headline": "German ELECTRA Large",
                "text": "<p>In this work we present the experiments which lead to the creation of our BERT and ELECTRA based German language models, GBERT and GELECTRA. By varying the input training data, model size, and the presence of Whole Word Masking (WWM) we were able to attain SoTA performance across a set of document classification and named entity recognition (NER) tasks for both models of base and large size. We adopt an evaluation driven approach in training these models and our results indicate that both adding more data and utilizing WWM improve model performance. By benchmarking against existing German models, we show that these models are the best German models to date. Our trained models will be made publicly available to the research community. </p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2010.10906",
                "caption": "German's Next Language Model",
                "credit": "Branden Chan, Stefan Schweter, Timo Möller"
            }
        },
        {
            "start_date": {
                "year": 2020,
                "month": 10,
                "day": 21
            },
            "text": {
                "headline": "GBERT-Large",
                "text": "<p>In this work we present the experiments which lead to the creation of our BERT and ELECTRA based German language models, GBERT and GELECTRA. By varying the input training data, model size, and the presence of Whole Word Masking (WWM) we were able to attain SoTA performance across a set of document classification and named entity recognition (NER) tasks for both models of base and large size. We adopt an evaluation driven approach in training these models and our results indicate that both adding more data and utilizing WWM improve model performance. By benchmarking against existing German models, we show that these models are the best German models to date. Our trained models will be made publicly available to the research community. </p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2010.10906",
                "caption": "German's Next Language Model",
                "credit": "Branden Chan, Stefan Schweter, Timo Möller"
            }
        },
        {
            "start_date": {
                "year": 2020,
                "month": 10,
                "day": 20
            },
            "text": {
                "headline": "mT5-XXL",
                "text": "<p>The recent “Text-to-Text Transfer Transformer” (T5) leveraged a unified text-to-text format and scale to attain state-of-the-art results on a wide variety of English-language NLP tasks. In this paper, we introduce mT5, a multilingual variant of T5 that was pre-trained on a new Common Crawl-based dataset covering 101 languages. We detail the design and modified training of mT5 and demonstrate its state-of-the-art performance on many multilingual benchmarks. We also describe a simple technique to prevent “accidental translation” in the zero-shot setting, where a generative model chooses to (partially) translate its prediction into the wrong language. All of the code and model checkpoints used in this work are publicly available.</p>"
            },
            "media": {
                "url": "https://aclanthology.org/2021.naacl-main.41/",
                "caption": "mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer",
                "credit": "Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, Colin Raffel"
            }
        },
        {
            "start_date": {
                "year": 2020,
                "month": 10,
                "day": 20
            },
            "text": {
                "headline": "Conformer + Wav2vec 2.0 + Noisy Student",
                "text": "<p>We employ a combination of recent developments in semi-supervised learning for automatic speech recognition to obtain state-of-the-art results on LibriSpeech utilizing the unlabeled audio of the Libri-Light dataset. More precisely, we carry out noisy student training with SpecAugment using giant Conformer models pre-trained using wav2vec 2.0 pre-training. By doing so, we are able to achieve word-error-rates (WERs) 1.4%/2.6% on the LibriSpeech test/test-other sets against the current state-of-the-art WERs 1.7%/3.3%.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2010.10504v2",
                "caption": "Pushing the Limits of Semi-Supervised Learning for Automatic Speech Recognition",
                "credit": "Yu Zhang, James Qin, Daniel S. Park, Wei Han, Chung-Cheng Chiu, Ruoming Pang, Quoc V. Le, Yonghui Wu"
            }
        },
        {
            "start_date": {
                "year": 2020,
                "month": 10,
                "day": 2
            },
            "text": {
                "headline": "LUKE",
                "text": "<p>Entity representations are useful in natural language tasks involving entities. In this paper, we propose new pretrained contextualized representations of words and entities based on the bidirectional transformer. The proposed model treats words and entities in a given text as independent tokens, and outputs contextualized representations of them. Our model is trained using a new pretraining task based on the masked language model of BERT. The task involves predicting randomly masked words and entities in a large entity-annotated corpus retrieved from Wikipedia. We also propose an entity-aware self-attention mechanism that is an extension of the self-attention mechanism of the transformer, and considers the types of tokens (words or entities) when computing attention scores. The proposed model achieves impressive empirical performance on a wide range of entity-related tasks. In particular, it obtains state-of-the-art results on five well-known datasets: Open Entity (entity typing), TACRED (relation classification), CoNLL-2003 (named entity recognition), ReCoRD (cloze-style question answering), and SQuAD 1.1 (extractive question answering). Our source code and pretrained representations are available at this https://github.com/studio-ousia/luke</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2010.01057v1",
                "caption": "LUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention",
                "credit": "Ikuya Yamada, Akari Asai, Hiroyuki Shindo, Hideaki Takeda, Yuji Matsumoto"
            }
        },
        {
            "start_date": {
                "year": 2020,
                "month": 9,
                "day": 1
            },
            "text": {
                "headline": "ProBERTa",
                "text": "<p>The scientific community is rapidly generating protein sequence information, but only a fraction of these proteins can be experimentally characterized. While promising deep learning approaches for protein prediction tasks have emerged, they have computational limitations or are designed to solve a specific task. We present a Transformer neural network that pre-trains task-agnostic sequence representations. This model is fine-tuned to solve two different protein prediction tasks: protein family classification and protein interaction prediction. Our method is comparable to existing state-of-the-art approaches for protein family classification while being much more general than other architectures. Further, our method outperforms all other approaches for protein interaction prediction. These results offer a promising framework for fine-tuning the pre-trained sequence representations for other protein prediction tasks.</p>"
            },
            "media": {
                "url": "https://dl.acm.org/doi/10.1145/3388440.3412467",
                "caption": "Transforming the Language of Life: Transformer Neural Networks for Protein Prediction Tasks",
                "credit": "Ananthan Nambiar, Maeve Heflin, Simon Liu, Sergei Maslov, Mark Hopkins, Anna Ritz"
            }
        },
        {
            "start_date": {
                "year": 2020,
                "month": 8,
                "day": 6
            },
            "text": {
                "headline": "ERNIE-GEN (large)",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2001.11314",
                "caption": "ERNIE-GEN: An Enhanced Multi-Flow Pre-training and Fine-tuning Framework for Natural Language Generation",
                "credit": "Dongling Xiao, Han Zhang, Yukun Li, Yu Sun, Hao Tian, Hua Wu, Haifeng Wang"
            }
        },
        {
            "start_date": {
                "year": 2020,
                "month": 8,
                "day": 3
            },
            "text": {
                "headline": "DeLighT",
                "text": "<p>We introduce a deep and light-weight transformer, DeLighT, that delivers similar or better performance than standard transformer-based models with significantly fewer parameters. DeLighT more efficiently allocates parameters both (1) within each Transformer block using the DeLighT transformation, a deep and light-weight transformation, and (2) across blocks using block-wise scaling, which allows for shallower and narrower DeLighT blocks near the input and wider and deeper DeLighT blocks near the output. Overall, DeLighT networks are 2.5 to 4 times deeper than standard transformer models and yet have fewer parameters and operations. Experiments on benchmark machine translation and language modeling tasks show that DeLighT matches or improves the performance of baseline Transformers with 2 to 3 times fewer parameters on average. Our source code is available at: \\url{this https URL}</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2008.00623",
                "caption": "DeLighT: Deep and Light-weight Transformer",
                "credit": "Sachin Mehta, Marjan Ghazvininejad, Srinivasan Iyer, Luke Zettlemoyer, Hannaneh Hajishirzi"
            }
        },
        {
            "start_date": {
                "year": 2020,
                "month": 7,
                "day": 27
            },
            "text": {
                "headline": "EfficientDet",
                "text": "<p>Model efficiency has become increasingly important in computer vision. In this paper, we systematically study neural network architecture design choices for object detection and propose several key optimizations to improve efficiency. First, we propose a weighted bi-directional feature pyramid network (BiFPN), which allows easy and fast multiscale feature fusion; Second, we propose a compound scaling method that uniformly scales the resolution, depth, and width for all backbone, feature network, and box/class prediction networks at the same time. Based on these optimizations and better backbones, we have developed a new family of object detectors, called EfficientDet, which consistently achieve much better efficiency than prior art across a wide spectrum of resource constraints. In particular, with single model and single-scale, our EfficientDet-D7 achieves state-of-the-art 55.1 AP on COCO test-dev with 77M parameters and 410B FLOPs, being 4x - 9x smaller and using 13x - 42x fewer FLOPs than previous detectors. Code is available at this https URL.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1911.09070",
                "caption": "EfficientDet: Scalable and Efficient Object Detection",
                "credit": "Mingxing Tan, Ruoming Pang, Quoc V. Le"
            }
        },
        {
            "start_date": {
                "year": 2020,
                "month": 7,
                "day": 16
            },
            "text": {
                "headline": "Hopfield Networks (2020)",
                "text": "<p>We introduce a modern Hopfield network with continuous states and a corresponding update rule. The new Hopfield network can store exponentially (with the dimension of the associative space) many patterns, retrieves the pattern with one update, and has exponentially small retrieval errors. It has three types of energy minima (fixed points of the update): (1) global fixed point averaging over all patterns, (2) metastable states averaging over a subset of patterns, and (3) fixed points which store a single pattern. The new update rule is equivalent to the attention mechanism used in transformers. This equivalence enables a characterization of the heads of transformer models. These heads perform in the first layers preferably global averaging and in higher layers partial averaging via metastable states. The new modern Hopfield network can be integrated into deep learning architectures as layers to allow the storage of and access to raw input data, intermediate results, or learned prototypes. These Hopfield layers enable new ways of deep learning, beyond fully-connected, convolutional, or recurrent networks, and provide pooling, memory, association, and attention mechanisms. We demonstrate the broad applicability of the Hopfield layers across various domains. Hopfield layers improved state-of-the-art on three out of four considered multiple instance learning problems as well as on immune repertoire classification with several hundreds of thousands of instances. On the UCI benchmark collections of small classification tasks, where deep learning methods typically struggle, Hopfield layers yielded a new state-of-the-art when compared to different machine learning methods. Finally, Hopfield layers achieved state-of-the-art on two drug design datasets. The implementation is available at: this https URL</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2008.02217",
                "caption": "Hopfield Networks is All You Need",
                "credit": "Hubert Ramsauer, Bernhard Schäfl, Johannes Lehner, Philipp Seidl, Michael Widrich, Thomas Adler, Lukas Gruber, Markus Holzleitner, Milena Pavlović, Geir Kjetil Sandve, Victor Greiff, David Kreil, Michael Kopp, Günter Klambauer, Johannes Brandstetter, Sepp Hochreiter"
            }
        },
        {
            "start_date": {
                "year": 2020,
                "month": 7,
                "day": 2
            },
            "text": {
                "headline": "SemExp",
                "text": "<p>This work studies the problem of object goal navigation which involves navigating to an instance of the given object category in unseen environments. End-to-end learning-based navigation methods struggle at this task as they are ineffective at exploration and long-term planning. We propose a modular system called, ‘GoalOriented Semantic Exploration’ which builds an episodic semantic map and uses it to explore the environment efficiently based on the goal object category. Empirical results in visually realistic simulation environments show that the proposed model outperforms a wide range of baselines including end-to-end learning-based methods as well as modular map-based methods and led to the winning entry of the CVPR2020 Habitat ObjectNav Challenge. Ablation analysis indicates that the proposed model learns semantic priors of the relative arrangement of objects in a scene, and uses them to explore efficiently. Domain-agnostic module design allows us to transfer our model to a mobile robot platform and achieve similar performance for object goal navigation in the real-world.</p>"
            },
            "media": {
                "url": "https://proceedings.neurips.cc/paper/2020/file/2c75cf2681788adaca63aa95ae028b22-Paper.pdf",
                "caption": "Object Goal Navigation using Goal-Oriented Semantic Exploration",
                "credit": "Devendra Singh Chaplot, Dhiraj Gandhi, Abhinav Gupta, Ruslan Salakhutdinov"
            }
        },
        {
            "start_date": {
                "year": 2020,
                "month": 6,
                "day": 30
            },
            "text": {
                "headline": "GShard (dense)",
                "text": "<p>Neural network scaling has been critical for improving the model quality in many real-world machine learning applications with vast amounts of training data and compute. Although this trend of scaling is affirmed to be a sure-fire approach for better model quality, there are challenges on the path such as the computation cost, ease of programming, and efficient implementation on parallel devices. GShard is a module composed of a set of lightweight annotation APIs and an extension to the XLA compiler. It provides an elegant way to express a wide range of parallel computation patterns with minimal changes to the existing model code. GShard enabled us to scale up multilingual neural machine translation Transformer model with Sparsely-Gated Mixture-of-Experts beyond 600 billion parameters using automatic sharding. We demonstrate that such a giant model can efficiently be trained on 2048 TPU v3 accelerators in 4 days to achieve far superior quality for translation from 100 languages to English compared to the prior art.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2006.16668",
                "caption": "GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding",
                "credit": "Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, Zhifeng Chen"
            }
        },
        {
            "start_date": {
                "year": 2020,
                "month": 6,
                "day": 17
            },
            "text": {
                "headline": "iGPT-XL",
                "text": "<p>Inspired by progress in unsupervised representation learning for natural language, we examine whether similar models can learn useful representations for images. We train a sequence Transformer to auto-regressively predict pixels, without incorporating knowledge of the 2D input structure. Despite training on low-resolution ImageNet without labels, we find that a GPT-2 scale model learns strong image representations as measured by linear probing, fine-tuning, and low-data classification. On CIFAR-10, we achieve 96.3% accuracy with a linear probe, outperforming a supervised Wide ResNet, and 99.0% accuracy with full finetuning, matching the top supervised pre-trained models. An even larger model trained on a mixture of ImageNet and web images is competitive with self-supervised benchmarks on ImageNet, achieving 72.0% top-1 accuracy on a linear probe of our features.</p>"
            },
            "media": {
                "url": "https://openai.com/research/image-gpt",
                "caption": "Generative Pretraining from Pixels",
                "credit": "Mark Chen, Alec Radford, Rewon Child, Jeff Wu, Heewoo Jun, Prafulla Dhariwal, David Luan, Ilya Sutskever"
            }
        },
        {
            "start_date": {
                "year": 2020,
                "month": 6,
                "day": 17
            },
            "text": {
                "headline": "iGPT-L",
                "text": "<p>Inspired by progress in unsupervised representation learning for natural language, we examine whether similar models can learn useful representations for images. We train a sequence Transformer to auto-regressively predict pixels, without incorporating knowledge of the 2D input structure. Despite training on low-resolution ImageNet without labels, we find that a GPT-2 scale model learns strong image representations as measured by linear probing, fine-tuning, and low-data classification. On CIFAR-10, we achieve 96.3% accuracy with a linear probe, outperforming a supervised Wide ResNet, and 99.0% accuracy with full finetuning, matching the top supervised pre-trained models. An even larger model trained on a mixture of ImageNet and web images is competitive with self-supervised benchmarks on ImageNet, achieving 72.0% top-1 accuracy on a linear probe of our features.</p>"
            },
            "media": {
                "url": "https://openai.com/blog/image-gpt/",
                "caption": "Generative Pretraining from Pixels",
                "credit": "Mark Chen, Alec Radford, Rewon Child, Jeff Wu, Heewoo Jun, Prafulla Dhariwal, David Luan, Ilya Sutskever"
            }
        },
        {
            "start_date": {
                "year": 2020,
                "month": 5,
                "day": 28
            },
            "text": {
                "headline": "GPT-3 175B (davinci)",
                "text": "<p>Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2005.14165",
                "caption": "Language Models are Few-Shot Learners",
                "credit": "Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei"
            }
        },
        {
            "start_date": {
                "year": 2020,
                "month": 5,
                "day": 26
            },
            "text": {
                "headline": "DETR",
                "text": "<p>Abstract. We present a new method that views object detection as a\ndirect set prediction problem. Our approach streamlines the detection\npipeline, effectively removing the need for many hand-designed components like a non-maximum suppression procedure or anchor generation\nthat explicitly encode our prior knowledge about the task. The main\ningredients of the new framework, called DEtection TRansformer or\nDETR, are a set-based global loss that forces unique predictions via bipartite matching, and a transformer encoder-decoder architecture. Given\na fixed small set of learned object queries, DETR reasons about the relations of the objects and the global image context to directly output\nthe final set of predictions in parallel. The new model is conceptually\nsimple and does not require a specialized library, unlike many other\nmodern detectors. DETR demonstrates accuracy and run-time performance on par with the well-established and highly-optimized Faster RCNN baseline on the challenging COCO object detection dataset. Moreover, DETR can be easily generalized to produce panoptic segmentation\nin a unified manner. We show that it significantly outperforms competitive baselines. Training code and pretrained models are available at\nhttps://github.com/facebookresearch/detr.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2005.12872",
                "caption": "End-to-End Object Detection with Transformers",
                "credit": "Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, Sergey Zagoruyko"
            }
        },
        {
            "start_date": {
                "year": 2020,
                "month": 5,
                "day": 22
            },
            "text": {
                "headline": "Retrieval-Augmented Generator",
                "text": "<p>Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2005.11401v4",
                "caption": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks",
                "credit": "Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, Douwe Kiela"
            }
        },
        {
            "start_date": {
                "year": 2020,
                "month": 5,
                "day": 16
            },
            "text": {
                "headline": "Conformer",
                "text": "<p>Recently Transformer and Convolution neural network (CNN) based models have shown promising results in Automatic Speech Recognition (ASR), outperforming Recurrent neural networks (RNNs). Transformer models are good at capturing content-based global interactions, while CNNs exploit local features effectively. In this work, we achieve the best of both worlds by studying how to combine convolution neural networks and transformers to model both local and global dependencies of an audio sequence in a parameter-efficient way. To this regard, we propose the convolution-augmented transformer for speech recognition, named Conformer. Conformer significantly outperforms the previous Transformer and CNN based models achieving state-of-the-art accuracies. On the widely used LibriSpeech benchmark, our model achieves WER of 2.1%/4.3% without using a language model and 1.9%/3.9% with an external language model on test/testother. We also observe competitive performance of 2.7%/6.3% with a small model of only 10M parameters.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2005.08100v1",
                "caption": "Conformer: Convolution-augmented Transformer for Speech Recognition",
                "credit": "Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu Zhang, Jiahui Yu, Wei Han, Shibo Wang, Zhengdong Zhang, Yonghui Wu, Ruoming Pang"
            }
        },
        {
            "start_date": {
                "year": 2020,
                "month": 5,
                "day": 7
            },
            "text": {
                "headline": "ContextNet",
                "text": "<p>Convolutional neural networks (CNN) have shown promising results for end-to-end speech recognition, albeit still behind other state-of-the-art methods in performance. In this paper, we study how to bridge this gap and go beyond with a novel CNN-RNN-transducer architecture, which we call ContextNet. ContextNet features a fully convolutional encoder that incorporates global context information into convolution layers by adding squeeze-and-excitation modules. In addition, we propose a simple scaling method that scales the widths of ContextNet that achieves good trade-off between computation and accuracy. We demonstrate that on the widely used LibriSpeech benchmark, ContextNet achieves a word error rate (WER) of 2.1%/4.6% without external language model (LM), 1.9%/4.1% with LM and 2.9%/7.0% with only 10M parameters on the clean/noisy LibriSpeech test sets. This compares to the previous best published system of 2.0%/4.6% with LM and 3.9%/11.3% with 20M parameters. The superiority of the proposed ContextNet model is also verified on a much larger internal dataset.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2005.03191v3",
                "caption": "ContextNet: Improving Convolutional Neural Networks for Automatic Speech Recognition with Global Context",
                "credit": "Wei Han, Zhengdong Zhang, Yu Zhang, Jiahui Yu, Chung-Cheng Chiu, James Qin, Anmol Gulati, Ruoming Pang, Yonghui Wu"
            }
        },
        {
            "start_date": {
                "year": 2020,
                "month": 5,
                "day": 6
            },
            "text": {
                "headline": "NAS+ESS (156M)",
                "text": "<p>Neural architecture search (NAS) has advanced significantly in recent years but most NAS systems restrict search to learning architectures of a recurrent or convolutional cell. In this paper, we extend the search space of NAS. In particular, we present a general approach to learn both intra-cell and inter-cell architectures (call it ESS). For a better search result, we design a joint learning method to perform intra-cell and inter-cell NAS simultaneously. We implement our model in a differentiable architecture search system. For recurrent neural language modeling, it outperforms a strong baseline significantly on the PTB and WikiText data, with a new state-of-the-art on PTB. Moreover, the learned architectures show good transferability to other systems. E.g., they improve state-of-the-art systems on the CoNLL and WNUT named entity recognition (NER) tasks and CoNLL chunking task, indicating a promising line of research on large-scale pre-learned architectures.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2005.02593",
                "caption": "Learning Architectures from an Extended Search Space for Language Modeling",
                "credit": "Yinqiao Li, Chi Hu, Yuhao Zhang, Nuo Xu, Yufan Jiang, Tong Xiao, Jingbo Zhu, Tongran Liu, Changliang Li"
            }
        },
        {
            "start_date": {
                "year": 2020,
                "month": 5,
                "day": 2
            },
            "text": {
                "headline": "UnifiedQA",
                "text": "<p>Question answering (QA) tasks have been posed using a variety of formats, such as extractive span selection, multiple choice, etc. This has led to format-specialized models, and even to an implicit division in the QA community. We argue that such boundaries are artificial and perhaps unnecessary, given the reasoning abilities we seek to teach are not governed by the format. As evidence, we use the latest advances in language modeling to build a single pre-trained QA model, UnifiedQA, that performs surprisingly well across 17 QA datasets spanning 4 diverse formats. UnifiedQA performs on par with 9 different models that were trained on individual datasets themselves. Even when faced with 12 unseen datasets of observed formats, UnifiedQA performs surprisingly well, showing strong generalization from its out-of-format training data. Finally, simply fine-tuning this pre-trained QA model into specialized models results in a new state of the art on 6 datasets, establishing UnifiedQA as a strong starting point for building QA systems.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2005.00700v3",
                "caption": "UnifiedQA: Crossing Format Boundaries With a Single QA System",
                "credit": "Daniel Khashabi, Sewon Min, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord, Peter Clark, Hannaneh Hajishirzi"
            }
        },
        {
            "start_date": {
                "year": 2020,
                "month": 5,
                "day": 2
            },
            "text": {
                "headline": "ATLAS",
                "text": "<p>Question answering (QA) tasks have been posed using a variety of formats, such as extractive span selection, multiple choice, etc. This has led to format-specialized models, and even to an implicit division in the QA community. We argue that such boundaries are artificial and perhaps unnecessary, given the reasoning abilities we seek to teach are not governed by the format. As evidence, we use the latest advances in language modeling to build a single pre-trained QA model, UnifiedQA, that performs surprisingly well across 17 QA datasets spanning 4 diverse formats. UnifiedQA performs on par with 9 different models that were trained on individual datasets themselves. Even when faced with 12 unseen datasets of observed formats, UnifiedQA performs surprisingly well, showing strong generalization from its out-of-format training data. Finally, simply fine-tuning this pre-trained QA model into specialized models results in a new state of the art on 6 datasets, establishing UnifiedQA as a strong starting point for building QA systems.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2005.00700",
                "caption": "UnifiedQA: Crossing Format Boundaries With a Single QA System",
                "credit": "Daniel Khashabi, Sewon Min, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord, Peter Clark, Hannaneh Hajishirzi"
            }
        },
        {
            "start_date": {
                "year": 2020,
                "month": 4,
                "day": 29
            },
            "text": {
                "headline": "Once for All",
                "text": "<p>We address the challenging problem of efficient inference across many devices and resource constraints, especially on edge devices. Conventional approaches either manually design or use neural architecture search (NAS) to find a specialized neural network and train it from scratch for each case, which is computationally prohibitive (causing CO2 emission as much as 5 cars' lifetime) thus unscalable. In this work, we propose to train a once-for-all (OFA) network that supports diverse architectural settings by decoupling training and search, to reduce the cost. We can quickly get a specialized sub-network by selecting from the OFA network without additional training. To efficiently train OFA networks, we also propose a novel progressive shrinking algorithm, a generalized pruning method that reduces the model size across many more dimensions than pruning (depth, width, kernel size, and resolution). It can obtain a surprisingly large number of sub-networks (>1019) that can fit different hardware platforms and latency constraints while maintaining the same level of accuracy as training independently. On diverse edge devices, OFA consistently outperforms state-of-the-art (SOTA) NAS methods (up to 4.0% ImageNet top1 accuracy improvement over MobileNetV3, or same accuracy but 1.5x faster than MobileNetV3, 2.6x faster than EfficientNet w.r.t measured latency) while reducing many orders of magnitude GPU hours and CO2 emission. In particular, OFA achieves a new SOTA 80.0% ImageNet top-1 accuracy under the mobile setting (<600M MACs). OFA is the winning solution for the 3rd Low Power Computer Vision Challenge (LPCVC), DSP classification track and the 4th LPCVC, both classification track and detection track. Code and 50 pre-trained models (for many devices & many latency constraints) are released at this https URL.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1908.09791",
                "caption": "Once for all: Train one network and specialize it for efficient deployment.",
                "credit": "Han Cai, Chuang Gan, Tianzhe Wang, Zhekai Zhang, and Song Han"
            }
        },
        {
            "start_date": {
                "year": 2020,
                "month": 4,
                "day": 27
            },
            "text": {
                "headline": "Go-explore",
                "text": "<p>The promise of reinforcement learning is to solve complex sequential decision problems autonomously by specifying a high-level reward function only. However, reinforcement learning algorithms struggle when, as is often the case, simple and intuitive rewards provide sparse and deceptive feedback. Avoiding these pitfalls requires thoroughly exploring the environment, but creating algorithms that can do so remains one of the central challenges of the field. We hypothesise that the main impediment to effective exploration originates from algorithms forgetting how to reach previously visited states (\"detachment\") and from failing to first return to a state before exploring from it (\"derailment\"). We introduce Go-Explore, a family of algorithms that addresses these two challenges directly through the simple principles of explicitly remembering promising states and first returning to such states before intentionally exploring. Go-Explore solves all heretofore unsolved Atari games and surpasses the state of the art on all hard-exploration games, with orders of magnitude improvements on the grand challenges Montezuma's Revenge and Pitfall. We also demonstrate the practical potential of Go-Explore on a sparse-reward pick-and-place robotics task. Additionally, we show that adding a goal-conditioned policy can further improve Go-Explore's exploration efficiency and enable it to handle stochasticity throughout training. The substantial performance gains from Go-Explore suggest that the simple principles of remembering states, returning to them, and exploring from them are a powerful and general approach to exploration, an insight that may prove critical to the creation of truly intelligent learning agents.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2004.12919",
                "caption": "First return, then explore",
                "credit": "Adrien Ecoffet, Joost Huizinga, Joel Lehman, Kenneth O. Stanley, Jeff Clune"
            }
        },
        {
            "start_date": {
                "year": 2020,
                "month": 4,
                "day": 8
            },
            "text": {
                "headline": "CURL",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2004.04136v4",
                "caption": "CURL: Contrastive Unsupervised Representations for Reinforcement Learning",
                "credit": "A Srinivas, M Laskin, P Abbeel"
            }
        },
        {
            "start_date": {
                "year": 2020,
                "month": 3,
                "day": 30
            },
            "text": {
                "headline": "Agent57",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2003.13350",
                "caption": "Agent57: Outperforming the Atari Human Benchmark",
                "credit": "AP Badia, B Piot, S Kapturowski"
            }
        },
        {
            "start_date": {
                "year": 2020,
                "month": 3,
                "day": 24
            },
            "text": {
                "headline": "MetNet",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2003.12140",
                "caption": "MetNet: A Neural Weather Model for Precipitation Forecasting",
                "credit": "Casper Kaae Sønderby, Lasse Espeholt, Jonathan Heek, Mostafa Dehghani, Avital Oliver, Tim Salimans, Shreya Agrawal, Jason Hickey, Nal Kalchbrenner"
            }
        },
        {
            "start_date": {
                "year": 2020,
                "month": 3,
                "day": 23
            },
            "text": {
                "headline": "ELECTRA",
                "text": "<p>Masked language modeling (MLM) pre-training methods such as BERT corrupt the input by replacing some tokens with [MASK] and then train a model to reconstruct the original tokens. While they produce good results when transferred to downstream NLP tasks, they generally require large amounts of compute to be effective. As an alternative, we propose a more sample-efficient pre-training task called replaced token detection. Instead of masking the input, our approach corrupts it by replacing some tokens with plausible alternatives sampled from a small generator network. Then, instead of training a model that predicts the original identities of the corrupted tokens, we train a discriminative model that predicts whether each token in the corrupted input was replaced by a generator sample or not. Thorough experiments demonstrate this new pre-training task is more efficient than MLM because the task is defined over all input tokens rather than just the small subset that was masked out. As a result, the contextual representations learned by our approach substantially outperform the ones learned by BERT given the same model size, data, and compute. The gains are particularly strong for small models; for example, we train a model on one GPU for 4 days that outperforms GPT (trained using 30x more compute) on the GLUE natural language understanding benchmark. Our approach also works well at scale, where it performs comparably to RoBERTa and XLNet while using less than 1/4 of their compute and outperforms them when using the same amount of compute.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2003.10555v1",
                "caption": "ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators",
                "credit": "Kevin Clark, Minh-Thang Luong, Quoc V. Le, Christopher D. Manning"
            }
        },
        {
            "start_date": {
                "year": 2020,
                "month": 3,
                "day": 17
            },
            "text": {
                "headline": "Tensor-Transformer(1core)+PN (WT103)",
                "text": "<p>The standard normalization method for neural network (NN) models used in Natural Language Processing (NLP) is layer normalization (LN). This is different than batch normalization (BN), which is widely-adopted in Computer Vision. The preferred use of LN in NLP is principally due to the empirical observation that a (naive/vanilla) use of BN leads to significant performance degradation for NLP tasks; however, a thorough understanding of the underlying reasons for this is not always evident. In this paper, we perform a systematic study of NLP transformer models to understand why BN has a poor performance, as compared to LN. We find that the statistics of NLP data across the batch dimension exhibit large fluctuations throughout training. This results in instability, if BN is naively implemented. To address this, we propose Power Normalization (PN), a novel normalization scheme that resolves this issue by (i) relaxing zero-mean normalization in BN, (ii) incorporating a running quadratic mean instead of per batch statistics to stabilize fluctuations, and (iii) using an approximate backpropagation for incorporating the running statistics in the forward pass. We show theoretically, under mild assumptions, that PN leads to a smaller Lipschitz constant for the loss, compared with BN. Furthermore, we prove that the approximate backpropagation scheme leads to bounded gradients. We extensively test PN for transformers on a range of NLP tasks, and we show that it significantly outperforms both LN and BN. In particular, PN outperforms LN by 0.4/0.6 BLEU on IWSLT14/WMT14 and 5.6/3.0 PPL on PTB/WikiText-103. We make our code publicly available at \\url{this https URL}.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2003.07845",
                "caption": "PowerNorm: Rethinking Batch Normalization in Transformers",
                "credit": "Sheng Shen, Zhewei Yao, Amir Gholami, Michael W. Mahoney, Kurt Keutzer"
            }
        },
        {
            "start_date": {
                "year": 2020,
                "month": 3,
                "day": 12
            },
            "text": {
                "headline": "Routing Transformer (WT-103)",
                "text": "<p>Self-attention has recently been adopted for a wide range of sequence modeling problems. Despite its effectiveness, self-attention suffers from quadratic compute and memory requirements with respect to sequence length. Successful approaches to reduce this complexity focused on attending to local sliding windows or a small set of locations independent of content. Our work proposes to learn dynamic sparse attention patterns that avoid allocating computation and memory to attend to content unrelated to the query of interest. This work builds upon two lines of research: it combines the modeling flexibility of prior work on content-based sparse attention with the efficiency gains from approaches based on local, temporal sparse attention. Our model, the Routing Transformer, endows self-attention with a sparse routing module based on online k-means while reducing the overall complexity of attention to O(n1.5d) from O(n2d) for sequence length n and hidden dimension d. We show that our model outperforms comparable sparse attention models on language modeling on Wikitext-103 (15.8 vs 18.3 perplexity) as well as on image generation on ImageNet-64 (3.43 vs 3.44 bits/dim) while using fewer self-attention layers. Additionally, we set a new state-of-the-art on the newly released PG-19 data-set, obtaining a test perplexity of 33.2 with a 22 layer Routing Transformer model trained on sequences of length 8192.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2003.05997",
                "caption": "Efficient Content-Based Sparse Attention with Routing Transformers",
                "credit": "Aurko Roy, Mohammad Saffar, Ashish Vaswani, David Grangier"
            }
        },
        {
            "start_date": {
                "year": 2020,
                "month": 3,
                "day": 11
            },
            "text": {
                "headline": "TransformerXL + spectrum control",
                "text": "<p>Recent Transformer-based models such as Transformer-XL and BERT have achieved huge success on various natural language processing tasks. However, contextualized embeddings at the output layer of these powerful models tend to degenerate and occupy an anisotropic cone in the vector space, which is called the representation degeneration problem. In this paper, we propose a novel spectrum control approach to address this degeneration problem. The core idea of our method is to directly guide the spectra training of the output embedding matrix with a slow-decaying singular value prior distribution through a reparameterization framework. We show that our proposed method encourages isotropy of the learned word representations while maintains the modeling power of these contextual neural models. We further provide a theoretical analysis and insight on the benefit of modeling singular value distribution. We demonstrate that our spectrum control method outperforms the state-of-the-art Transformer-XL modeling for language model, and various Transformer-based models for machine translation, on common benchmark datasets for these tasks.</p>"
            },
            "media": {
                "url": "https://openreview.net/forum?id=ByxY8CNtvr",
                "caption": "Improving Neural Language Generation with Spectrum Control",
                "credit": "Lingxiao Wang, Jing Huang, Kevin Huang, Ziniu Hu, Guangtao Wang, Quanquan Gu"
            }
        },
        {
            "start_date": {
                "year": 2020,
                "month": 2,
                "day": 28
            },
            "text": {
                "headline": "TCAN (WT2)",
                "text": "<p>With the development of feed-forward models, the default model for sequence modeling has gradually evolved to replace recurrent networks. Many powerful feed-forward models based on convolutional networks and attention mechanism were proposed and show more potential to handle sequence modeling tasks. We wonder that is there an architecture that can not only achieve an approximate substitution of recurrent network, but also absorb the advantages of feed-forward models. So we propose an exploratory architecture referred to Temporal Convolutional Attention-based Network (TCAN) which combines temporal convolutional network and attention mechanism. TCAN includes two parts, one is Temporal Attention (TA) which captures relevant features inside the sequence, the other is Enhanced Residual (ER) which extracts shallow layer's important information and transfers to deep layers. We improve the state-of-the-art results of bpc/perplexity to 30.28 on word-level PTB, 1.092 on character-level PTB, and 9.20 on WikiText-2.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2002.12530",
                "caption": "Temporal Convolutional Attention-based Network For Sequence Modeling",
                "credit": "Hongyan Hao, Yan Wang, Yudi Xia, Jian Zhao, Furao Shen"
            }
        },
        {
            "start_date": {
                "year": 2020,
                "month": 2,
                "day": 21
            },
            "text": {
                "headline": "Feedback Transformer",
                "text": "<p>Transformers have been successfully applied to sequential, auto-regressive tasks despite being feedforward networks. Unlike recurrent neural networks, Transformers use attention to capture temporal relations while processing input tokens in parallel. While this parallelization makes them computationally efficient, it restricts the model from fully exploiting the sequential nature of the input. The representation at a given layer can only access representations from lower layers, rather than the higher level representations already available. In this work, we propose the Feedback Transformer architecture that exposes all previous representations to all future representations, meaning the lowest representation of the current timestep is formed from the highest-level abstract representation of the past. We demonstrate on a variety of benchmarks in language modeling, machine translation, and reinforcement learning that the increased representation capacity can create small, shallow models with much stronger performance than comparable Transformers.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2002.09402",
                "caption": "Addressing Some Limitations of Transformers with Feedback Memory",
                "credit": "Angela Fan, Thibaut Lavril, Edouard Grave, Armand Joulin, Sainbayar Sukhbaatar"
            }
        },
        {
            "start_date": {
                "year": 2020,
                "month": 2,
                "day": 13
            },
            "text": {
                "headline": "Turing-NLG",
                "text": "<p>Turing Natural Language Generation (T-NLG) is a 17 billion parameter language model by Microsoft that outperforms the state of the art on many downstream NLP tasks. We present a demo of the model, including its freeform generation, question answering, and summarization capabilities, to academics for feedback and research purposes. <|endoftext|></p>"
            },
            "media": {
                "url": "https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/",
                "caption": "Turing-NLG: A 17-billion-parameter language model by Microsoft",
                "credit": "Corby Rosset"
            }
        },
        {
            "start_date": {
                "year": 2020,
                "month": 2,
                "day": 13
            },
            "text": {
                "headline": "SimCLR",
                "text": "<p>This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5% top-1 accuracy, which is a 7% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1% of the labels, we achieve 85.8% top-5 accuracy, outperforming AlexNet with 100X fewer labels.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2002.05709",
                "caption": "A Simple Framework for Contrastive Learning of Visual Representations",
                "credit": "Ting Chen, Simon Kornblith, Mohammad Norouzi, Geoffrey Hinton"
            }
        },
        {
            "start_date": {
                "year": 2020,
                "month": 2,
                "day": 9
            },
            "text": {
                "headline": "ALBERT-xxlarge",
                "text": "<p>Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations and longer training times. To address these problems, we present two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT. Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the original BERT. We also use a self-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks with multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and \\squad benchmarks while having fewer parameters compared to BERT-large. The code and the pretrained models are available at this https URL.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1909.11942",
                "caption": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations.",
                "credit": "Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut"
            }
        },
        {
            "start_date": {
                "year": 2020,
                "month": 2,
                "day": 8
            },
            "text": {
                "headline": "TaLK Convolution",
                "text": "<p>To date, most state-of-the-art sequence modeling architectures use attention to build generative models for language based tasks. Some of these models use all the available sequence tokens to generate an attention distribution which results in time complexity of O(n2). Alternatively, they utilize depthwise convolutions with softmax normalized kernels of size k acting as a limited-window self-attention, resulting in time complexity of O(k⋅n). In this paper, we introduce Time-aware Large Kernel (TaLK) Convolutions, a novel adaptive convolution operation that learns to predict the size of a summation kernel instead of using a fixed-sized kernel matrix. This method yields a time complexity of O(n), effectively making the sequence encoding process linear to the number of tokens. We evaluate the proposed method on large-scale standard machine translation, abstractive summarization and language modeling datasets and show that TaLK Convolutions constitute an efficient improvement over other attention/convolution based approaches.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2002.03184",
                "caption": "Time-aware Large Kernel Convolutions",
                "credit": "Vasileios Lioutas, Yuhong Guo"
            }
        },
        {
            "start_date": {
                "year": 2020,
                "month": 2,
                "day": 8
            },
            "text": {
                "headline": "Perceiver IO (optical flow)",
                "text": "<p>A central goal of machine learning is the development of systems that can solve many problems in as many data domains as possible. Current architectures, however, cannot be applied beyond a small set of stereotyped settings, as they bake in domain & task assumptions or scale poorly to large inputs or outputs. In this work, we propose Perceiver IO, a general-purpose architecture that handles data from arbitrary settings while scaling linearly with the size of inputs and outputs. Our model augments the Perceiver with a flexible querying mechanism that enables outputs of various sizes and semantics, doing away with the need for task-specific architecture engineering. The same architecture achieves strong results on tasks spanning natural language and visual understanding, multi-task and multi-modal reasoning, and StarCraft II. As highlights, Perceiver IO outperforms a Transformer-based BERT baseline on the GLUE language benchmark despite removing input tokenization and achieves state-of-the-art performance on Sintel optical flow estimation with no explicit mechanisms for multiscale correspondence.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2107.14795",
                "caption": "Perceiver IO: A General Architecture for Structured Inputs & Outputs",
                "credit": "Andrew Jaegle, Sebastian Borgeaud, Jean-Baptiste Alayrac, Carl Doersch, Catalin Ionescu, David Ding, Skanda Koppula, Daniel Zoran, Andrew Brock, Evan Shelhamer, Olivier Hénaff,\nMatthew M. Botvinick, Andrew Zisserman, Oriol Vinyals, João Carreira"
            }
        },
        {
            "start_date": {
                "year": 2020,
                "month": 2,
                "day": 7
            },
            "text": {
                "headline": "Theseus 6/768",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2002.02925",
                "caption": "BERT-of-Theseus: Compressing BERT by Progressive Module Replacing",
                "credit": "Canwen Xu, Wangchunshu Zhou, Tao Ge, Furu Wei, Ming Zhou"
            }
        },
        {
            "start_date": {
                "year": 2020,
                "month": 1,
                "day": 28
            },
            "text": {
                "headline": "Meena",
                "text": "<p>We present Meena, a multi-turn open-domain chatbot trained end-to-end on data mined and filtered from public domain social media conversations. This 2.6B parameter neural network is simply trained to minimize perplexity of the next token. We also propose a human evaluation metric called Sensibleness and Specificity Average (SSA), which captures key elements of a human-like multi-turn conversation. Our experiments show strong correlation between perplexity and SSA. The fact that the best perplexity end-to-end trained Meena scores high on SSA (72% on multi-turn evaluation) suggests that a human-level SSA of 86% is potentially within reach if we can better optimize perplexity. Additionally, the full version of Meena (with a filtering mechanism and tuned decoding) scores 79% SSA, 23% higher in absolute SSA than the existing chatbots we evaluated.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2001.09977",
                "caption": "Towards a Human-like Open-Domain Chatbot",
                "credit": "Dongling Xiao, Han Zhang, Yukun Li, Yu Sun, Hao Tian, Hua Wu, Haifeng Wang"
            }
        },
        {
            "start_date": {
                "year": 2020,
                "month": 1,
                "day": 19
            },
            "text": {
                "headline": "ContextNet + Noisy Student",
                "text": "<p>Recently, a semi-supervised learning method known as \"noisy student training\" has been shown to improve image classification performance of deep networks significantly. Noisy student training is an iterative self-training method that leverages augmentation to improve network performance. In this work, we adapt and improve noisy student training for automatic speech recognition, employing (adaptive) SpecAugment as the augmentation method. We find effective methods to filter, balance and augment the data generated in between self-training iterations. By doing so, we are able to obtain word error rates (WERs) 4.2%/8.6% on the clean/noisy LibriSpeech test sets by only using the clean 100h subset of LibriSpeech as the supervised set and the rest (860h) as the unlabeled set. Furthermore, we are able to achieve WERs 1.7%/3.4% on the clean/noisy LibriSpeech test sets by using the unlab-60k subset of LibriLight as the unlabeled set for LibriSpeech 960h. We are thus able to improve upon the previous state-of-the-art clean/noisy test WERs achieved on LibriSpeech 100h (4.74%/12.20%) and LibriSpeech (1.9%/4.1%).</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/2005.09629v2",
                "caption": "Improved Noisy Student Training for Automatic Speech Recognition",
                "credit": "Daniel S. Park, Yu Zhang, Ye Jia, Wei Han, Chung-Cheng Chiu, Bo Li, Yonghui Wu, Quoc V. Le"
            }
        },
        {
            "start_date": {
                "year": 2020,
                "month": 1,
                "day": 15
            },
            "text": {
                "headline": "AlphaFold",
                "text": "<p>Protein structure prediction can be used to determine the three-dimensional shape of a protein from its amino acid sequence. This problem is of fundamental importance as the structure of a protein largely determines its function; however, protein structures can be difficult to determine experimentally. Considerable progress has recently been made by leveraging genetic information. It is possible to infer which amino acid residues are in contact by analysing covariation in homologous sequences, which aids in the prediction of protein structures. Here we show that we can train a neural network to make accurate predictions of the distances between pairs of residues, which convey more information about the structure than contact predictions. Using this information, we construct a potential of mean force that can accurately describe the shape of a protein. We find that the resulting potential can be optimized by a simple gradient descent algorithm to generate structures without complex sampling procedures. The resulting system, named AlphaFold, achieves high accuracy, even for sequences with fewer homologous sequences. In the recent Critical Assessment of Protein Structure Prediction (CASP13)—a blind assessment of the state of the field—AlphaFold created high-accuracy structures (with template modelling (TM) scores of 0.7 or higher) for 24 out of 43 free modelling domains, whereas the next best method, which used sampling and contact information, achieved such accuracy for only 14 out of 43 domains. AlphaFold represents a considerable advance in protein-structure prediction. We expect this increased accuracy to enable insights into the function and malfunction of proteins, especially in cases for which no structures for homologous proteins have been experimentally determined.</p>"
            },
            "media": {
                "url": "https://www.nature.com/articles/s41586-019-1923-7",
                "caption": "Improved protein structure prediction using potentials from deep learning",
                "credit": "Andrew W. Senior, Richard Evans, John Jumper, James Kirkpatrick, Laurent Sifre, Tim Green, Chongli Qin, Augustin Žídek, Alexander W. R. Nelson, Alex Bridgland, Hugo Penedones, Stig Petersen, Karen Simonyan, Steve Crossan, Pushmeet Kohli, David T. Jones, David Silver, Koray Kavukcuoglu, Demis Hassabis"
            }
        },
        {
            "start_date": {
                "year": 2019,
                "month": 12,
                "day": 24
            },
            "text": {
                "headline": "Big Transfer (BiT-L)",
                "text": "<p>Transfer of pre-trained representations improves sample efficiency and simplifies hyperparameter tuning when training deep neural networks for vision. We revisit the paradigm of pre-training on large supervised datasets and fine-tuning the model on a target task. We scale up pre-training, and propose a simple recipe that we call Big Transfer (BiT). By combining a few carefully selected components, and transferring using a simple heuristic, we achieve strong performance on over 20 datasets. BiT performs well across a surprisingly wide range of data regimes -- from 1 example per class to 1M total examples. BiT achieves 87.5% top-1 accuracy on ILSVRC-2012, 99.4% on CIFAR-10, and 76.3% on the 19 task Visual Task Adaptation Benchmark (VTAB). On small datasets, BiT attains 76.8% on ILSVRC-2012 with 10 examples per class, and 97.0% on CIFAR-10 with 10 examples per class. We conduct detailed analysis of the main components that lead to high transfer performance.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1912.11370",
                "caption": "Big Transfer (BiT): General Visual Representation Learning",
                "credit": "Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly, Neil Houlsby"
            }
        },
        {
            "start_date": {
                "year": 2019,
                "month": 12,
                "day": 19
            },
            "text": {
                "headline": "DD-PPO",
                "text": "<p>We present Decentralized Distributed Proximal Policy Optimization (DD-PPO), a method for distributed reinforcement learning in resource-intensive simulated environments. DD-PPO is distributed (uses multiple machines), decentralized (lacks a centralized server), and synchronous (no computation is ever \"stale\"), making it conceptually simple and easy to implement. In our experiments on training virtual robots to navigate in Habitat-Sim, DD-PPO exhibits near-linear scaling -- achieving a speedup of 107x on 128 GPUs over a serial implementation. We leverage this scaling to train an agent for 2.5 Billion steps of experience (the equivalent of 80 years of human experience) -- over 6 months of GPU-time training in under 3 days of wall-clock time with 64 GPUs. </p>"
            },
            "media": {
                "url": "https://openreview.net/forum?id=H1gX8C4YPr",
                "caption": "DD-PPO: Learning Near-Perfect PointGoal Navigators from 2.5 Billion Frames",
                "credit": "Erik Wijmans, Abhishek Kadian, Ari Morcos, Stefan Lee, Irfan Essa, Devi Parikh, Manolis Savva, Dhruv Batra"
            }
        },
        {
            "start_date": {
                "year": 2019,
                "month": 12,
                "day": 13
            },
            "text": {
                "headline": "OpenAI Five Rerun",
                "text": "<p>On April 13th, 2019, OpenAI Five became the first AI system to defeat the world champions at an esports game. The game of Dota 2 presents novel challenges for AI systems such as long time horizons, imperfect information, and complex, continuous state-action spaces, all challenges which will become increasingly central to more capable AI systems. OpenAI Five leveraged existing reinforcement learning techniques, scaled to learn from batches of approximately 2 million frames every 2 seconds. We developed a distributed training system and tools for continual training which allowed us to train OpenAI Five for 10 months. By defeating the Dota 2 world champion (Team OG), OpenAI Five demonstrates that self-play reinforcement learning can achieve superhuman performance on a difficult task.</p>"
            },
            "media": {
                "url": "https://cdn.openai.com/dota-2.pdf",
                "caption": "Dota 2 with Large Scale Deep Reinforcement Learning",
                "credit": "Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung,\nPrzemysław “Psyho\" Dębiak, Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, Rafal Józefowicz, Scott Gray, Catherine Olsson, Jakub Pachocki, Michael Petrov, Henrique Pondé de Oliveira Pinto, Jonathan Raiman, Tim Salimans, Jeremy Schlatter, Jonas Schneider, Szymon Sidor, Ilya Sutskever, Jie Tang, Filip Wolski, Susan Zhang"
            }
        },
        {
            "start_date": {
                "year": 2019,
                "month": 12,
                "day": 13
            },
            "text": {
                "headline": "OpenAI Five",
                "text": "<p>On April 13th, 2019, OpenAI Five became the first AI system to defeat the world champions at an esports game. The game of Dota 2 presents novel challenges for AI systems such as long time horizons, imperfect information, and complex, continuous state-action spaces, all challenges which will become increasingly central to more capable AI systems. OpenAI Five leveraged existing reinforcement learning techniques, scaled to learn from batches of approximately 2 million frames every 2 seconds. We developed a distributed training system and tools for continual training which allowed us to train OpenAI Five for 10 months. By defeating the Dota 2 world champion (Team OG), OpenAI Five demonstrates that self-play reinforcement learning can achieve superhuman performance on a difficult task.\n</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1912.06680",
                "caption": "Dota 2 with Large Scale Deep Reinforcement Learning",
                "credit": "Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemysław Dębiak, Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, Rafal Józefowicz, Scott Gray, Catherine Olsson, Jakub Pachocki, Michael Petrov, Henrique P. d.O. Pinto, Jonathan Raiman, Tim Salimans, Jeremy Schlatter, Jonas Schneider, Szymon Sidor, Ilya Sutskever, Jie Tang, Filip Wolski, Susan Zhang"
            }
        },
        {
            "start_date": {
                "year": 2019,
                "month": 12,
                "day": 5
            },
            "text": {
                "headline": "MMLSTM",
                "text": "<p>Abstract—Language model plays an important role in natural\nlanguage processing (NLP) systems like machine translation,\nspeech recognition, learning token embeddings, natural language generation and text classification. Recently, the multi-layer Long Short-Term Memory (LSTM) models have been demonstrated to achieve promising performance on word-level language modeling.\nFor each LSTM layer, larger hidden size usually means more\ndiverse semantic features, which enables the language model to\nperform better. However, we have observed that when a certain\nLSTM layer reaches a sufficiently large scale, the promotion\nof overall effect will slow down as its hidden size increases. In\nthis paper, we analyze that an important factor leading to this\nphenomenon is the high correlation between the newly extended hidden states and original hidden states, which hinders diverse feature expression of the LSTM. As a result, when the scale is large enough, simply lengthening the LSTM hidden states will cost tremendous extra parameters but has little effect. We propose a simple yet effective improvement on each LSTM layer consisting of a large-scale Major LSTM and a smallscale Minor LSTM to break the high correlation between the two parts of hidden states, which we call Major-Minor LSTMs (MMLSTMs). In experiments, we demonstrate the language model with MMLSTMs surpasses the existing state-of-the-art model on Penn Treebank (PTB) and WikiText-2 (WT2) datasets, and outperforms the baseline by 3.3 points in perplexity on WikiText-103 dataset without increasing model parameter counts.</p>"
            },
            "media": {
                "url": "http://repository.uwl.ac.uk/id/eprint/6490/1/Loo_etal_IEEE_TNNLS_2019_Major-minor_long_short-term_memory_for_word-level_language_model.pdf",
                "caption": "Major–Minor Long Short-Term Memory for Word-Level Language Model",
                "credit": "Kai Shuang, Rui Li, Mengyu Gu, Jonathan Loo, Sen Su"
            }
        },
        {
            "start_date": {
                "year": 2019,
                "month": 12,
                "day": 4
            },
            "text": {
                "headline": "StarGAN v2",
                "text": "<p>A good image-to-image translation model should learn a mapping between different visual domains while satisfying the following properties: 1) diversity of generated images and 2) scalability over multiple domains. Existing methods address either of the issues, having limited diversity or multiple models for all domains. We propose StarGAN v2, a single framework that tackles both and shows significantly improved results over the baselines. Experiments on CelebA-HQ and a new animal faces dataset (AFHQ) validate our superiority in terms of visual quality, diversity, and scalability. To better assess image-to-image translation models, we release AFHQ, high-quality animal faces with large inter- and intra-domain differences. The code, pretrained models, and dataset can be found at this https URL.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1912.01865",
                "caption": "StarGAN v2: Diverse Image Synthesis for Multiple Domains",
                "credit": "Yunjey Choi, Youngjung Uh, Jaejun Yoo, Jung-Woo Ha"
            }
        },
        {
            "start_date": {
                "year": 2019,
                "month": 11,
                "day": 27
            },
            "text": {
                "headline": "Transformer-XL DeFINE (141M)",
                "text": "<p>For sequence models with large vocabularies, a majority of network parameters lie in the input and output layers. In this work, we describe a new method, DeFINE, for learning deep token representations efficiently. Our architecture uses a hierarchical structure with novel skip-connections which allows for the use of low dimensional input and output layers, reducing total parameters and training time while delivering similar or better performance versus existing methods. DeFINE can be incorporated easily in new or existing sequence models. Compared to state-of-the-art methods including adaptive input representations, this technique results in a 6% to 20% drop in perplexity. On WikiText-103, DeFINE reduces the total parameters of Transformer-XL by half with minimal impact on performance. On the Penn Treebank, DeFINE improves AWD-LSTM by 4 points with a 17% reduction in parameters, achieving comparable performance to state-of-the-art methods with fewer parameters. For machine translation, DeFINE improves the efficiency of the Transformer model by about 1.4 times while delivering similar performance.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1911.12385",
                "caption": "DeFINE: DEep Factorized INput Token Embeddings for Neural Sequence Modeling",
                "credit": "Sachin Mehta, Rik Koncel-Kedziorski, Mohammad Rastegari, Hannaneh Hajishirzi"
            }
        },
        {
            "start_date": {
                "year": 2019,
                "month": 11,
                "day": 25
            },
            "text": {
                "headline": "Photo-Geometric Autoencoder",
                "text": "<p>We propose a method to learn 3D deformable object categories from raw single-view images, without external supervision. The method is based on an autoencoder that factors each input image into depth, albedo, viewpoint and illumination. In order to disentangle these components without supervision, we use the fact that many object categories have, at least in principle, a symmetric structure. We show that reasoning about illumination allows us to exploit the underlying object symmetry even if the appearance is not symmetric due to shading. Furthermore, we model objects that are probably, but not certainly, symmetric by predicting a symmetry probability map, learned end-to-end with the other components of the model. Our experiments show that this method can recover very accurately the 3D shape of human faces, cat faces and cars from single-view images, without any supervision or a prior shape model. On benchmarks, we demonstrate superior accuracy compared to another method that uses supervision at the level of 2D image correspondences.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1911.11130",
                "caption": "Unsupervised Learning of Probably Symmetric Deformable 3D Objects From Images in the Wild",
                "credit": "Shangzhe Wu, Christian Rupprecht, Andrea Vedaldi\n"
            }
        },
        {
            "start_date": {
                "year": 2019,
                "month": 11,
                "day": 19
            },
            "text": {
                "headline": "Transformer - LibriVox + Decoding/Rescoring",
                "text": "<p>We study pseudo-labeling for the semi-supervised training of ResNet, Time-Depth Separable ConvNets, and Transformers for speech recognition, with either CTC or Seq2Seq loss functions. We perform experiments on the standard LibriSpeech dataset, and leverage additional unlabeled data from LibriVox through pseudo-labeling. We show that while Transformer-based acoustic models have superior performance with the supervised dataset alone, semi-supervision improves all models across architectures and loss functions and bridges much of the performance gaps between them. In doing so, we reach a new state-of-the-art for end-to-end acoustic models decoded with an external language model in the standard supervised learning setting, and a new absolute state-of-the-art with semi-supervised training. Finally, we study the effect of leveraging different amounts of unlabeled audio, propose several ways of evaluating the characteristics of unlabeled audio which improve acoustic modeling, and show that acoustic models trained with more audio rely less on external language models.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1911.08460v3",
                "caption": "End-to-end ASR: from Supervised to Semi-Supervised Learning with Modern Architectures",
                "credit": "Gabriel Synnaeve, Qiantong Xu, Jacob Kahn, Tatiana Likhomanenko, Edouard Grave, Vineel Pratap, Anuroop Sriram, Vitaliy Liptchinsky, Ronan Collobert"
            }
        },
        {
            "start_date": {
                "year": 2019,
                "month": 11,
                "day": 19
            },
            "text": {
                "headline": "MuZero",
                "text": "<p>Constructing agents with planning capabilities has long been one of the main challenges in the pursuit of artificial intelligence. Tree-based planning methods have enjoyed huge success in challenging domains, such as chess and Go, where a perfect simulator is available. However, in real-world problems the dynamics governing the environment are often complex and unknown. In this work we present the MuZero algorithm which, by combining a tree-based search with a learned model, achieves superhuman performance in a range of challenging and visually complex domains, without any knowledge of their underlying dynamics. MuZero learns a model that, when applied iteratively, predicts the quantities most directly relevant to planning: the reward, the action-selection policy, and the value function. When evaluated on 57 different Atari games - the canonical video game environment for testing AI techniques, in which model-based planning approaches have historically struggled - our new algorithm achieved a new state of the art. When evaluated on Go, chess and shogi, without any knowledge of the game rules, MuZero matched the superhuman performance of the AlphaZero algorithm that was supplied with the game rules.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1911.08265v2",
                "caption": "Mastering Atari Go Chess and Shogi by Planning with a Learned Model",
                "credit": "J Schrittwieser, I Antonoglou, T Hubert, K Simonyan"
            }
        },
        {
            "start_date": {
                "year": 2019,
                "month": 11,
                "day": 13
            },
            "text": {
                "headline": "MoCo",
                "text": "<p>We present Momentum Contrast (MoCo) for unsupervised visual representation learning. From a perspective on contrastive learning as dictionary look-up, we build a dynamic dictionary with a queue and a moving-averaged encoder. This enables building a large and consistent dictionary on-the-fly that facilitates contrastive unsupervised learning. MoCo provides competitive results under the common linear protocol on ImageNet classification. More importantly, the representations learned by MoCo transfer well to downstream tasks. MoCo can outperform its supervised pre-training counterpart in 7 detection/segmentation tasks on PASCAL VOC, COCO, and other datasets, sometimes surpassing it by large margins. This suggests that the gap between unsupervised and supervised representation learning has been largely closed in many vision tasks.\n</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1911.05722",
                "caption": "Momentum Contrast for Unsupervised Visual Representation Learning",
                "credit": "Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xe, Ross Girshick"
            }
        },
        {
            "start_date": {
                "year": 2019,
                "month": 11,
                "day": 11
            },
            "text": {
                "headline": "Noisy Student (L2)",
                "text": "<p>We present Noisy Student Training, a semi-supervised learning approach that works well even when labeled data is abundant. Noisy Student Training achieves 88.4% top-1 accuracy on ImageNet, which is 2.0% better than the state-of-the-art model that requires 3.5B weakly labeled Instagram images. On robustness test sets, it improves ImageNet-A top-1 accuracy from 61.0% to 83.7%, reduces ImageNet-C mean corruption error from 45.7 to 28.3, and reduces ImageNet-P mean flip rate from 27.8 to 12.2.\nNoisy Student Training extends the idea of self-training and distillation with the use of equal-or-larger student models and noise added to the student during learning. On ImageNet, we first train an EfficientNet model on labeled images and use it as a teacher to generate pseudo labels for 300M unlabeled images. We then train a larger EfficientNet as a student model on the combination of labeled and pseudo labeled images. We iterate this process by putting back the student as the teacher. During the learning of the student, we inject noise such as dropout, stochastic depth, and data augmentation via RandAugment to the student so that the student generalizes better than the teacher. Models are available at this https URL. Code is available at this https URL.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1911.04252v4",
                "caption": "Self-training with Noisy Student improves ImageNet classification",
                "credit": "Q Xie, MT Luong, E Hovy, QV Lee"
            }
        },
        {
            "start_date": {
                "year": 2019,
                "month": 11,
                "day": 10
            },
            "text": {
                "headline": "Sandwich Transformer",
                "text": "<p>Multilayer transformer networks consist of interleaved self-attention and feedforward sublayers. Could ordering the sublayers in a different pattern lead to better performance? We generate randomly ordered transformers and train them with the language modeling objective. We observe that some of these models are able to achieve better performance than the interleaved baseline, and that those successful variants tend to have more self-attention at the bottom and more feedforward sublayers at the top. We propose a new transformer pattern that adheres to this property, the sandwich transformer, and show that it improves perplexity on multiple word-level and character-level language modeling benchmarks, at no cost in parameters, memory, or training time. However, the sandwich reordering pattern does not guarantee performance gains across every task, as we demonstrate on machine translation models. Instead, we suggest that further exploration of task-specific sublayer reorderings is needed in order to unlock additional gains.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1911.03864",
                "caption": "Improving Transformer Models by Reordering their Sublayers",
                "credit": "Ofir Press, Noah A. Smith, Omer Levy"
            }
        },
        {
            "start_date": {
                "year": 2019,
                "month": 11,
                "day": 10
            },
            "text": {
                "headline": "CamemBERT",
                "text": "<p>Pretrained language models are now ubiquitous in Natural Language Processing. Despite their success, most available models have either been trained on English data or on the concatenation of data in multiple languages. This makes practical use of such models --in all languages except English-- very limited. In this paper, we investigate the feasibility of training monolingual Transformer-based language models for other languages, taking French as an example and evaluating our language models on part-of-speech tagging, dependency parsing, named entity recognition and natural language inference tasks. We show that the use of web crawled data is preferable to the use of Wikipedia data. More surprisingly, we show that a relatively small web crawled dataset (4GB) leads to results that are as good as those obtained using larger datasets (130+GB). Our best performing model CamemBERT reaches or improves the state of the art in all four downstream tasks.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1911.03894",
                "caption": "CamemBERT: a Tasty French Language Model",
                "credit": "Louis Martin, Benjamin Muller, Pedro Javier Ortiz Suárez, Yoann Dupont, Laurent Romary, Éric Villemonte de la Clergerie, Djamé Seddah, Benoît Sagot"
            }
        },
        {
            "start_date": {
                "year": 2019,
                "month": 11,
                "day": 5
            },
            "text": {
                "headline": "XLM-RoBERTa",
                "text": "<p>This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +14.6% average accuracy on XNLI, +13% average F1 score on MLQA, and +2.4% F1 score on NER. XLM-R performs particularly well on low-resource languages, improving 15.7% in XNLI accuracy for Swahili and 11.4% for Urdu over previous XLM models. We also present a detailed empirical analysis of the key factors that are required to achieve these gains, including the trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource languages at scale. Finally, we show, for the first time, the possibility of multilingual modeling without sacrificing per-language performance; XLM-R is very competitive with strong monolingual models on the GLUE and XNLI benchmarks. We will make our code, data and models publicly available.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1911.02116",
                "caption": "Unsupervised Cross-lingual Representation Learning at Scale",
                "credit": "Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, Veselin Stoyanov\n"
            }
        },
        {
            "start_date": {
                "year": 2019,
                "month": 11,
                "day": 1
            },
            "text": {
                "headline": "Base LM + kNN LM + Continuous Cache",
                "text": "<p>We introduce kNN-LMs, which extend a pre-trained neural language model (LM) by linearly interpolating it with a k-nearest neighbors (kNN) model. The nearest neighbors are computed according to distance in the pre-trained LM embedding space, and can be drawn from any text collection, including the original LM training data. Applying this augmentation to a strong Wikitext-103 LM, with neighbors drawn from the original training set, our kNN-LM achieves a new state-of-the-art perplexity of 15.79 - a 2.9 point improvement with no additional training. We also show that this approach has implications for efficiently scaling up to larger training sets and allows for effective domain adaptation, by simply varying the nearest neighbor datastore, again without further training. Qualitatively, the model is particularly helpful in predicting rare patterns, such as factual knowledge. Together, these results strongly suggest that learning similarity between sequences of text is easier than predicting the next word, and that nearest neighbor search is an effective approach for language modeling in the long tail.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1911.00172",
                "caption": "Generalization through Memorization: Nearest Neighbor Language Models",
                "credit": "Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, Mike Lewis"
            }
        },
        {
            "start_date": {
                "year": 2019,
                "month": 10,
                "day": 30
            },
            "text": {
                "headline": "AlphaStar",
                "text": "<p>Many real-world applications require artificial agents to compete and coordinate with other agents in complex environments. As a stepping stone to this goal, the domain of StarCraft has emerged as an important challenge for artificial intelligence research, owing to its iconic and enduring status among the most difficult professional esports and its relevance to the real world in terms of its raw complexity and multi-agent challenges. Over the course of a decade and numerous competitions1–3, the strongest agents have simplified important aspects of the game, utilized superhuman capabilities, or employed hand-crafted sub-systems4. Despite these advantages, no previous agent has come close to matching the overall skill of top StarCraft players. We chose to address the challenge of StarCraft using generalpurpose learning methods that are in principle applicable to other complex domains: a multi-agent reinforcement learning algorithm that uses data from both human and agent games within a diverse league of continually adapting strategies and counter-strategies, each represented by deep neural networks5,6. We evaluated our agent, AlphaStar, in the full game of StarCraft II, through a series of online games against human players. AlphaStar was rated at Grandmaster level for all three StarCraft races and above 99.8% of officially ranked human players.</p>"
            },
            "media": {
                "url": "https://www.deepmind.com/blog/alphastar-grandmaster-level-in-starcraft-ii-using-multi-agent-reinforcement-learning",
                "caption": "Grandmaster level in StarCraft II using multi-agent reinforcement learning",
                "credit": "Oriol Vinyals,Igor Babuschkin,Wojciech M. Czarnecki,Michaël Mathieu,Andrew Dudzik,Junyoung Chung,David H. Choi,Richard Powell,Timo Ewalds,Petko Georgiev,Junhyuk Oh,Dan Horgan,Manuel Kroiss,Ivo Danihelka,Aja Huang,Laurent Sifre,Trevor Cai,John P. Agapiou,Max Jaderberg,Alexander S. Vezhnevets,Rémi Leblond,Tobias Pohlen,Valentin Dalibard,David Budden,Yury Sulsky,James Molloy,Tom L. Paine,Caglar Gulcehre,Ziyu Wang,Tobias Pfaff,Yuhuai Wu,Roman Ring,Dani Yogatama,Dario Wünsch,Katrina McKinney,Oliver Smith,Tom Schaul,Timothy Lillicrap,Koray Kavukcuoglu,Demis Hassabis,Chris Apps,David Silver"
            }
        },
        {
            "start_date": {
                "year": 2019,
                "month": 10,
                "day": 29
            },
            "text": {
                "headline": "BART-large",
                "text": "<p>We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of the original sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa with comparable training resources on GLUE and SQuAD, achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 6 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also report ablation experiments that replicate other pretraining schemes within the BART framework, to better measure which factors most influence end-task performance.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1910.13461",
                "caption": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension",
                "credit": "Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, Luke Zettlemoyer"
            }
        },
        {
            "start_date": {
                "year": 2019,
                "month": 10,
                "day": 23
            },
            "text": {
                "headline": "T5-11B",
                "text": "<p>Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ``Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1910.10683",
                "caption": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
                "credit": "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu"
            }
        },
        {
            "start_date": {
                "year": 2019,
                "month": 10,
                "day": 23
            },
            "text": {
                "headline": "T5-3B",
                "text": "<p>Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ``Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1910.10683",
                "caption": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
                "credit": "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu"
            }
        },
        {
            "start_date": {
                "year": 2019,
                "month": 10,
                "day": 11
            },
            "text": {
                "headline": "M4-50B",
                "text": "<p>Over the last few years there has been enormous progress in the quality of machine translation (MT) systems, breaking language barriers around the world thanks to the developments in neural machine translation (NMT). The success of NMT however, owes largely to the great amounts of supervised training data. But what about languages where data is scarce, or even absent? Multilingual NMT, with the inductive bias that “the learning signal from one language should benefit the quality of translation to other languages”, is a potential remedy.</p>"
            },
            "media": {
                "url": "https://blog.research.google/2019/10/exploring-massively-multilingual.html",
                "caption": "Exploring Massively Multilingual, Massive Neural Machine Translation",
                "credit": "Ankur Bapna, Orhan Firat"
            }
        },
        {
            "start_date": {
                "year": 2019,
                "month": 10,
                "day": 2
            },
            "text": {
                "headline": "DistilBERT",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1910.01108",
                "caption": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter",
                "credit": "Victor Sanh, Lysandre Debut, Julien Chaumond, Thomas Wolf"
            }
        },
        {
            "start_date": {
                "year": 2019,
                "month": 10,
                "day": 2
            },
            "text": {
                "headline": "AlphaX-1",
                "text": "<p>Neural Architecture Search (NAS) has shown great success in automating the design of neural networks, but the prohibitive amount of computations behind current NAS methods requires further investigations in improving the sample efficiency and the network evaluation cost to get better results in a shorter time. In this paper, we present a novel scalable Monte Carlo Tree Search (MCTS) based NAS agent, named AlphaX, to tackle these two aspects. AlphaX improves the search efficiency by adaptively balancing the exploration and exploitation at the state level, and by a Meta-Deep Neural Network (DNN) to predict network accuracies for biasing the search toward a promising region. To amortize the network evaluation cost, AlphaX accelerates MCTS rollouts with a distributed design and reduces the number of epochs in evaluating a network by transfer learning guided with the tree structure in MCTS. In 12 GPU days and 1000 samples, AlphaX found an architecture that reaches 97.84\\% top-1 accuracy on CIFAR-10, and 75.5\\% top-1 accuracy on ImageNet, exceeding SOTA NAS methods in both the accuracy and sampling efficiency. Particularly, we also evaluate AlphaX on NASBench-101, a large scale NAS dataset; AlphaX is 3x and 2.8x more sample efficient than Random Search and Regularized Evolution in finding the global optimum. Finally, we show the searched architecture improves a variety of vision applications from Neural Style Transfer, to Image Captioning and Object Detection.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1903.11059",
                "caption": "AlphaX: eXploring Neural Architectures with Deep Neural Networks and Monte Carlo Tree Search",
                "credit": "Linnan Wang, Yiyang Zhao, Yuu Jinnai, Yuandong Tian, Rodrigo Fonseca1"
            }
        },
        {
            "start_date": {
                "year": 2019,
                "month": 9,
                "day": 26
            },
            "text": {
                "headline": "ALBERT",
                "text": "<p>Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations and longer training times. To address these problems, we present two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT. Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the original BERT. We also use a self-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks with multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and \\squad benchmarks while having fewer parameters compared to BERT-large. The code and the pretrained models are available at this https URL.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1909.11942",
                "caption": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations",
                "credit": "Z Lan, M Chen, S Goodman, K Gimpel"
            }
        },
        {
            "start_date": {
                "year": 2019,
                "month": 9,
                "day": 25
            },
            "text": {
                "headline": "Adaptive Inputs + LayerDrop",
                "text": "<p>Overparameterized transformer networks have obtained state of the art results in various natural language processing tasks, such as machine translation, language modeling, and question answering. These models contain hundreds of millions of parameters, necessitating a large amount of computation and making them prone to overfitting. In this work, we explore LayerDrop, a form of structured dropout, which has a regularization effect during training and allows for efficient pruning at inference time. In particular, we show that it is possible to select sub-networks of any depth from one large network without having to finetune them and with limited impact on performance. We demonstrate the effectiveness of our approach by improving the state of the art on machine translation, language modeling, summarization, question answering, and language understanding benchmarks. Moreover, we show that our approach leads to small BERT-like models of higher quality compared to training from scratch or using distillation.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1909.11556",
                "caption": "Reducing Transformer Depth on Demand with Structured Dropout",
                "credit": "Angela Fan, Edouard Grave, Armand Joulin"
            }
        },
        {
            "start_date": {
                "year": 2019,
                "month": 9,
                "day": 17
            },
            "text": {
                "headline": "Megatron-LM (8.3B)",
                "text": "<p>Recent work in language modeling demonstrates that training large transformer models advances the state of the art in Natural Language Processing applications. However, very large models can be quite difficult to train due to memory constraints. In this work, we present our techniques for training very large transformer models and implement a simple, efficient intra-layer model parallel approach that enables training transformer models with billions of parameters. Our approach does not require a new compiler or library changes, is orthogonal and complimentary to pipeline model parallelism, and can be fully implemented with the insertion of a few communication operations in native PyTorch. We illustrate this approach by converging transformer based models up to 8.3 billion parameters using 512 GPUs. We sustain 15.1 PetaFLOPs across the entire application with 76% scaling efficiency when compared to a strong single GPU baseline that sustains 39 TeraFLOPs, which is 30% of peak FLOPs. To demonstrate that large language models can further advance the state of the art (SOTA), we train an 8.3 billion parameter transformer language model similar to GPT-2 and a 3.9 billion parameter model similar to BERT. We show that careful attention to the placement of layer normalization in BERT-like models is critical to achieving increased performance as the model size grows. Using the GPT-2 model we achieve SOTA results on the WikiText103 (10.8 compared to SOTA perplexity of 15.8) and LAMBADA (66.5% compared to SOTA accuracy of 63.2%) datasets. Our BERT model achieves SOTA results on the RACE dataset (90.9% compared to SOTA accuracy of 89.4%).</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1909.08053",
                "caption": "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism",
                "credit": "Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, Bryan Catanzaro"
            }
        },
        {
            "start_date": {
                "year": 2019,
                "month": 9,
                "day": 17
            },
            "text": {
                "headline": "Megatron-BERT",
                "text": "<p>Recent work in language modeling demonstrates that training large transformer models advances the state of the art in Natural Language Processing applications. However, very large models can be quite difficult to train due to memory constraints. In this work, we present our techniques for training very large transformer models and implement a simple, efficient intra-layer model parallel approach that enables training transformer models with billions of parameters. Our approach does not require a new compiler or library changes, is orthogonal and complimentary to pipeline model parallelism, and can be fully implemented with the insertion of a few communication operations in native PyTorch. We illustrate this approach by converging transformer based models up to 8.3 billion parameters using 512 GPUs. We sustain 15.1 PetaFLOPs across the entire application with 76% scaling efficiency when compared to a strong single GPU baseline that sustains 39 TeraFLOPs, which is 30% of peak FLOPs. To demonstrate that large language models can further advance the state of the art (SOTA), we train an 8.3 billion parameter transformer language model similar to GPT-2 and a 3.9 billion parameter model similar to BERT. We show that careful attention to the placement of layer normalization in BERT-like models is critical to achieving increased performance as the model size grows. Using the GPT-2 model we achieve SOTA results on the WikiText103 (10.8 compared to SOTA perplexity of 15.8) and LAMBADA (66.5% compared to SOTA accuracy of 63.2%) datasets. Our BERT model achieves SOTA results on the RACE dataset (90.9% compared to SOTA accuracy of 89.4%).</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1909.08053",
                "caption": "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism",
                "credit": "Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, Bryan Catanzaro"
            }
        },
        {
            "start_date": {
                "year": 2019,
                "month": 9,
                "day": 6
            },
            "text": {
                "headline": "ResNet-152 + ObjectNet",
                "text": "<p>We collect a large real-world test set, ObjectNet, for object recognition with controls where object backgrounds, rotations, and imaging viewpoints are random. Most scientific experiments have controls, confounds which are removed from the data, to ensure that subjects cannot perform a task by exploiting trivial correlations in the data. Historically, large machine learning and computer vision datasets have lacked such controls. This has resulted in models that must be fine-tuned for new datasets and perform better on datasets than in real-world applications. When tested on ObjectNet, object detectors show a 40-45% drop in performance, with respect to their performance on other benchmarks, due to the controls for biases. Controls make ObjectNet robust to fine-tuning showing only small performance increases. We develop a highly automated platform that enables gathering datasets with controls by crowdsourcing image capturing and annotation. ObjectNet is the same size as the ImageNet test set (50,000 images), and by design does not come paired with a training set in order to encourage generalization. The dataset is both easier than ImageNet – objects are largely centered and unoccluded – and harder, due to the controls. Although we focus on object recognition here, data with controls can be gathered at scale using automated tools throughout machine learning to generate datasets that exercise models in new ways thus providing valuable feedback to researchers. This work opens up new avenues for research in generalizable, robust, and more human-like computer vision and in creating datasets where results are predictive of real-world performance.</p>"
            },
            "media": {
                "url": "https://papers.nips.cc/paper/2019/file/97af07a14cacba681feacf3012730892-Paper.pdf",
                "caption": "Objectnet: A large-scale bias-controlled dataset for pushing the limits of object recognition models",
                "credit": "Andrei Barbu, David Mayo, Julian Alverio, William Luo, Christopher Wang, Dan Gutfre- und, Josh Tenenbaum, and Boris Katz"
            }
        },
        {
            "start_date": {
                "year": 2019,
                "month": 9,
                "day": 4
            },
            "text": {
                "headline": "UDSMProt",
                "text": "<p>Motivation: Inferring the properties of a protein from its amino acid sequence is one of the key problems in bioinformatics. Most state-of-the-art approaches for protein classification tasks are tailored to single classi- fication tasks and rely on handcrafted features such as position-specific-scoring matrices from expensive database searches. We argue that this level of performance can be reached or even be surpassed by learning a task-agnostic representation once, using self-supervised language modeling, and transferring it to specific tasks by a simple finetuning step.\nResults: We put forward a universal deep sequence model that is pretrained on unlabeled protein se- quences from Swiss-Prot and finetuned on protein classification tasks. We apply it to three prototypical tasks, namely enzyme class prediction, gene ontology prediction and remote homology and fold detection. The proposed method performs on par with state-of-the-art algorithms that were tailored to these specific tasks or, for two out of three tasks, even outperforms them. These results stress the possibility of inferring protein properties from the sequence alone and, on more general grounds, the prospects of modern natural language processing methods in omics.</p>"
            },
            "media": {
                "url": "https://www.biorxiv.org/content/10.1101/704874v2.full.pdf",
                "caption": "UDSMProt: Universal Deep Sequence Models for Protein Classification",
                "credit": "Nils Strodthoff, Patrick Wagner, Markus Wenzel, and Wojciech Samek"
            }
        },
        {
            "start_date": {
                "year": 2019,
                "month": 9,
                "day": 4
            },
            "text": {
                "headline": "Mogrifier (d2, MoS2, MC) + dynamic eval",
                "text": "<p>Many advances in Natural Language Processing have been based upon more expressive models for how inputs interact with the context in which they occur. Recurrent networks, which have enjoyed a modicum of success, still lack the generalization and systematicity ultimately required for modelling language. In this work, we propose an extension to the venerable Long Short-Term Memory in the form of mutual gating of the current input and the previous output. This mechanism affords the modelling of a richer space of interactions between inputs and their context. Equivalently, our model can be viewed as making the transition function given by the LSTM context-dependent. Experiments demonstrate markedly improved generalization on language modelling in the range of 3-4 perplexity points on Penn Treebank and Wikitext-2, and 0.01-0.05 bpc on four character-based datasets. We establish a new state of the art on all datasets with the exception of Enwik8, where we close a large gap between the LSTM and Transformer models.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1909.01792",
                "caption": "Mogrifier LSTM",
                "credit": "Gábor Melis, Tomáš Kočiský, Phil Blunsom"
            }
        },
        {
            "start_date": {
                "year": 2019,
                "month": 7,
                "day": 22
            },
            "text": {
                "headline": "EN^2AS with performance reward",
                "text": "<p>One-Shot Neural architecture search (NAS) attracts broad attention recently due to its capacity to reduce the computational hours through weight sharing. However, extensive experiments on several recent works show that there is no positive correlation between the validation accuracy with inherited weights from the supernet and the test accuracy after re-training for One-Shot NAS. Different from devising a controller to find the best performing architecture with inherited weights, this paper focuses on how to sample architectures to train the supernet to make it more predictive. A single-path supernet is adopted, where only a small part of weights are optimized in each step, to reduce the memory demand greatly. Furthermore, we abandon devising complicated reward based architecture sampling controller, and sample architectures to train supernet based on novelty search. An efficient novelty search method for NAS is devised in this paper, and extensive experiments demonstrate the effectiveness and efficiency of our novelty search based architecture sampling method. The best architecture obtained by our algorithm with the same search space achieves the state-of-the-art test error rate of 2.51\\% on CIFAR-10 with only 7.5 hours search time in a single GPU, and a validation perplexity of 60.02 and a test perplexity of 57.36 on PTB. We also transfer these search cell structures to larger datasets ImageNet and WikiText-2, respectively.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1907.09109",
                "caption": "Efficient Novelty-Driven Neural Architecture Search",
                "credit": "Miao Zhang, Huiqi Li, Shirui Pan, Taoping Liu, Steven Su"
            }
        },
        {
            "start_date": {
                "year": 2019,
                "month": 7,
                "day": 11
            },
            "text": {
                "headline": "Pluribus",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://www.science.org/cms/asset/910714a7-ee2a-486e-9970-42fb893b08d9/pap.pdf",
                "caption": "Superhuman AI for multiplayer poker",
                "credit": "Noam Brown, Tuomas Sandholm"
            }
        },
        {
            "start_date": {
                "year": 2019,
                "month": 7,
                "day": 4
            },
            "text": {
                "headline": "BigBiGAN",
                "text": "<p>Adversarially trained generative models (GANs) have recently achieved compelling image synthesis results. But despite early successes in using GANs for unsupervised representation learning, they have since been superseded by approaches based on self-supervision. In this work we show that progress in image generation quality translates to substantially improved representation learning performance. Our approach, BigBiGAN, builds upon the state-of-the-art BigGAN model, extending it to representation learning by adding an encoder and modifying the discriminator. We extensively evaluate the representation learning and generation capabilities of these BigBiGAN models, demonstrating that these generation-based models achieve the state of the art in unsupervised representation learning on ImageNet, as well as in unconditional image generation. Pretrained BigBiGAN models -- including image generators and encoders -- are available on TensorFlow Hub (this https URL).</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1907.02544",
                "caption": "Large Scale Adversarial Representation Learning",
                "credit": "Jeff Donahue, Karen Simonyan"
            }
        },
        {
            "start_date": {
                "year": 2019,
                "month": 7,
                "day": 1
            },
            "text": {
                "headline": "RoBERTa Large",
                "text": "<p>Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1907.11692",
                "caption": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
                "credit": "Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov"
            }
        },
        {
            "start_date": {
                "year": 2019,
                "month": 6,
                "day": 24
            },
            "text": {
                "headline": "Tensorized Transformer (257M)",
                "text": "<p>Latest development of neural models has connected the encoder and decoder through a self-attention mechanism. In particular, Transformer, which is solely based on self-attention, has led to breakthroughs in Natural Language Processing (NLP) tasks. However, the multi-head attention mechanism, as a key component of Transformer, limits the effective deployment of the model to a resource-limited setting. In this paper, based on the ideas of tensor decomposition and parameters sharing, we propose a novel self-attention model (namely Multi-linear attention) with Block-Term Tensor Decomposition (BTD). We test and verify the proposed attention method on three language modeling tasks (i.e., PTB, WikiText-103 and One-billion) and a neural machine translation task (i.e., WMT-2016 English-German). Multi-linear attention can not only largely compress the model parameters but also obtain performance improvements, compared with a number of language modeling approaches, such as Transformer, Transformer-XL, and Transformer with tensor train decomposition.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1906.09777",
                "caption": "A Tensorized Transformer for Language Modeling",
                "credit": "Xindian Ma, Peng Zhang, Shuai Zhang, Nan Duan, Yuexian Hou, Ming Zhou, Dawei Song"
            }
        },
        {
            "start_date": {
                "year": 2019,
                "month": 6,
                "day": 19
            },
            "text": {
                "headline": "Walking Minotaur robot",
                "text": "<p>Deep reinforcement learning (deep RL) holds the promise of automating the acquisition of complex controllers that can map sensory inputs directly to low-level actions. In the domain of robotic locomotion, deep RL could enable learning locomotion skills with minimal engineering and without an explicit model of the robot dynamics. Unfortunately, applying deep RL to real-world robotic tasks is exceptionally difficult, primarily due to poor sample complexity and sensitivity to hyperparameters. While hyperparameters can be easily tuned in simulated domains, tuning may be prohibitively expensive on physical systems, such as legged robots, that can be damaged through extensive trial-and-error learning. In this paper, we propose a sample-efficient deep RL algorithm based on maximum entropy RL that requires minimal per-task tuning and only a modest number of trials to learn neural network policies. We apply this method to learning walking gaits on a real-world Minitaur robot. Our method can acquire a stable gait from scratch directly in the real world in about two hours, without relying on any model or simulation, and the resulting policy is robust to moderate variations in the environment. We further show that our algorithm achieves state-of-the-art performance on simulated benchmarks with a single set of hyperparameters. Videos of training and the learned policy can be found on the project website.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1812.11103",
                "caption": "Learning to Walk via Deep Reinforcement Learning",
                "credit": "Tuomas Haarnoja, Sehoon Ha, Aurick Zhou, Jie Tan, George Tucker, Sergey Levine"
            }
        },
        {
            "start_date": {
                "year": 2019,
                "month": 6,
                "day": 17
            },
            "text": {
                "headline": "LaNet-L (CIFAR-10)",
                "text": "<p>Neural Architecture Search (NAS) has emerged as a promising technique for automatic neural network design. However, existing MCTS based NAS approaches often utilize manually designed action space, which is not directly related to the performance metric to be optimized (e.g., accuracy), leading to sample-inefficient explorations of architectures. To improve the sample efficiency, this paper proposes Latent Action Neural Architecture Search (LaNAS), which learns actions to recursively partition the search space into good or bad regions that contain networks with similar performance metrics. During the search phase, as different action sequences lead to regions with different performance, the search efficiency can be significantly improved by biasing towards the good regions. On three NAS tasks, empirical results demonstrate that LaNAS is at least an order more sample efficient than baseline methods including evolutionary algorithms, Bayesian optimizations, and random search. When applied in practice, both one-shot and regular LaNAS consistently outperform existing results. Particularly, LaNAS achieves 99.0% accuracy on CIFAR-10 and 80.8% top1 accuracy at 600 MFLOPS on ImageNet in only 800 samples, significantly outperforming AmoebaNet with 33x fewer samples. Our code is publicly available at this https URL.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1906.06832",
                "caption": "Sample-Efficient Neural Architecture Search by Learning Action Space",
                "credit": "Linnan Wang, Saining Xie, Teng Li, Rodrigo Fonseca, Yuandong Tian"
            }
        },
        {
            "start_date": {
                "year": 2019,
                "month": 6,
                "day": 15
            },
            "text": {
                "headline": "PG-SWGAN",
                "text": "<p>In generative modeling, the Wasserstein distance (WD) has emerged as a useful metric to measure the discrepancy between generated and real data distributions. Unfortunately, it is challenging to approximate the WD of high-dimensional distributions. In contrast, the sliced Wasserstein distance (SWD) factorizes high-dimensional distributions into their multiple one-dimensional marginal distributions and is thus easier to approximate. In this paper, we introduce novel approximations of the primal and dual SWD. Instead of using a large number of random projections, as it is done by conventional SWD approximation methods, we propose to approximate SWDs with a small number of parameterized orthogonal projections in an end-to-end deep learning fashion. As concrete applications of our SWD approximations, we design two types of differentiable SWD blocks to equip modern generative frameworks---Auto-Encoders (AE) and Generative Adversarial Networks (GAN). In the experiments, we not only show the superiority of the proposed generative models on standard image synthesis benchmarks, but also demonstrate the state-of-the-art performance on challenging high resolution image and video generation in an unsupervised manner.</p>"
            },
            "media": {
                "url": "https://openaccess.thecvf.com/content_CVPR_2019/html/Wu_Sliced_Wasserstein_Generative_Models_CVPR_2019_paper.html",
                "caption": "Sliced Wasserstein Generative Models",
                "credit": "Jiqing Wu, Zhiwu Huang, Dinesh Acharya, Wen Li, Janine Thoma, Danda Pani Paudel, Luc Van Gool"
            }
        },
        {
            "start_date": {
                "year": 2019,
                "month": 6,
                "day": 14
            },
            "text": {
                "headline": "FixRes ResNeXt-101 WSL",
                "text": "<p>Data-augmentation is key to the training of neural networks for image classification. This paper first shows that existing augmentations induce a significant discrepancy between the typical size of the objects seen by the classifier at train and test time. We experimentally validate that, for a target test resolution, using a lower train resolution offers better classification at test time.\nWe then propose a simple yet effective and efficient strategy to optimize the classifier performance when the train and test resolutions differ. It involves only a computationally cheap fine-tuning of the network at the test resolution. This enables training strong classifiers using small training images. For instance, we obtain 77.1% top-1 accuracy on ImageNet with a ResNet-50 trained on 128x128 images, and 79.8% with one trained on 224x224 image. In addition, if we use extra training data we get 82.5% with the ResNet-50 train with 224x224 images.\nConversely, when training a ResNeXt-101 32x48d pre-trained in weakly-supervised fashion on 940 million public images at resolution 224x224 and further optimizing for test resolution 320x320, we obtain a test top-1 accuracy of 86.4% (top-5: 98.0%) (single-crop). To the best of our knowledge this is the highest ImageNet single-crop, top-1 and top-5 accuracy to date.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1906.06423",
                "caption": "Fixing the train-test resolution discrepancy",
                "credit": "Hugo Touvron, Andrea Vedaldi, Matthijs Douze, Hervé Jégou"
            }
        },
        {
            "start_date": {
                "year": 2019,
                "month": 6,
                "day": 13
            },
            "text": {
                "headline": "Char-CNN-BiLSTM",
                "text": "<p>Speech processing systems rely on robust feature extraction to handle phonetic and semantic variations found in natural language. While techniques exist for desensitizing features to common noise patterns produced by Speech-to-Text (STT) and Text-to-Speech (TTS) systems, the question remains how to best leverage state-of-the-art language models (which capture rich semantic features, but are trained on only written text) on inputs with ASR errors. In this paper, we present Telephonetic, a data augmentation framework that helps robustify language model features to ASR corrupted inputs. To capture phonetic alterations, we employ a character-level language model trained using probabilistic masking. Phonetic augmentations are generated in two stages: a TTS encoder (Tacotron 2, WaveGlow) and a STT decoder (DeepSpeech). Similarly, semantic perturbations are produced by sampling from nearby words in an embedding space, which is computed using the BERT language model. Words are selected for augmentation according to a hierarchical grammar sampling strategy. Telephonetic is evaluated on the Penn Treebank (PTB) corpus, and demonstrates its effectiveness as a bootstrapping technique for transferring neural language models to the speech domain. Notably, our language model achieves a test perplexity of 37.49 on PTB, which to our knowledge is state-of-the-art among models trained only on PTB.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1906.05678",
                "caption": "Telephonetic: Making Neural Language Models Robust to ASR and Semantic Noise",
                "credit": "Chris Larson, Tarek Lahlou, Diana Mingels, Zachary Kulis, Erik Mueller"
            }
        },
        {
            "start_date": {
                "year": 2019,
                "month": 6,
                "day": 10
            },
            "text": {
                "headline": "AWD-LSTM + MoS + Partial Shuffled",
                "text": "<p>Recently, substantial progress has been made in language modeling by using deep neural networks. However, in practice, large scale neural language models have been shown to be prone to overfitting. In this paper, we present a simple yet highly effective adversarial training mechanism for regularizing neural language models. The idea is to introduce adversarial noise to the output embedding layer while training the models. We show that the optimal adversarial noise yields a simple closed-form solution, thus allowing us to develop a simple and time efficient algorithm. Theoretically, we show that our adversarial mechanism effectively encourages the diversity of the embedding vectors, helping to increase the robustness of models. Empirically, we show that our method improves on the single model state-of-the-art results for language modeling on Penn Treebank (PTB) and Wikitext-2, achieving test perplexity scores of 46.01 and 38.07, respectively. When applied to machine translation, our method improves over various transformer-based translation baselines in BLEU scores on the WMT14 English-German and IWSLT14 German-English tasks.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1906.03805",
                "caption": "Improving Neural Language Modeling via Adversarial Training",
                "credit": "Dilin Wang, Chengyue Gong, Qiang Liu"
            }
        },
        {
            "start_date": {
                "year": 2019,
                "month": 6,
                "day": 4
            },
            "text": {
                "headline": "Transformer-XL Large + Phrase Induction",
                "text": "<p>Common language models typically predict the next word given the context. In this work, we propose a method that improves language modeling by learning to align the given context and the following phrase. The model does not require any linguistic annotation of phrase segmentation. Instead, we define syntactic heights and phrase segmentation rules, enabling the model to automatically induce phrases, recognize their task-specific heads, and generate phrase embeddings in an unsupervised learning manner. Our method can easily be applied to language models with different network architectures since an independent module is used for phrase induction and context-phrase alignment, and no change is required in the underlying language modeling network. Experiments have shown that our model outperformed several strong baseline models on different data sets. We achieved a new state-of-the-art performance of 17.4 perplexity on the Wikitext-103 dataset. Additionally, visualizing the outputs of the phrase induction module showed that our model is able to learn approximate phrase-level structural knowledge without any annotation.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1906.01702",
                "caption": "Improving Neural Language Models by Segmenting, Attending, and Predicting the Future",
                "credit": "Hongyin Luo, Lan Jiang, Yonatan Belinkov, James Glass"
            }
        },
        {
            "start_date": {
                "year": 2019,
                "month": 6,
                "day": 3
            },
            "text": {
                "headline": "AMDIM",
                "text": "<p>We propose an approach to self-supervised representation learning based on maximizing mutual information between features extracted from multiple views of a shared context. For example, one could produce multiple views of a local spatio-temporal context by observing it from different locations (e.g., camera positions within a scene), and via different modalities (e.g., tactile, auditory, or visual). Or, an ImageNet image could provide a context from which one produces multiple views by repeatedly applying data augmentation. Maximizing mutual information between features extracted from these views requires capturing information about high-level factors whose influence spans multiple views -- e.g., presence of certain objects or occurrence of certain events.\nFollowing our proposed approach, we develop a model which learns image representations that significantly outperform prior methods on the tasks we consider. Most notably, using self-supervised learning, our model learns representations which achieve 68.1% accuracy on ImageNet using standard linear evaluation. This beats prior results by over 12% and concurrent results by 7%. When we extend our model to use mixture-based representations, segmentation behaviour emerges as a natural side-effect. Our code is available online: this https URL.\n</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1906.00910",
                "caption": "Learning Representations by Maximizing Mutual Information Across Views",
                "credit": "Philip Bachman, R Devon Hjelm, William Buchwalter"
            }
        },
        {
            "start_date": {
                "year": 2019,
                "month": 6,
                "day": 1
            },
            "text": {
                "headline": "XLNet",
                "text": "<p>With the capability of modeling bidirectional contexts, denoising autoencoding based pretraining like BERT achieves better performance than pretraining approaches based on autoregressive language modeling. However, relying on corrupting the input with masks, BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy. In light of these pros and cons, we propose XLNet, a generalized autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and (2) overcomes the limitations of BERT thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into pretraining. Empirically, under comparable experiment settings, XLNet outperforms BERT on 20 tasks, often by a large margin, including question answering, natural language inference, sentiment analysis, and document ranking.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1906.08237",
                "caption": "XLNet: Generalized Autoregressive Pretraining for Language Understanding",
                "credit": "Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, Quoc V. Le"
            }
        },
        {
            "start_date": {
                "year": 2019,
                "month": 6,
                "day": 1
            },
            "text": {
                "headline": "XLM",
                "text": "<p>Recent studies have demonstrated the efficiency of generative pretraining for English natural language understanding. In this work, we extend this approach to multiple languages and show the effectiveness of cross-lingual pretraining. We propose two methods to learn cross-lingual language models (XLMs): one unsupervised that only relies on monolingual data, and one supervised that leverages parallel data with a new cross-lingual language model objective. We obtain state-of-the-art results on cross-lingual classification, unsupervised and supervised machine translation. On XNLI, our approach pushes the state of the art by an absolute gain of 4.9% accuracy. On unsupervised machine translation, we obtain 34.3 BLEU on WMT'16 German-English, improving the previous state of the art by more than 9 BLEU. On supervised machine translation, we obtain a new state of the art of 38.5 BLEU on WMT'16 Romanian-English, outperforming the previous best approach by more than 4 BLEU. Our code and pretrained models will be made publicly available.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1901.07291",
                "caption": "Cross-lingual Language Model Pretraining",
                "credit": "Guillaume Lample, Alexis Conneau"
            }
        },
        {
            "start_date": {
                "year": 2019,
                "month": 5,
                "day": 31
            },
            "text": {
                "headline": "DLRM-2020",
                "text": "<p>With the advent of deep learning, neural network-based recommendation models have emerged as an important tool for tackling personalization and recommendation tasks. These networks differ significantly from other deep learning networks due to their need to handle categorical features and are not well studied or understood. In this paper, we develop a state-of-the-art deep learning recommendation model (DLRM) and provide its implementation in both PyTorch and Caffe2 frameworks. In addition, we design a specialized parallelization scheme utilizing model parallelism on the embedding tables to mitigate memory constraints while exploiting data parallelism to scale-out compute from the fully-connected layers. We compare DLRM against existing recommendation models and characterize its performance on the Big Basin AI platform, demonstrating its usefulness as a benchmark for future algorithmic experimentation and system co-design.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1906.00091",
                "caption": "Deep Learning Recommendation Model for Personalization and Recommendation Systems",
                "credit": "Maxim Naumov, Dheevatsa Mudigere, Hao-Jun Michael Shi, Jianyu Huang, Narayanan Sundaraman, Jongsoo Park, Xiaodong Wang, Udit Gupta, Carole-Jean Wu, Alisson G. Azzolini, Dmytro Dzhulgakov, Andrey Mallevich, Ilia Cherniavskii, Yinghai Lu, Raghuraman Krishnamoorthi, Ansha Yu, Volodymyr Kondratenko, Stephanie Pereira, Xianjie Chen, Wenlin Chen, Vijay Rao, Bill Jia, Liang Xiong, Misha Smelyanskiy"
            }
        },
        {
            "start_date": {
                "year": 2019,
                "month": 5,
                "day": 29
            },
            "text": {
                "headline": "MnasNet-A3",
                "text": "<p>Designing convolutional neural networks (CNN) for mobile devices is challenging because mobile models need to be small and fast, yet still accurate. Although significant efforts have been dedicated to design and improve mobile CNNs on all dimensions, it is very difficult to manually balance these trade-offs when there are so many architectural possibilities to consider. In this paper, we propose an automated mobile neural architecture search (MNAS) approach, which explicitly incorporate model latency into the main objective so that the search can identify a model that achieves a good trade-off between accuracy and latency. Unlike previous work, where latency is considered via another, often inaccurate proxy (e.g., FLOPS), our approach directly measures real-world inference latency by executing the model on mobile phones. To further strike the right balance between flexibility and search space size, we propose a novel factorized hierarchical search space that encourages layer diversity throughout the network. Experimental results show that our approach consistently outperforms state-of-the-art mobile CNN models across multiple vision tasks. On the ImageNet classification task, our MnasNet achieves 75.2% top-1 accuracy with 78ms latency on a Pixel phone, which is 1.8x faster than MobileNetV2 [29] with 0.5% higher accuracy and 2.3x faster than NASNet [36] with 1.2% higher accuracy. Our MnasNet also achieves better mAP quality than MobileNets for COCO object detection. Code is at this https URL</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1807.11626",
                "caption": "MnasNet: Platform-Aware Neural Architecture Search for Mobile",
                "credit": "Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler, Andrew Howard, Quoc V. Le"
            }
        },
        {
            "start_date": {
                "year": 2019,
                "month": 5,
                "day": 29
            },
            "text": {
                "headline": "MnasNet-A1 + SSDLite",
                "text": "<p>Designing convolutional neural networks (CNN) for mobile devices is challenging because mobile models need to be small and fast, yet still accurate. Although significant efforts have been dedicated to design and improve mobile CNNs on all dimensions, it is very difficult to manually balance these trade-offs when there are so many architectural possibilities to consider. In this paper, we propose an automated mobile neural architecture search (MNAS) approach, which explicitly incorporate model latency into the main objective so that the search can identify a model that achieves a good trade-off between accuracy and latency. Unlike previous work, where latency is considered via another, often inaccurate proxy (e.g., FLOPS), our approach directly measures real-world inference latency by executing the model on mobile phones. To further strike the right balance between flexibility and search space size, we propose a novel factorized hierarchical search space that encourages layer diversity throughout the network. Experimental results show that our approach consistently outperforms state-of-the-art mobile CNN models across multiple vision tasks. On the ImageNet classification task, our MnasNet achieves 75.2% top-1 accuracy with 78ms latency on a Pixel phone, which is 1.8x faster than MobileNetV2 [29] with 0.5% higher accuracy and 2.3x faster than NASNet [36] with 1.2% higher accuracy. Our MnasNet also achieves better mAP quality than MobileNets for COCO object detection. Code is at this https URL</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1807.11626",
                "caption": "MnasNet: Platform-Aware Neural Architecture Search for Mobile",
                "credit": "Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler, Andrew Howard, Quoc V. Le"
            }
        },
        {
            "start_date": {
                "year": 2019,
                "month": 5,
                "day": 28
            },
            "text": {
                "headline": "EfficientNet-L2",
                "text": "<p>Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are available. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet.\nTo go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves state-of-the-art 84.3% top-1 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet. Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7%), Flowers (98.8%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters. Source code is at this https URL.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1905.11946",
                "caption": "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks",
                "credit": "Mingxing Tan, Quoc V. Le"
            }
        },
        {
            "start_date": {
                "year": 2019,
                "month": 5,
                "day": 22
            },
            "text": {
                "headline": "CPC v2",
                "text": "<p>Human observers can learn to recognize new categories of images from a handful of examples, yet doing so with artificial ones remains an open challenge. We hypothesize that data-efficient recognition is enabled by representations which make the variability in natural signals more predictable. We therefore revisit and improve Contrastive Predictive Coding, an unsupervised objective for learning such representations. This new implementation produces features which support state-of-the-art linear classification accuracy on the ImageNet dataset. When used as input for non-linear classification with deep neural networks, this representation allows us to use 2-5x less labels than classifiers trained directly on image pixels. Finally, this unsupervised representation substantially improves transfer learning to object detection on the PASCAL VOC dataset, surpassing fully supervised pre-trained ImageNet classifiers.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1905.09272",
                "caption": "Data-Efficient Image Recognition with Contrastive Predictive Coding",
                "credit": "Olivier J. Hénaff, Aravind Srinivas, Jeffrey De Fauw, Ali Razavi, Carl Doersch, S. M. Ali Eslami, Aaron van den Oord"
            }
        },
        {
            "start_date": {
                "year": 2019,
                "month": 5,
                "day": 14
            },
            "text": {
                "headline": "AWD-LSTM-DRILL + dynamic evaluation† (WT2)",
                "text": "<p>Many tasks, including language generation, benefit from learning the structure of the output space, particularly when the space of output labels is large and the data is sparse. State-of-the-art neural language models indirectly capture the output space structure in their classifier weights since they lack parameter sharing across output labels. Learning shared output label mappings helps, but existing methods have limited expressivity and are prone to overfitting. In this paper, we investigate the usefulness of more powerful shared mappings for output labels, and propose a deep residual output mapping with dropout between layers to better capture the structure of the output space and avoid overfitting. Evaluations on three language generation tasks show that our output label mapping can match or improve state-of-the-art recurrent and self-attention architectures, and suggest that the classifier does not necessarily need to be high-rank to better model natural language if it is better at capturing the structure of the output space.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1905.05513",
                "caption": "Deep Residual Output Layers for Neural Language Generation",
                "credit": "Nikolaos Pappas, James Henderson"
            }
        },
        {
            "start_date": {
                "year": 2019,
                "month": 5,
                "day": 2
            },
            "text": {
                "headline": "ResNeXt-101 Billion-scale",
                "text": "<p>This paper presents a study of semi-supervised learning with large convolutional networks. We propose a pipeline, based on a teacher/student paradigm, that leverages a large collection of unlabelled images (up to 1 billion). Our main goal is to improve the performance for a given target architecture, like ResNet-50 or ResNext. We provide an extensive analysis of the success factors of our approach, which leads us to formulate some recommendations to produce high-accuracy models for image classification with semi-supervised learning. As a result, our approach brings important gains to standard architectures for image, video and fine-grained classification. For instance, by leveraging one billion unlabelled images, our learned vanilla ResNet-50 achieves 81.2% top-1 accuracy on the ImageNet benchmark.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1905.00546",
                "caption": "Billion-scale semi-supervised learning for image classification",
                "credit": "I. Zeki Yalniz, Hervé Jégou, Kan Chen, Manohar Paluri, Dhruv Mahajan"
            }
        },
        {
            "start_date": {
                "year": 2019,
                "month": 5,
                "day": 2
            },
            "text": {
                "headline": "ResNet-50 Billion-scale",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1905.00546",
                "caption": "Billion-scale semi-supervised learning for image classification",
                "credit": "I. Zeki Yalniz, Hervé Jégou, Kan Chen, Manohar Paluri, Dhruv Mahajan"
            }
        },
        {
            "start_date": {
                "year": 2019,
                "month": 4,
                "day": 26
            },
            "text": {
                "headline": "Neuro-Symbolic Concept Learner",
                "text": "<p>We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them; instead, our model learns by simply looking at images and reading paired questions and answers. Our model builds an object-based scene representation and translates sentences into executable, symbolic programs. To bridge the learning of two modules, we use a neuro-symbolic reasoning module that executes these programs on the latent scene representation. Analogical to human concept learning, the perception module learns visual concepts based on the language description of the object being referred to. Meanwhile, the learned visual concepts facilitate learning new words and parsing new sentences. We use curriculum learning to guide the searching over the large compositional space of images and language. Extensive experiments demonstrate the accuracy and efficiency of our model on learning visual concepts, word representations, and semantic parsing of sentences. Further, our method allows easy generalization to new object attributes, compositions, language concepts, scenes and questions, and even new program domains. It also empowers applications including visual question answering and bidirectional image-text retrieval.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1904.12584",
                "caption": "The Neuro-Symbolic Concept Learner: Interpreting Scenes, Words, and Sentences From Natural Supervision",
                "credit": "Jiayuan Mao, Chuang Gan, Pushmeet Kohli, Joshua B. Tenenbaum, Jiajun Wu"
            }
        },
        {
            "start_date": {
                "year": 2019,
                "month": 4,
                "day": 21
            },
            "text": {
                "headline": "DANet",
                "text": "<p>In this paper, we address the scene segmentation task by capturing rich contextual dependencies based on the self-attention mechanism. Unlike previous works that capture contexts by multi-scale features fusion, we propose a Dual Attention Networks (DANet) to adaptively integrate local features with their global dependencies. Specifically, we append two types of attention modules on top of traditional dilated FCN, which model the semantic interdependencies in spatial and channel dimensions respectively. The position attention module selectively aggregates the features at each position by a weighted sum of the features at all positions. Similar features would be related to each other regardless of their distances. Meanwhile, the channel attention module selectively emphasizes interdependent channel maps by integrating associated features among all channel maps. We sum the outputs of the two attention modules to further improve feature representation which contributes to more precise segmentation results. We achieve new state-of-the-art segmentation performance on three challenging scene segmentation datasets, i.e., Cityscapes, PASCAL Context and COCO Stuff dataset. In particular, a Mean IoU score of 81.5% on Cityscapes test set is achieved without using coarse data.\n</p>"
            },
            "media": {
                "url": "https://openaccess.thecvf.com/content_CVPR_2019/html/Fu_Dual_Attention_Network_for_Scene_Segmentation_CVPR_2019_paper.html",
                "caption": "Dual Attention Network for Scene Segmentation",
                "credit": "Jun Fu, Jing Liu, Haijie Tian, Yong Li, Yongjun Bao, Zhiwei Fang, Hanqing Lu"
            }
        },
        {
            "start_date": {
                "year": 2019,
                "month": 4,
                "day": 20
            },
            "text": {
                "headline": "BERT-Large-CAS (PTB+WT2+WT103)",
                "text": "<p>The Transformer architecture is superior to RNN-based models in computational efficiency. Recently, GPT and BERT demonstrate the efficacy of Transformer models on various NLP tasks using pre-trained language models on large-scale corpora. Surprisingly, these Transformer architectures are suboptimal for language model itself. Neither self-attention nor the positional encoding in the Transformer is able to efficiently incorporate the word-level sequential context crucial to language modeling.\nIn this paper, we explore effective Transformer architectures for language model, including adding additional LSTM layers to better capture the sequential context while still keeping the computation efficient. We propose Coordinate Architecture Search (CAS) to find an effective architecture through iterative refinement of the model. Experimental results on the PTB, WikiText-2, and WikiText-103 show that CAS achieves perplexities between 20.42 and 34.11 on all problems, i.e. on average an improvement of 12.0 perplexity units compared to state-of-the-art LSTMs. The source code is publicly available.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1904.09408",
                "caption": "Language Models with Transformers",
                "credit": "Chenguang Wang, Mu Li, Alexander J. Smola"
            }
        },
        {
            "start_date": {
                "year": 2019,
                "month": 4,
                "day": 18
            },
            "text": {
                "headline": "SpecAugment",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1904.08779",
                "caption": "SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition",
                "credit": " Daniel S. Park, William Chan, Yu Zhang, Chung-Cheng Chiu, Barret Zoph, Ekin D. Cubuk, Quoc V. Le"
            }
        },
        {
            "start_date": {
                "year": 2019,
                "month": 4,
                "day": 17
            },
            "text": {
                "headline": "Transformer-XL + RMS dynamic eval",
                "text": "<p>This research note combines two methods that have recently improved the state of the art in language modeling: Transformers and dynamic evaluation. Transformers use stacked layers of self-attention that allow them to capture long range dependencies in sequential data. Dynamic evaluation fits models to the recent sequence history, allowing them to assign higher probabilities to re-occurring sequential patterns. By applying dynamic evaluation to Transformer-XL models, we improve the state of the art on enwik8 from 0.99 to 0.94 bits/char, text8 from 1.08 to 1.04 bits/char, and WikiText-103 from 18.3 to 16.4 perplexity points.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1904.08378",
                "caption": "Dynamic Evaluation of Transformer Language Models",
                "credit": "Ben Krause, Emmanuel Kahembwe, Iain Murray, Steve Renals"
            }
        },
        {
            "start_date": {
                "year": 2019,
                "month": 4,
                "day": 10
            },
            "text": {
                "headline": "MEGNet (molecule model)",
                "text": "<p>Graph networks are a new machine learning (ML) paradigm that supports both relational reasoning and combinatorial generalization. Here, we develop universal MatErials Graph Network (MEGNet) models for accurate property prediction in both molecules and crystals. We demonstrate that the MEGNet models outperform prior ML models such as the SchNet in 11 out of 13 properties of the QM9 molecule data set. Similarly, we show that MEGNet models trained on ∼60,000 crystals in the Materials Project substantially outperform prior ML models in the prediction of the formation energies, band gaps and elastic moduli of crystals, achieving better than DFT accuracy over a much larger data set. We present two new strategies to address data limitations common in materials science and chemistry. First, we demonstrate a physically-intuitive approach to unify four separate molecular MEGNet models for the internal energy at 0 K and room temperature, enthalpy and Gibbs free energy into a single free energy MEGNet model by incorporating the temperature, pressure and entropy as global state inputs. Second, we show that the learned element embeddings in MEGNet models encode periodic chemical trends and can be transfer-learned from a property model trained on a larger data set (formation energies) to improve property models with smaller amounts of data (band gaps and elastic moduli).</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1812.05055",
                "caption": "Graph Networks as a Universal Machine Learning Framework for Molecules and Crystals",
                "credit": "Chi Chen, Weike Ye, Yunxing Zuo, Chen Zheng, Shyue Ping Ong"
            }
        },
        {
            "start_date": {
                "year": 2019,
                "month": 4,
                "day": 10
            },
            "text": {
                "headline": "MEGNet (crystal formation energy model)",
                "text": "<p>Graph networks are a new machine learning (ML) paradigm that supports both relational reasoning and combinatorial generalization. Here, we develop universal MatErials Graph Network (MEGNet) models for accurate property prediction in both molecules and crystals. We demonstrate that the MEGNet models outperform prior ML models such as the SchNet in 11 out of 13 properties of the QM9 molecule data set. Similarly, we show that MEGNet models trained on ∼60,000 crystals in the Materials Project substantially outperform prior ML models in the prediction of the formation energies, band gaps and elastic moduli of crystals, achieving better than DFT accuracy over a much larger data set. We present two new strategies to address data limitations common in materials science and chemistry. First, we demonstrate a physically-intuitive approach to unify four separate molecular MEGNet models for the internal energy at 0 K and room temperature, enthalpy and Gibbs free energy into a single free energy MEGNet model by incorporating the temperature, pressure and entropy as global state inputs. Second, we show that the learned element embeddings in MEGNet models encode periodic chemical trends and can be transfer-learned from a property model trained on a larger data set (formation energies) to improve property models with smaller amounts of data (band gaps and elastic moduli).</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1812.05055",
                "caption": "Graph Networks as a Universal Machine Learning Framework for Molecules and Crystals",
                "credit": "Chi Chen, Weike Ye, Yunxing Zuo, Chen Zheng, Shyue Ping Ong"
            }
        },
        {
            "start_date": {
                "year": 2019,
                "month": 4,
                "day": 10
            },
            "text": {
                "headline": "MEGNet (crystal elasticity model)",
                "text": "<p>Graph networks are a new machine learning (ML) paradigm that supports both relational reasoning and combinatorial generalization. Here, we develop universal MatErials Graph Network (MEGNet) models for accurate property prediction in both molecules and crystals. We demonstrate that the MEGNet models outperform prior ML models such as the SchNet in 11 out of 13 properties of the QM9 molecule data set. Similarly, we show that MEGNet models trained on ∼60,000 crystals in the Materials Project substantially outperform prior ML models in the prediction of the formation energies, band gaps and elastic moduli of crystals, achieving better than DFT accuracy over a much larger data set. We present two new strategies to address data limitations common in materials science and chemistry. First, we demonstrate a physically-intuitive approach to unify four separate molecular MEGNet models for the internal energy at 0 K and room temperature, enthalpy and Gibbs free energy into a single free energy MEGNet model by incorporating the temperature, pressure and entropy as global state inputs. Second, we show that the learned element embeddings in MEGNet models encode periodic chemical trends and can be transfer-learned from a property model trained on a larger data set (formation energies) to improve property models with smaller amounts of data (band gaps and elastic moduli).</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1812.05055",
                "caption": "Graph Networks as a Universal Machine Learning Framework for Molecules and Crystals",
                "credit": "Chi Chen, Weike Ye, Yunxing Zuo, Chen Zheng, Shyue Ping Ong"
            }
        },
        {
            "start_date": {
                "year": 2019,
                "month": 4,
                "day": 10
            },
            "text": {
                "headline": "MEGNet (crystal band gap model)",
                "text": "<p>Graph networks are a new machine learning (ML) paradigm that supports both relational reasoning and combinatorial generalization. Here, we develop universal MatErials Graph Network (MEGNet) models for accurate property prediction in both molecules and crystals. We demonstrate that the MEGNet models outperform prior ML models such as the SchNet in 11 out of 13 properties of the QM9 molecule data set. Similarly, we show that MEGNet models trained on ∼60,000 crystals in the Materials Project substantially outperform prior ML models in the prediction of the formation energies, band gaps and elastic moduli of crystals, achieving better than DFT accuracy over a much larger data set. We present two new strategies to address data limitations common in materials science and chemistry. First, we demonstrate a physically-intuitive approach to unify four separate molecular MEGNet models for the internal energy at 0 K and room temperature, enthalpy and Gibbs free energy into a single free energy MEGNet model by incorporating the temperature, pressure and entropy as global state inputs. Second, we show that the learned element embeddings in MEGNet models encode periodic chemical trends and can be transfer-learned from a property model trained on a larger data set (formation energies) to improve property models with smaller amounts of data (band gaps and elastic moduli).</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1812.05055",
                "caption": "Graph Networks as a Universal Machine Learning Framework for Molecules and Crystals",
                "credit": "Chi Chen, Weike Ye, Yunxing Zuo, Chen Zheng, Shyue Ping Ong"
            }
        },
        {
            "start_date": {
                "year": 2019,
                "month": 4,
                "day": 8
            },
            "text": {
                "headline": "WeNet (Penn Treebank)",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1904.03819",
                "caption": "WeNet: Weighted Networks for Recurrent Network Architecture Search",
                "credit": "Zhiheng Huang, Bing Xiang"
            }
        },
        {
            "start_date": {
                "year": 2019,
                "month": 4,
                "day": 8
            },
            "text": {
                "headline": "True-Regularization+Finetune+Dynamic-Eval",
                "text": "<p>Recurrent Neural Networks (RNNs) have dominated language modeling because of their superior performance over traditional N-gram based models. In many applications, a large Recurrent Neural Network language model (RNNLM) or an ensemble of several RNNLMs is used. These models have large memory footprints and require heavy computation. In this paper, we examine the effect of applying knowledge distillation in reducing the model size for RNNLMs. In addition, we propose a trust regularization method to improve the knowledge distillation training for RNNLMs. Using knowledge distillation with trust regularization, we reduce the parameter size to a third of that of the previously published best model while maintaining the state-of-the-art perplexity result on Penn Treebank data. In a speech recognition N-bestrescoring task, we reduce the RNNLM model size to 18.5% of the baseline system, with no degradation in word error rate(WER) performance on Wall Street Journal data set.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1904.04163",
                "caption": "Knowledge Distillation For Recurrent Neural Network Language Modeling With Trust Regularization",
                "credit": "Yangyang Shi, Mei-Yuh Hwang, Xin Lei, Haoyu Sheng"
            }
        },
        {
            "start_date": {
                "year": 2019,
                "month": 4,
                "day": 4
            },
            "text": {
                "headline": "Cross-lingual alignment",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1902.09492",
                "caption": "Cross-lingual alignment of contextual word embeddings, with applications to zero- shot dependency parsing.",
                "credit": "Tal Schuster, Ori Ram, Regina Barzilay, and Amir Globerson."
            }
        },
        {
            "start_date": {
                "year": 2019,
                "month": 4,
                "day": 1
            },
            "text": {
                "headline": "FAIRSEQ Adaptive Inputs",
                "text": "<p>fairseq is an open-source sequence modeling toolkit that allows researchers and developers to train custom models for translation, summarization, language modeling, and other text generation tasks. The toolkit is based on PyTorch and supports distributed training across multiple GPUs and machines. We also support fast mixed-precision training and inference on modern GPUs. A demo video can be found at this https URL</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1904.01038",
                "caption": "fairseq: A Fast, Extensible Toolkit for Sequence Modeling",
                "credit": "Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, Michael Auli"
            }
        },
        {
            "start_date": {
                "year": 2019,
                "month": 3,
                "day": 26
            },
            "text": {
                "headline": "SciBERT",
                "text": "<p>Obtaining large-scale annotated data for NLP tasks in the scientific domain is challenging and expensive. We release SciBERT, a pretrained language model based on BERT (Devlin et al., 2018) to address the lack of high-quality, large-scale labeled scientific data. SciBERT leverages unsupervised pretraining on a large multi-domain corpus of scientific publications to improve performance on downstream scientific NLP tasks. We evaluate on a suite of tasks including sequence tagging, sentence classification and dependency parsing, with datasets from a variety of scientific domains. We demonstrate statistically significant improvements over BERT and achieve new state-of-the-art results on several of these tasks. The code and pretrained models are available at this https://github.com/allenai/scibert/</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1903.10676",
                "caption": "SciBERT: A Pretrained Language Model for Scientific Text",
                "credit": "Iz Beltagy, Kyle Lo, Arman Cohan"
            }
        },
        {
            "start_date": {
                "year": 2019,
                "month": 2,
                "day": 28
            },
            "text": {
                "headline": "NMT Transformer 437M",
                "text": "<p>Multilingual neural machine translation (NMT) enables training a single model that supports translation from multiple source languages into multiple target languages. In this paper, we push the limits of multilingual NMT in terms of number of languages being used. We perform extensive experiments in training massively multilingual NMT models, translating up to 102 languages to and from English within a single model. We explore different setups for training such models and analyze the trade-offs between translation quality and various modeling decisions. We report results on the publicly available TED talks multilingual corpus where we show that massively multilingual many-to-many models are effective in low resource settings, outperforming the previous state-of-the-art while supporting up to 59 languages. Our experiments on a large-scale dataset with 102 languages to and from English and up to one million examples per direction also show promising results, surpassing strong bilingual baselines and encouraging future work on massively multilingual NMT.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1903.00089",
                "caption": "Massively Multilingual Neural Machine Translation",
                "credit": "Roee Aharoni, Melvin Johnson, Orhan Firat"
            }
        },
        {
            "start_date": {
                "year": 2019,
                "month": 2,
                "day": 27
            },
            "text": {
                "headline": "KataGo",
                "text": "<p>By introducing several improvements to the AlphaZero process and architecture, we greatly accelerate self-play learning in Go, achieving a 50x reduction in computation over comparable methods. Like AlphaZero and replications such as ELF OpenGo and Leela Zero, our bot KataGo only learns from neural-net-guided Monte Carlo tree search self-play. But whereas AlphaZero required thousands of TPUs over several days and ELF required thousands of GPUs over two weeks, KataGo surpasses ELF's final model after only 19 days on fewer than 30 GPUs. Much of the speedup involves non-domain-specific improvements that might directly transfer to other problems. Further gains from domain-specific techniques reveal the remaining efficiency gap between the best methods and purely general methods such as AlphaZero. Our work is a step towards making learning in state spaces as large as Go possible without large-scale computational resources.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1902.10565",
                "caption": "Accelerating Self-Play Learning in Go",
                "credit": "David J. Wu"
            }
        },
        {
            "start_date": {
                "year": 2019,
                "month": 2,
                "day": 23
            },
            "text": {
                "headline": "ProxylessNAS",
                "text": "<p>Neural architecture search (NAS) has a great impact by automatically designing effective neural network architectures. However, the prohibitive computational demand of conventional NAS algorithms (e.g. 104 GPU hours) makes it difficult to \\emph{directly} search the architectures on large-scale tasks (e.g. ImageNet). Differentiable NAS can reduce the cost of GPU hours via a continuous representation of network architecture but suffers from the high GPU memory consumption issue (grow linearly w.r.t. candidate set size). As a result, they need to utilize~\\emph{proxy} tasks, such as training on a smaller dataset, or learning with only a few blocks, or training just for a few epochs. These architectures optimized on proxy tasks are not guaranteed to be optimal on the target task. In this paper, we present \\emph{ProxylessNAS} that can \\emph{directly} learn the architectures for large-scale target tasks and target hardware platforms. We address the high memory consumption issue of differentiable NAS and reduce the computational cost (GPU hours and GPU memory) to the same level of regular training while still allowing a large candidate set. Experiments on CIFAR-10 and ImageNet demonstrate the effectiveness of directness and specialization. On CIFAR-10, our model achieves 2.08\\% test error with only 5.7M parameters, better than the previous state-of-the-art architecture AmoebaNet-B, while using 6× fewer parameters. On ImageNet, our model achieves 3.1\\% better top-1 accuracy than MobileNetV2, while being 1.2× faster with measured GPU latency. We also apply ProxylessNAS to specialize neural architectures for hardware with direct hardware metrics (e.g. latency) and provide insights for efficient CNN architecture design.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1812.00332",
                "caption": "ProxylessNAS: Direct neural architecture search on target task and hardware",
                "credit": "Han Cai, Ligeng Zhu, and Song Han"
            }
        },
        {
            "start_date": {
                "year": 2019,
                "month": 2,
                "day": 14
            },
            "text": {
                "headline": "GPT-2 (1.5B)",
                "text": "<p>Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset - matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.</p>"
            },
            "media": {
                "url": "https://openai.com/blog/better-language-models/",
                "caption": "Language Models are Unsupervised Multitask Learners",
                "credit": "Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever"
            }
        },
        {
            "start_date": {
                "year": 2019,
                "month": 2,
                "day": 1
            },
            "text": {
                "headline": "Hanabi 4 player",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1902.00506",
                "caption": "The Hanabi Challenge: A New Frontier for AI Research",
                "credit": ""
            }
        },
        {
            "start_date": {
                "year": 2019,
                "month": 1,
                "day": 31
            },
            "text": {
                "headline": "MT-DNN",
                "text": "<p>In this paper, we present a Multi-Task Deep Neural Network (MT-DNN) for learning representations across multiple natural language understanding (NLU) tasks. MT-DNN not only leverages large amounts of cross-task data, but also benefits from a regularization effect that leads to more general representations in order to adapt to new tasks and domains. MT-DNN extends the model proposed in Liu et al. (2015) by incorporating a pre-trained bidirectional transformer language model, known as BERT (Devlin et al., 2018). MT-DNN obtains new state-of-the-art results on ten NLU tasks, including SNLI, SciTail, and eight out of nine GLUE tasks, pushing the GLUE benchmark to 82.7% (2.2% absolute improvement). We also demonstrate using the SNLI and SciTail datasets that the representations learned by MT-DNN allow domain adaptation with substantially fewer in-domain labels than the pre-trained BERT representations. The code and pre-trained models are publicly available at this https URL.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1901.11504",
                "caption": "Multi-Task Deep Neural Networks for Natural Language Understanding",
                "credit": "X Liu, P He, W Chen, J Gao"
            }
        },
        {
            "start_date": {
                "year": 2019,
                "month": 1,
                "day": 9
            },
            "text": {
                "headline": "Transformer-XL (257M)",
                "text": "<p>Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, Transformer-XL learns dependency that is 80% longer than RNNs and 450% longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1901.02860",
                "caption": "Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context",
                "credit": "Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V. Le, Ruslan Salakhutdinov"
            }
        },
        {
            "start_date": {
                "year": 2019,
                "month": 1,
                "day": 4
            },
            "text": {
                "headline": "Decoupled weight decay regularization",
                "text": "<p>L2 regularization and weight decay regularization are equivalent for standard stochastic gradient descent (when rescaled by the learning rate), but as we demonstrate this is \\emph{not} the case for adaptive gradient algorithms, such as Adam. While common implementations of these algorithms employ L2 regularization (often calling it \"weight decay\" in what may be misleading due to the inequivalence we expose), we propose a simple modification to recover the original formulation of weight decay regularization by \\emph{decoupling} the weight decay from the optimization steps taken w.r.t. the loss function. We provide empirical evidence that our proposed modification (i) decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard SGD and Adam and (ii) substantially improves Adam's generalization performance, allowing it to compete with SGD with momentum on image classification datasets (on which it was previously typically outperformed by the latter). Our proposed decoupled weight decay has already been adopted by many researchers, and the community has implemented it in TensorFlow and PyTorch; the complete source code for our experiments is available at this https URL</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1711.05101",
                "caption": "Decoupled weight decay regularization.",
                "credit": "Ilya Loshchilov and Frank Hutter"
            }
        },
        {
            "start_date": {
                "year": 2019,
                "month": 1,
                "day": 1
            },
            "text": {
                "headline": "Transformer ELMo",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://www.semanticscholar.org/paper/Dissecting-Contextual-Word-Embeddings%3A-Architecture-Peters-Neumann/ac11062f1f368d97f4c826c317bf50dcc13fdb59",
                "caption": "Dissecting Contextual Word Embeddings: Architecture and Representation",
                "credit": "ME Peters, M Neumann, L Zettlemoyer, W Yih"
            }
        },
        {
            "start_date": {
                "year": 2018,
                "month": 11,
                "day": 16
            },
            "text": {
                "headline": "GPipe (Transformer)",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1811.06965",
                "caption": "GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism",
                "credit": "Y Huang, Y Cheng, A Bapna, O Firat"
            }
        },
        {
            "start_date": {
                "year": 2018,
                "month": 11,
                "day": 16
            },
            "text": {
                "headline": "GPipe (Amoeba)",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1811.06965",
                "caption": "GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism",
                "credit": "Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Mia Xu Chen, Dehao Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V. Le, Yonghui Wu, Zhifeng Chen"
            }
        },
        {
            "start_date": {
                "year": 2018,
                "month": 11,
                "day": 15
            },
            "text": {
                "headline": "Multi-cell LSTM",
                "text": "<p>Language models, being at the heart of many NLP problems, are always of great interest to researchers. Neural language models come with the advantage of distributed representations and long range contexts. With its particular dynamics that allow the cycling of information within the network, `Recurrent neural network' (RNN) becomes an ideal paradigm for neural language modeling. Long Short-Term Memory (LSTM) architecture solves the inadequacies of the standard RNN in modeling long-range contexts. In spite of a plethora of RNN variants, possibility to add multiple memory cells in LSTM nodes was seldom explored. Here we propose a multi-cell node architecture for LSTMs and study its applicability for neural language modeling. The proposed multi-cell LSTM language models outperform the state-of-the-art results on well-known Penn Treebank (PTB) setup.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1811.06477",
                "caption": "Multi-cell LSTM Based Neural Language Model",
                "credit": "Thomas Cherian, Akshay Badola, Vineet Padmanabhan"
            }
        },
        {
            "start_date": {
                "year": 2018,
                "month": 11,
                "day": 12
            },
            "text": {
                "headline": "Fine-tuned-AWD-LSTM-DOC (fin)",
                "text": "<p>Cross-entropy loss is a common choice when it comes to multiclass classification tasks and language modeling in particular. Minimizing this loss results in language models of very good quality. We show that it is possible to fine-tune these models and make them perform even better if they are fine-tuned with sum of cross-entropy loss and reverse Kullback-Leibler divergence. The latter is estimated using discriminator network that we train in advance. During fine-tuning probabilities of rare words that are usually underestimated by language models become bigger. The novel approach that we propose allows us to reach state-of-the-art quality on Penn Treebank: perplexity decreases from 52.4 to 52.1. Our fine-tuning algorithm is rather fast, scales well to different architectures and datasets and requires almost no hyperparameter tuning: the only hyperparameter that needs to be tuned is learning rate.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1811.04623",
                "caption": "Fine-tuning of Language Models with Discriminator",
                "credit": "Vadim Popov, Mikhail Kudinov"
            }
        },
        {
            "start_date": {
                "year": 2018,
                "month": 11,
                "day": 5
            },
            "text": {
                "headline": "Mesh-TensorFlow Transformer 4.9B (language)",
                "text": "<p>Batch-splitting (data-parallelism) is the dominant distributed Deep Neural Network (DNN) training strategy, due to its universal applicability and its amenability to Single-Program-Multiple-Data (SPMD) programming. However, batch-splitting suffers from problems including the inability to train very large models (due to memory constraints), high latency, and inefficiency at small batch sizes. All of these can be solved by more general distribution strategies (model-parallelism). Unfortunately, efficient model-parallel algorithms tend to be complicated to discover, describe, and to implement, particularly on large clusters. We introduce Mesh-TensorFlow, a language for specifying a general class of distributed tensor computations. Where data-parallelism can be viewed as splitting tensors and operations along the \"batch\" dimension, in Mesh-TensorFlow, the user can specify any tensor-dimensions to be split across any dimensions of a multi-dimensional mesh of processors. A Mesh-TensorFlow graph compiles into a SPMD program consisting of parallel operations coupled with collective communication primitives such as Allreduce. We use Mesh-TensorFlow to implement an efficient data-parallel, model-parallel version of the Transformer sequence-to-sequence model. Using TPU meshes of up to 512 cores, we train Transformer models with up to 5 billion parameters, surpassing state of the art results on WMT'14 English-to-French translation task and the one-billion-word language modeling benchmark. Mesh-Tensorflow is available at this https URL .</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1811.02084",
                "caption": "Mesh-TensorFlow: Deep Learning for Supercomputers",
                "credit": "Noam Shazeer, Youlong Cheng, Niki Parmar, Dustin Tran, Ashish Vaswani, Penporn Koanantakool, Peter Hawkins, Hyoukjoong Mingsheng Lee, Cliff Hong, Ryan Young, Blake Sepassi,  Hechtman"
            }
        },
        {
            "start_date": {
                "year": 2018,
                "month": 11,
                "day": 5
            },
            "text": {
                "headline": "Mesh-TensorFlow Transformer 2.9B (translation)",
                "text": "<p>Batch-splitting (data-parallelism) is the dominant distributed Deep Neural Network (DNN) training strategy, due to its universal applicability and its amenability to Single-Program-Multiple-Data (SPMD) programming. However, batch-splitting suffers from problems including the inability to train very large models (due to memory constraints), high latency, and inefficiency at small batch sizes. All of these can be solved by more general distribution strategies (model-parallelism). Unfortunately, efficient model-parallel algorithms tend to be complicated to discover, describe, and to implement, particularly on large clusters. We introduce Mesh-TensorFlow, a language for specifying a general class of distributed tensor computations. Where data-parallelism can be viewed as splitting tensors and operations along the \"batch\" dimension, in Mesh-TensorFlow, the user can specify any tensor-dimensions to be split across any dimensions of a multi-dimensional mesh of processors. A Mesh-TensorFlow graph compiles into a SPMD program consisting of parallel operations coupled with collective communication primitives such as Allreduce. We use Mesh-TensorFlow to implement an efficient data-parallel, model-parallel version of the Transformer sequence-to-sequence model. Using TPU meshes of up to 512 cores, we train Transformer models with up to 5 billion parameters, surpassing state of the art results on WMT'14 English-to-French translation task and the one-billion-word language modeling benchmark. Mesh-Tensorflow is available at this https URL .</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1811.02084",
                "caption": "Mesh-TensorFlow: Deep Learning for Supercomputers",
                "credit": "Noam Shazeer, Youlong Cheng, Niki Parmar, Dustin Tran, Ashish Vaswani, Penporn Koanantakool, Peter Hawkins, Hyoukjoong Mingsheng Lee, Cliff Hong, Ryan Young, Blake Sepassi,  Hechtman"
            }
        },
        {
            "start_date": {
                "year": 2018,
                "month": 10,
                "day": 31
            },
            "text": {
                "headline": "MemoReader",
                "text": "<p>Machine reading comprehension helps machines learn to utilize most of the human\nknowledge written in the form of text. Existing approaches made a significant progress comparable to human-level performance, but they\nare still limited in understanding, up to a few paragraphs, failing to properly comprehend\nlengthy document. In this paper, we propose a novel deep neural network architecture to handle a long-range dependency in RC tasks. In\ndetail, our method has two novel aspects: (1) an advanced memory-augmented architecture\nand (2) an expanded gated recurrent unit with dense connections that mitigate potential information distortion occurring in the memory.\nOur proposed architecture is widely applicable\nto other models. We have performed extensive experiments with well-known benchmark\ndatasets such as TriviaQA, QUASAR-T, and\nSQuAD. The experimental results demonstrate\nthat the proposed method outperforms existing\nmethods, especially for lengthy documents.</p>"
            },
            "media": {
                "url": "https://aclanthology.org/D18-1237/",
                "caption": "MemoReader: Large-Scale Reading Comprehension through Neural Memory Controller\n",
                "credit": "Seohyun Back, Seunghak Yu, Sathish Indurthi, Jihie Kim, Jaegul Choo"
            }
        },
        {
            "start_date": {
                "year": 2018,
                "month": 10,
                "day": 15
            },
            "text": {
                "headline": "TrellisNet",
                "text": "<p>We present trellis networks, a new architecture for sequence modeling. On the one hand, a trellis network is a temporal convolutional network with special structure, characterized by weight tying across depth and direct injection of the input into deep layers. On the other hand, we show that truncated recurrent networks are equivalent to trellis networks with special sparsity structure in their weight matrices. Thus trellis networks with general weight matrices generalize truncated recurrent networks. We leverage these connections to design high-performing trellis networks that absorb structural and algorithmic elements from both recurrent and convolutional models. Experiments demonstrate that trellis networks outperform the current state of the art methods on a variety of challenging benchmarks, including word-level language modeling and character-level language modeling tasks, and stress tests designed to evaluate long-term memory retention. The code is available at this https URL .</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1810.06682",
                "caption": "Trellis Networks for Sequence Modeling",
                "credit": "Shaojie Bai, J. Zico Kolter, Vladlen Koltun"
            }
        },
        {
            "start_date": {
                "year": 2018,
                "month": 10,
                "day": 11
            },
            "text": {
                "headline": "MetaMimic",
                "text": "<p>Humans are experts at high-fidelity imitation -- closely mimicking a demonstration, often in one attempt. Humans use this ability to quickly solve a task instance, and to bootstrap learning of new tasks. Achieving these abilities in autonomous agents is an open problem. In this paper, we introduce an off-policy RL algorithm (MetaMimic) to narrow this gap. MetaMimic can learn both (i) policies for high-fidelity one-shot imitation of diverse novel skills, and (ii) policies that enable the agent to solve tasks more efficiently than the demonstrators. MetaMimic relies on the principle of storing all experiences in a memory and replaying these to learn massive deep neural network policies by off-policy RL. This paper introduces, to the best of our knowledge, the largest existing neural networks for deep RL and shows that larger networks with normalization are needed to achieve one-shot high-fidelity imitation on a challenging manipulation task. The results also show that both types of policy can be learned from vision, in spite of the task rewards being sparse, and without access to demonstrator actions.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1810.05017",
                "caption": "One-Shot High-Fidelity Imitation: Training Large-Scale Deep Nets with RL",
                "credit": "Tom Le Paine, Sergio Gomez"
            }
        },
        {
            "start_date": {
                "year": 2018,
                "month": 10,
                "day": 11
            },
            "text": {
                "headline": "BERT-Large",
                "text": "<p>We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1810.04805",
                "caption": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
                "credit": "J Devlin, MW Chang, K Lee, K Toutanova"
            }
        },
        {
            "start_date": {
                "year": 2018,
                "month": 9,
                "day": 28
            },
            "text": {
                "headline": "Transformer (Adaptive Input Embeddings) WT103",
                "text": "<p>We introduce adaptive input representations for neural language modeling which extend the adaptive softmax of Grave et al. (2017) to input representations of variable capacity. There are several choices on how to factorize the input and output layers, and whether to model words, characters or sub-word units. We perform a systematic comparison of popular choices for a self-attentional architecture. Our experiments show that models equipped with adaptive embeddings are more than twice as fast to train than the popular character input CNN while having a lower number of parameters. On the WIKITEXT-103 benchmark we achieve 18.7 perplexity, an improvement of 10.5 perplexity compared to the previously best published result and on the BILLION WORD benchmark, we achieve 23.02 perplexity.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1809.10853",
                "caption": "Adaptive Input Representations for Neural Language Modeling",
                "credit": "Alexei Baevski, Michael Auli"
            }
        },
        {
            "start_date": {
                "year": 2018,
                "month": 9,
                "day": 28
            },
            "text": {
                "headline": "BigGAN-deep 512x512",
                "text": "<p>Despite recent progress in generative image modeling, successfully generating high-resolution, diverse samples from complex datasets such as ImageNet remains an elusive goal. To this end, we train Generative Adversarial Networks at the largest scale yet attempted, and study the instabilities specific to such scale. We find that applying orthogonal regularization to the generator renders it amenable to a simple \"truncation trick,\" allowing fine control over the trade-off between sample fidelity and variety by reducing the variance of the Generator's input. Our modifications lead to models which set the new state of the art in class-conditional image synthesis. When trained on ImageNet at 128x128 resolution, our models (BigGANs) achieve an Inception Score (IS) of 166.5 and Frechet Inception Distance (FID) of 7.4, improving over the previous best IS of 52.52 and FID of 18.6.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1809.11096",
                "caption": "Large Scale GAN Training for High Fidelity Natural Image Synthesis",
                "credit": "Andrew Brock, Jeff Donahue, Karen Simonyan"
            }
        },
        {
            "start_date": {
                "year": 2018,
                "month": 9,
                "day": 24
            },
            "text": {
                "headline": "LSTM+NeuralCache",
                "text": "<p>Neural cache language models (LMs) extend the idea of regular cache language models by making the cache probability dependent on the similarity between the current context and the context of the words in the cache. We make an extensive comparison of 'regular' cache models with neural cache models, both in terms of perplexity and WER after rescoring first-pass ASR results. Furthermore, we propose two extensions to this neural cache model that make use of the content value/information weight of the word: firstly, combining the cache probability and LM probability with an information-weighted interpolation and secondly, selectively adding only content words to the cache. We obtain a 29.9%/32.1% (validation/test set) relative improvement in perplexity with respect to a baseline LSTM LM on the WikiText-2 dataset, outperforming previous work on neural cache LMs. Additionally, we observe significant WER reductions with respect to the baseline model on the WSJ ASR task.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1809.08826",
                "caption": "Information-Weighted Neural Cache Language Models for ASR",
                "credit": "Lyan Verwimp, Joris Pelemans, Hugo Van hamme, Patrick Wambacq"
            }
        },
        {
            "start_date": {
                "year": 2018,
                "month": 9,
                "day": 18
            },
            "text": {
                "headline": "AWD-LSTM-MoS + dynamic evaluation (WT2, 2018)",
                "text": "<p>Continuous word representation (aka word embedding) is a basic building block in many neural network-based models used in natural language processing tasks. Although it is widely accepted that words with similar semantics should be close to each other in the embedding space, we find that word embeddings learned in several tasks are biased towards word frequency: the embeddings of high-frequency and low-frequency words lie in different subregions of the embedding space, and the embedding of a rare word and a popular word can be far from each other even if they are semantically similar. This makes learned word embeddings ineffective, especially for rare words, and consequently limits the performance of these neural network models. In this paper, we develop a neat, simple yet effective way to learn \\emph{FRequency-AGnostic word Embedding} (FRAGE) using adversarial training. We conducted comprehensive studies on ten datasets across four natural language processing tasks, including word similarity, language modeling, machine translation and text classification. Results show that with FRAGE, we achieve higher performance than the baselines in all tasks.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1809.06858",
                "caption": "FRAGE: Frequency-Agnostic Word Representation",
                "credit": "Chengyue Gong, Di He, Xu Tan, Tao Qin, Liwei Wang, Tie-Yan Liu"
            }
        },
        {
            "start_date": {
                "year": 2018,
                "month": 9,
                "day": 17
            },
            "text": {
                "headline": "Transformer + Simple Recurrent Unit",
                "text": "<p>Common recurrent neural architectures scale poorly due to the intrinsic difficulty in parallelizing their state computations. In this work, we propose the Simple Recurrent Unit (SRU), a light recurrent unit that balances model capacity and scalability. SRU is designed to provide expressive recurrence, enable highly parallelized implementation, and comes with careful initialization to facilitate training of deep models. We demonstrate the effectiveness of SRU on multiple NLP tasks. SRU achieves 5--9x speed-up over cuDNN-optimized LSTM on classification and question answering datasets, and delivers stronger results than LSTM and convolutional models. We also obtain an average of 0.7 BLEU improvement over the Transformer model on translation by incorporating SRU into the architecture.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1709.02755v5",
                "caption": "Simple Recurrent Units for Highly Parallelizable Recurrence",
                "credit": "Tao Lei, Yu Zhang, Sida I. Wang, Hui Dai, Yoav Artzi"
            }
        },
        {
            "start_date": {
                "year": 2018,
                "month": 9,
                "day": 1
            },
            "text": {
                "headline": "ESRGAN",
                "text": "<p>The Super-Resolution Generative Adversarial Network (SRGAN) is a seminal work that is capable of generating realistic textures during single image super-resolution. However, the hallucinated details are often accompanied with unpleasant artifacts. To further enhance the visual quality, we thoroughly study three key components of SRGAN - network architecture, adversarial loss and perceptual loss, and improve each of them to derive an Enhanced SRGAN (ESRGAN). In particular, we introduce the Residual-in-Residual Dense Block (RRDB) without batch normalization as the basic network building unit. Moreover, we borrow the idea from relativistic GAN to let the discriminator predict relative realness instead of the absolute value. Finally, we improve the perceptual loss by using the features before activation, which could provide stronger supervision for brightness consistency and texture recovery. Benefiting from these improvements, the proposed ESRGAN achieves consistently better visual quality with more realistic and natural textures than SRGAN and won the first place in the PIRM2018-SR Challenge. The code is available at this https URL .</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1809.00219",
                "caption": "ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks",
                "credit": "Xintao Wang, Ke Yu, Shixiang Wu, Jinjin Gu, Yihao Liu, Chao Dong, Chen Change Loy, Yu Qiao, Xiaoou Tang"
            }
        },
        {
            "start_date": {
                "year": 2018,
                "month": 8,
                "day": 30
            },
            "text": {
                "headline": "(ensemble): AWD-LSTM-DOC (fin) × 5 (WT2)",
                "text": "<p>This paper proposes a state-of-the-art recurrent neural network (RNN) language model that combines probability distributions computed not only from a final RNN layer but also from middle layers. Our proposed method raises the expressive power of a language model based on the matrix factorization interpretation of language modeling introduced by Yang et al. (2018). The proposed method improves the current state-of-the-art language model and achieves the best score on the Penn Treebank and WikiText-2, which are the standard benchmark datasets. Moreover, we indicate our proposed method contributes to two application tasks: machine translation and headline generation. Our code is publicly available at: this https URL.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1808.10143",
                "caption": "Direct Output Connection for a High-Rank Language Model",
                "credit": "Sho Takase, Jun Suzuki, Masaaki Nagata"
            }
        },
        {
            "start_date": {
                "year": 2018,
                "month": 8,
                "day": 28
            },
            "text": {
                "headline": "Big Transformer for Back-Translation",
                "text": "<p>An effective method to improve neural machine translation with monolingual data is to augment the parallel training corpus with back-translations of target language sentences. This work broadens the understanding of back-translation and investigates a number of methods to generate synthetic source sentences. We find that in all but resource poor settings back-translations obtained via sampling or noised beam outputs are most effective. Our analysis shows that sampling or noisy synthetic data gives a much stronger training signal than data generated by beam or greedy search. We also compare how synthetic data compares to genuine bitext and study various domain effects. Finally, we scale to hundreds of millions of monolingual sentences and achieve a new state of the art of 35 BLEU on the WMT'14 English-German test set. </p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1808.09381",
                "caption": "Understanding Back-Translation at Scale",
                "credit": "Sergey Edunov, Myle Ott, Michael Auli, David Grangier"
            }
        },
        {
            "start_date": {
                "year": 2018,
                "month": 8,
                "day": 14
            },
            "text": {
                "headline": "AWD-LSTM-MoS+PDR + dynamic evaluation (WT2)",
                "text": "<p>Highly regularized LSTMs achieve impressive results on several benchmark datasets in language modeling. We propose a new regularization method based on decoding the last token in the context using the predicted distribution of the next token. This biases the model towards retaining more contextual information, in turn improving its ability to predict the next token. With negligible overhead in the number of parameters and training time, our Past Decode Regularization (PDR) method achieves a word level perplexity of 55.6 on the Penn Treebank and 63.5 on the WikiText-2 datasets using a single softmax. We also show gains by using PDR in combination with a mixture-of-softmaxes, achieving a word level perplexity of 53.8 and 60.5 on these datasets. In addition, our method achieves 1.169 bits-per-character on the Penn Treebank Character dataset for character level language modeling. These results constitute a new state-of-the-art in their respective settings.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1808.05908",
                "caption": "Improved Language Modeling by Decoding the Past",
                "credit": "Siddhartha Brahma"
            }
        },
        {
            "start_date": {
                "year": 2018,
                "month": 7,
                "day": 10
            },
            "text": {
                "headline": "Big-Little Net (speech)",
                "text": "<p>In this paper, we propose a novel Convolutional Neural Network (CNN) architecture for learning multi-scale feature representations with good tradeoffs between speed and accuracy. This is achieved by using a multi-branch network, which has different computational complexity at different branches. Through frequent merging of features from branches at distinct scales, our model obtains multi-scale features while using less computation. The proposed approach demonstrates improvement of model efficiency and performance on both object recognition and speech recognition tasks,using popular architectures including ResNet and ResNeXt. For object recognition, our approach reduces computation by 33% on object recognition while improving accuracy with 0.9%. Furthermore, our model surpasses state-of-the-art CNN acceleration approaches by a large margin in accuracy and FLOPs reduction. On the task of speech recognition, our proposed multi-scale CNNs save 30% FLOPs with slightly better word error rates, showing good generalization across domains. The codes are available at https://github.com/IBM/BigLittleNet.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1807.03848",
                "caption": "Big-Little Net: An Efficient Multi-Scale Feature Representation for Visual and Speech Recognition",
                "credit": "Chun-Fu (Richard) Chen, Quanfu Fan, Neil Mallinar, Tom Sercu, Rogerio Feris"
            }
        },
        {
            "start_date": {
                "year": 2018,
                "month": 7,
                "day": 10
            },
            "text": {
                "headline": "Big-Little Net",
                "text": "<p>In this paper, we propose a novel Convolutional Neural Network (CNN) architecture for learning multi-scale feature representations with good tradeoffs between speed and accuracy. This is achieved by using a multi-branch network, which has different computational complexity at different branches. Through frequent merging of features from branches at distinct scales, our model obtains multi-scale features while using less computation. The proposed approach demonstrates improvement of model efficiency and performance on both object recognition and speech recognition tasks,using popular architectures including ResNet and ResNeXt. For object recognition, our approach reduces computation by 33% on object recognition while improving accuracy with 0.9%. Furthermore, our model surpasses state-of-the-art CNN acceleration approaches by a large margin in accuracy and FLOPs reduction. On the task of speech recognition, our proposed multi-scale CNNs save 30% FLOPs with slightly better word error rates, showing good generalization across domains.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1807.03848",
                "caption": "Big-Little Net: An Efficient Multi-Scale Feature Representation for Visual and Speech Recognition",
                "credit": "Chun-Fu Chen, Quanfu Fan, Neil Mallinar, Tom Sercu, and Rogerio Feris"
            }
        },
        {
            "start_date": {
                "year": 2018,
                "month": 7,
                "day": 8
            },
            "text": {
                "headline": "RCAN",
                "text": "<p>Convolutional neural network (CNN) depth is of crucial importance for image super-resolution (SR). However, we observe that deeper networks for image SR are more difficult to train. The low-resolution (LR) inputs and features contain abundant low-frequency information, which is treated equally across channels, hence hindering the representational ability of CNNs. To solve these problems, we propose the very deep residual channel attention networks (RCAN). Specifically, we propose residual in residual (RIR) structure to form very deep network, which consists of several residual groups with long skip connections. Each residual group contains some residual blocks with short skip connections. Meanwhile, RIR allows abundant low-frequency information to be bypassed through multiple skip connections, making the main network focus on learning high-frequency information. Furthermore, we propose channel attention mechanism to adaptively rescale channel-wise features by considering interdependencies among channels. Extensive experiments show that our RCAN achieves better accuracy and visual improvements against state-of-the-art methods.</p>"
            },
            "media": {
                "url": "https://openaccess.thecvf.com/content_ECCV_2018/html/Yulun_Zhang_Image_Super-Resolution_Using_ECCV_2018_paper.html",
                "caption": "Image Super-Resolution Using Very Deep Residual Channel Attention Networks",
                "credit": " Yulun Zhang, Kunpeng Li, Kai Li, Lichen Wang, Bineng Zhong, Yun Fu"
            }
        },
        {
            "start_date": {
                "year": 2018,
                "month": 7,
                "day": 3
            },
            "text": {
                "headline": "Population-based DRL",
                "text": "<p>Recent progress in artificial intelligence through reinforcement learning (RL) has shown great success on increasingly complex single-agent environments and two-player turn-based games. However, the real-world contains multiple agents, each learning and acting independently to cooperate and compete with other agents, and environments reflecting this degree of complexity remain an open challenge. In this work, we demonstrate for the first time that an agent can achieve human-level in a popular 3D multiplayer first-person video game, Quake III Arena Capture the Flag, using only pixels and game points as input. These results were achieved by a novel two-tier optimisation process in which a population of independent RL agents are trained concurrently from thousands of parallel matches with agents playing in teams together and against each other on randomly generated environments. Each agent in the population learns its own internal reward signal to complement the sparse delayed reward from winning, and selects actions using a novel temporally hierarchical representation that enables the agent to reason at multiple timescales. During game-play, these agents display human-like behaviours such as navigating, following, and defending based on a rich learned representation that is shown to encode high-level game knowledge. In an extensive tournament-style evaluation the trained agents exceeded the win-rate of strong human players both as teammates and opponents, and proved far stronger than existing state-of-the-art agents. These results demonstrate a significant jump in the capabilities of artificial agents, bringing us closer to the goal of human-level intelligence.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1807.01281",
                "caption": "Human-level performance in first-person multiplayer games with population-based deep reinforcement learning",
                "credit": "Max Jaderberg, Wojciech M. Czarnecki, Iain Dunning, Luke Marris, Guy Lever, Antonio Garcia Castaneda, Charles Beattie, Neil C. Rabinowitz, Ari S. Morcos, Avraham Ruderman, Nicolas Sonnerat, Tim Green, Louise Deason, Joel Z. Leibo, David Silver, Demis Hassabis, Koray Kavukcuoglu, Thore Graepel"
            }
        },
        {
            "start_date": {
                "year": 2018,
                "month": 6,
                "day": 30
            },
            "text": {
                "headline": "ShuffleNet v2",
                "text": "<p>Currently, the neural network architecture design is mostly guided by the \\emph{indirect} metric of computation complexity, i.e., FLOPs. However, the \\emph{direct} metric, e.g., speed, also depends on the other factors such as memory access cost and platform characterics. Thus, this work proposes to evaluate the direct metric on the target platform, beyond only considering FLOPs. Based on a series of controlled experiments, this work derives several practical \\emph{guidelines} for efficient network design. Accordingly, a new architecture is presented, called \\emph{ShuffleNet V2}. Comprehensive ablation experiments verify that our model is the state-of-the-art in terms of speed and accuracy tradeoff.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1807.11164",
                "caption": "ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design",
                "credit": "Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng, Jian Sun"
            }
        },
        {
            "start_date": {
                "year": 2018,
                "month": 6,
                "day": 27
            },
            "text": {
                "headline": "QT-Opt",
                "text": "<p>In this paper, we study the problem of learning vision-based dynamic manipulation skills using a scalable reinforcement learning approach. We study this problem in the context of grasping, a longstanding challenge in robotic manipulation. In contrast to static learning behaviors that choose a grasp point and then execute the desired grasp, our method enables closed-loop vision-based control, whereby the robot continuously updates its grasp strategy based on the most recent observations to optimize long-horizon grasp success. To that end, we introduce QT-Opt, a scalable self-supervised vision-based reinforcement learning framework that can leverage over 580k real-world grasp attempts to train a deep neural network Q-function with over 1.2M parameters to perform closed-loop, real-world grasping that generalizes to 96% grasp success on unseen objects. Aside from attaining a very high success rate, our method exhibits behaviors that are quite distinct from more standard grasping systems: using only RGB vision-based perception from an over-the-shoulder camera, our method automatically learns regrasping strategies, probes objects to find the most effective grasps, learns to reposition objects and perform other non-prehensile pre-grasp manipulations, and responds dynamically to disturbances and perturbations.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1806.10293",
                "caption": "QT-Opt: Scalable Deep Reinforcement Learning for Vision-Based Robotic Manipulation",
                "credit": "Dmitry Kalashnikov, Alex Irpan, Peter Pastor, Julian Ibarz, Alexander Herzog, Eric Jang, Deirdre Quillen, Ethan Holly, Mrinal Kalakrishnan, Vincent Vanhoucke, Sergey Levine"
            }
        },
        {
            "start_date": {
                "year": 2018,
                "month": 6,
                "day": 24
            },
            "text": {
                "headline": "DARTS",
                "text": "<p>This paper addresses the scalability challenge of architecture search by formulating the task in a differentiable manner. Unlike conventional approaches of applying evolution or reinforcement learning over a discrete and non-differentiable search space, our method is based on the continuous relaxation of the architecture representation, allowing efficient search of the architecture using gradient descent. Extensive experiments on CIFAR-10, ImageNet, Penn Treebank and WikiText-2 show that our algorithm excels in discovering high-performance convolutional architectures for image classification and recurrent architectures for language modeling, while being orders of magnitude faster than state-of-the-art non-differentiable techniques. Our implementation has been made publicly available to facilitate further research on efficient architecture search algorithms.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1806.09055",
                "caption": "DARTS: Differentiable Architecture Search",
                "credit": "Hanxiao Liu, Karen Simonyan, Yiming Yang"
            }
        },
        {
            "start_date": {
                "year": 2018,
                "month": 6,
                "day": 18
            },
            "text": {
                "headline": "MobileNetV2",
                "text": "<p>In this paper we describe a new mobile architecture, MobileNetV2, that improves the state of the art performance of mobile models on multiple tasks and benchmarks as well as across a spectrum of different model sizes. We also describe efficient ways of applying these mobile models to object detection in a novel framework we call SSDLite. Additionally, we demonstrate how to build mobile semantic segmentation models through a reduced form of DeepLabv3 which we call Mobile DeepLabv3. is based on an inverted residual structure where the shortcut connections are between the thin bottleneck layers. The intermediate expansion layer uses lightweight depthwise convolutions to filter features as a source of non-linearity. Additionally, we find that it is important to remove non-linearities in the narrow layers in order to maintain representational power. We demonstrate that this improves performance and provide an intuition that led to this design. Finally, our approach allows decoupling of the input/output domains from the expressiveness of the transformation, which provides a convenient framework for further analysis. We measure our performance on ImageNet [1] classification, COCO object detection [2], VOC image segmentation [3]. We evaluate the trade-offs between accuracy, and number of operations measured by multiply-adds (MAdd), as well as actual latency, and the number of parameters.</p>"
            },
            "media": {
                "url": "https://ieeexplore.ieee.org/document/8578572",
                "caption": "MobileNetV2: Inverted Residuals and Linear Bottlenecks",
                "credit": "M Sandler, A Howard, M Zhu"
            }
        },
        {
            "start_date": {
                "year": 2018,
                "month": 6,
                "day": 5
            },
            "text": {
                "headline": "Relational Memory Core",
                "text": "<p>Memory-based neural networks model temporal data by leveraging an ability to remember information for long periods. It is unclear, however, whether they also have an ability to perform complex relational reasoning with the information they remember. Here, we first confirm our intuitions that standard memory architectures may struggle at tasks that heavily involve an understanding of the ways in which entities are connected -- i.e., tasks involving relational reasoning. We then improve upon these deficits by using a new memory module -- a \\textit{Relational Memory Core} (RMC) -- which employs multi-head dot product attention to allow memories to interact. Finally, we test the RMC on a suite of tasks that may profit from more capable relational reasoning across sequential information, and show large gains in RL domains (e.g. Mini PacMan), program evaluation, and language modeling, achieving state-of-the-art results on the WikiText-103, Project Gutenberg, and GigaWord datasets.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1806.01822",
                "caption": "Relational recurrent neural networks",
                "credit": "Adam Santoro, Ryan Faulkner, David Raposo, Jack Rae, Mike Chrzanowski, Theophane Weber, Daan Wierstra, Oriol Vinyals, Razvan Pascanu, Timothy Lillicrap"
            }
        },
        {
            "start_date": {
                "year": 2018,
                "month": 6,
                "day": 1
            },
            "text": {
                "headline": "GPT-1",
                "text": "<p>Natural language understanding comprises a wide range of diverse tasks such as textual entailment, question answering, semantic similarity assessment, and document classification. Although large unlabeled text corpora are abundant, labeled data for learning these specific tasks is scarce, making it challenging for discriminatively trained models to perform adequately. We demonstrate that large gains on these tasks can be realized by generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative fine-tuning on each specific task. In contrast to previous approaches, we make use of task-aware input transformations during fine-tuning to achieve effective transfer while requiring minimal changes to the model architecture. We demonstrate the effectiveness of our approach on a wide range of benchmarks for natural language understanding. Our general task-agnostic model outperforms discriminatively trained models that use architectures specifically crafted for each task, significantly improving upon the state of the art in 9 out of the 12 tasks studied. For instance, we achieve absolute improvements of 8.9% on commonsense reasoning (Stories Cloze Test), 5.7% on question answering (RACE), and 1.5% on textual entailment (MultiNLI).</p>"
            },
            "media": {
                "url": "https://openai.com/blog/language-unsupervised/",
                "caption": "Improving Language Understanding by Generative Pre-Training",
                "credit": "A Radford, K Narasimhan, T Salimans, I Sutskever"
            }
        },
        {
            "start_date": {
                "year": 2018,
                "month": 5,
                "day": 22
            },
            "text": {
                "headline": "aLSTM(depth-2)+RecurrentPolicy (WT2)",
                "text": "<p>Standard neural network architectures are non-linear only by virtue of a simple element-wise activation function, making them both brittle and excessively large. In this paper, we consider methods for making the feed-forward layer more flexible while preserving its basic structure. We develop simple drop-in replacements that learn to adapt their parameterization conditional on the input, thereby increasing statistical efficiency significantly. We present an adaptive LSTM that advances the state of the art for the Penn Treebank and WikiText-2 word-modeling tasks while using fewer parameters and converging in less than half as many iterations.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1805.08574",
                "caption": "Breaking the Activation Function Bottleneck through Adaptive Parameterization",
                "credit": "Sebastian Flennerhag, Hujun Yin, John Keane, Mark Elliot"
            }
        },
        {
            "start_date": {
                "year": 2018,
                "month": 5,
                "day": 3
            },
            "text": {
                "headline": "Dropout-LSTM+Noise(Bernoulli) (WT2)",
                "text": "<p>Recurrent neural networks (RNNs) are powerful models of sequential data. They have been successfully used in domains such as text and speech. However, RNNs are susceptible to overfitting; regularization is important. In this paper we develop Noisin, a new method for regularizing RNNs. Noisin injects random noise into the hidden states of the RNN and then maximizes the corresponding marginal likelihood of the data. We show how Noisin applies to any RNN and we study many different types of noise. Noisin is unbiased--it preserves the underlying RNN on average. We characterize how Noisin regularizes its RNN both theoretically and empirically. On language modeling benchmarks, Noisin improves over dropout by as much as 12.2% on the Penn Treebank and 9.4% on the Wikitext-2 dataset. We also compared the state-of-the-art language model of Yang et al. 2017, both with and without Noisin. On the Penn Treebank, the method with Noisin more quickly reaches state-of-the-art performance.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1805.01500",
                "caption": "Noisin: Unbiased Regularization for Recurrent Neural Networks",
                "credit": "Adji B. Dieng, Rajesh Ranganath, Jaan Altosaar, David M. Blei"
            }
        },
        {
            "start_date": {
                "year": 2018,
                "month": 5,
                "day": 2
            },
            "text": {
                "headline": "ResNeXt-101 32x48d",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1805.00932",
                "caption": "Exploring the Limits of Weakly Supervised Pretraining",
                "credit": "Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan Li, Ashwin Bharambe, Laurens van der Maaten"
            }
        },
        {
            "start_date": {
                "year": 2018,
                "month": 4,
                "day": 14
            },
            "text": {
                "headline": "Diffractive Deep Neural Network",
                "text": "<p>We introduce an all-optical Diffractive Deep Neural Network (D2NN) architecture that can learn to implement various functions after deep learning-based design of passive diffractive layers that work collectively. We experimentally demonstrated the success of this framework by creating 3D-printed D2NNs that learned to implement handwritten digit classification and the function of an imaging lens at terahertz spectrum. With the existing plethora of 3D-printing and other lithographic fabrication methods as well as spatial-light-modulators, this all-optical deep learning framework can perform, at the speed of light, various complex functions that computer-based neural networks can implement, and will find applications in all-optical image analysis, feature detection and object classification, also enabling new camera designs and optical components that can learn to perform unique tasks using D2NNs.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1804.08711",
                "caption": "All-Optical Machine Learning Using Diffractive Deep Neural Networks",
                "credit": "Xing Lin, Yair Rivenson, Nezih T Yardimci, Muhammed Veli, Yi Luo, Mona Jarrahi, and Aydogan Ozcan"
            }
        },
        {
            "start_date": {
                "year": 2018,
                "month": 4,
                "day": 8
            },
            "text": {
                "headline": "YOLOv3",
                "text": "<p>We present some updates to YOLO! We made a bunch of little design changes to make it better. We also trained this new network that's pretty swell. It's a little bigger than last time but more accurate. It's still fast though, don't worry. At 320x320 YOLOv3 runs in 22 ms at 28.2 mAP, as accurate as SSD but three times faster. When we look at the old .5 IOU mAP detection metric YOLOv3 is quite good. It achieves 57.9 mAP@50 in 51 ms on a Titan X, compared to 57.5 mAP@50 in 198 ms by RetinaNet, similar performance but 3.8x faster. As always, all the code is online at this https URL</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1804.02767",
                "caption": "YOLOv3: An Incremental Improvement",
                "credit": "Joseph Redmon, Ali Farhadi"
            }
        },
        {
            "start_date": {
                "year": 2018,
                "month": 3,
                "day": 27
            },
            "text": {
                "headline": "LSTM (Hebbian, Cache, MbPA)",
                "text": "<p>Neural networks trained with backpropagation often struggle to identify classes that have been observed a small number of times. In applications where most class labels are rare, such as language modelling, this can become a performance bottleneck. One potential remedy is to augment the network with a fast-learning non-parametric model which stores recent activations and class labels into an external memory. We explore a simplified architecture where we treat a subset of the model parameters as fast memory stores. This can help retain information over longer time intervals than a traditional memory, and does not require additional space or compute. In the case of image classification, we display faster binding of novel classes on an Omniglot image curriculum task. We also show improved performance for word-based language models on news reports (GigaWord), books (Project Gutenberg) and Wikipedia articles (WikiText-103) --- the latter achieving a state-of-the-art perplexity of 29.2.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1803.10049",
                "caption": "Fast Parametric Learning with Activation Memorization",
                "credit": "Jack W Rae, Chris Dyer, Peter Dayan, Timothy P Lillicrap"
            }
        },
        {
            "start_date": {
                "year": 2018,
                "month": 3,
                "day": 22
            },
            "text": {
                "headline": "4 layer QRNN (h=2500)",
                "text": "<p>Many of the leading approaches in language modeling introduce novel, complex and specialized architectures. We take existing state-of-the-art word level language models based on LSTMs and QRNNs and extend them to both larger vocabularies as well as character-level granularity. When properly tuned, LSTMs and QRNNs achieve state-of-the-art results on character-level (Penn Treebank, enwik8) and word-level (WikiText-103) datasets, respectively. Results are obtained in only 12 hours (WikiText-103) to 2 days (enwik8) using a single modern GPU.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1803.08240",
                "caption": "An Analysis of Neural Language Modeling at Multiple Scales",
                "credit": "Stephen Merity, Nitish Shirish Keskar, Richard Socher"
            }
        },
        {
            "start_date": {
                "year": 2018,
                "month": 3,
                "day": 21
            },
            "text": {
                "headline": "Rotation",
                "text": "<p>Over the last years, deep convolutional neural networks (ConvNets) have transformed the field of computer vision thanks to their unparalleled capacity to learn high level semantic image features. However, in order to successfully learn those features, they usually require massive amounts of manually labeled data, which is both expensive and impractical to scale. Therefore, unsupervised semantic feature learning, i.e., learning without requiring manual annotation effort, is of crucial importance in order to successfully harvest the vast amount of visual data that are available today. In our work we propose to learn image features by training ConvNets to recognize the 2d rotation that is applied to the image that it gets as input. We demonstrate both qualitatively and quantitatively that this apparently simple task actually provides a very powerful supervisory signal for semantic feature learning. We exhaustively evaluate our method in various unsupervised feature learning benchmarks and we exhibit in all of them state-of-the-art performance. Specifically, our results on those benchmarks demonstrate dramatic improvements w.r.t. prior state-of-the-art approaches in unsupervised representation learning and thus significantly close the gap with supervised feature learning. For instance, in PASCAL VOC 2007 detection task our unsupervised pre-trained AlexNet model achieves the state-of-the-art (among unsupervised methods) mAP of 54.4% that is only 2.4 points lower from the supervised case. We get similarly striking results when we transfer our unsupervised learned features on various other tasks, such as ImageNet classification, PASCAL classification, PASCAL segmentation, and CIFAR-10 classification. The code and models of our paper will be published on: this https URL .</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1803.07728",
                "caption": "Unsupervised Representation Learning by Predicting Image Rotations",
                "credit": "Spyros Gidaris, Praveer Singh, Nikos Komodakis"
            }
        },
        {
            "start_date": {
                "year": 2018,
                "month": 3,
                "day": 4
            },
            "text": {
                "headline": "LSTM (2018)",
                "text": "<p>For most deep learning practitioners, sequence modeling is synonymous with recurrent networks. Yet recent results indicate that convolutional architectures can outperform recurrent networks on tasks such as audio synthesis and machine translation. Given a new sequence modeling task or dataset, which architecture should one use? We conduct a systematic evaluation of generic convolutional and recurrent architectures for sequence modeling. The models are evaluated across a broad range of standard tasks that are commonly used to benchmark recurrent networks. Our results indicate that a simple convolutional architecture outperforms canonical recurrent networks such as LSTMs across a diverse range of tasks and datasets, while demonstrating longer effective memory. We conclude that the common association between sequence modeling and recurrent networks should be reconsidered, and convolutional networks should be regarded as a natural starting point for sequence modeling tasks. To assist related work, we have made code available at this http URL .</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1803.01271",
                "caption": "An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling",
                "credit": "Shaojie Bai, J. Zico Kolter, Vladlen Koltun"
            }
        },
        {
            "start_date": {
                "year": 2018,
                "month": 3,
                "day": 1
            },
            "text": {
                "headline": "Chinese - English translation",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://www.microsoft.com/en-us/research/publication/achieving-human-parity-on-automatic-chinese-to-english-news-translation/",
                "caption": "Achieving Human Parity on Automatic Chinese to English News Translation",
                "credit": "H Hassan, A Aue, C Chen, V Chowdhary"
            }
        },
        {
            "start_date": {
                "year": 2018,
                "month": 2,
                "day": 24
            },
            "text": {
                "headline": "Residual Dense Network",
                "text": "<p>A very deep convolutional neural network (CNN) has recently achieved great success for image super-resolution (SR) and offered hierarchical features as well. However, most deep CNN based SR models do not make full use of the hierarchical features from the original low-resolution (LR) images, thereby achieving relatively-low performance. In this paper, we propose a novel residual dense network (RDN) to address this problem in image SR. We fully exploit the hierarchical features from all the convolutional layers. Specifically, we propose residual dense block (RDB) to extract abundant local features via dense connected convolutional layers. RDB further allows direct connections from the state of preceding RDB to all the layers of current RDB, leading to a contiguous memory (CM) mechanism. Local feature fusion in RDB is then used to adaptively learn more effective features from preceding and current local features and stabilizes the training of wider network. After fully obtaining dense local features, we use global feature fusion to jointly and adaptively learn global hierarchical features in a holistic way. Extensive experiments on benchmark datasets with different degradation models show that our RDN achieves favorable performance against state-of-the-art methods.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1802.08797v2",
                "caption": "Residual Dense Network for Image Super-Resolution",
                "credit": " Yulun Zhang, Yapeng Tian, Yu Kong, Bineng Zhong, Yun Fu"
            }
        },
        {
            "start_date": {
                "year": 2018,
                "month": 2,
                "day": 16
            },
            "text": {
                "headline": "Spectrally Normalized GAN",
                "text": "<p>One of the challenges in the study of generative adversarial networks is the instability of its training. In this paper, we propose a novel weight normalization technique called spectral normalization to stabilize the training of the discriminator. Our new normalization technique is computationally light and easy to incorporate into existing implementations. We tested the efficacy of spectral normalization on CIFAR10, STL-10, and ILSVRC2012 dataset, and we experimentally confirmed that spectrally normalized GANs (SN-GANs) is capable of generating images of better or equal quality relative to the previous training stabilization techniques.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1802.05957",
                "caption": "Spectral Normalization for Generative Adversarial Networks",
                "credit": "Takeru Miyato, Toshiki Kataoka, Masanori Koyama, Yuichi Yoshida"
            }
        },
        {
            "start_date": {
                "year": 2018,
                "month": 2,
                "day": 15
            },
            "text": {
                "headline": "TCN (P-MNIST)",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://openreview.net/forum?id=rk8wKk-R-",
                "caption": "Convolutional Sequence Modeling Revisited",
                "credit": "Shaojie Bai, J. Zico Kolter, Vladlen Koltun"
            }
        },
        {
            "start_date": {
                "year": 2018,
                "month": 2,
                "day": 9
            },
            "text": {
                "headline": "ENAS",
                "text": "<p>We propose Efficient Neural Architecture Search (ENAS), a fast and inexpensive approach for automatic model design. In ENAS, a controller learns to discover neural network architectures by searching for an optimal subgraph within a large computational graph. The controller is trained with policy gradient to select a subgraph that maximizes the expected reward on the validation set. Meanwhile the model corresponding to the selected subgraph is trained to minimize a canonical cross entropy loss. Thanks to parameter sharing between child models, ENAS is fast: it delivers strong empirical performances using much fewer GPU-hours than all existing automatic model design approaches, and notably, 1000x less expensive than standard Neural Architecture Search. On the Penn Treebank dataset, ENAS discovers a novel architecture that achieves a test perplexity of 55.8, establishing a new state-of-the-art among all methods without post-training processing. On the CIFAR-10 dataset, ENAS designs novel architectures that achieve a test error of 2.89%, which is on par with NASNet (Zoph et al., 2018), whose test error is 2.65%.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1802.03268",
                "caption": "Efficient Neural Architecture Search via Parameter Sharing",
                "credit": "Hieu Pham, Melody Y. Guan, Barret Zoph, Quoc V. Le, Jeff Dean"
            }
        },
        {
            "start_date": {
                "year": 2018,
                "month": 2,
                "day": 7
            },
            "text": {
                "headline": "DeepLabV3+",
                "text": "<p>Spatial pyramid pooling module or encode-decoder structure are used in deep neural networks for semantic segmentation task. The former networks are able to encode multi-scale contextual information by probing the incoming features with filters or pooling operations at multiple rates and multiple effective fields-of-view, while the latter networks can capture sharper object boundaries by gradually recovering the spatial information. In this work, we propose to combine the advantages from both methods. Specifically, our proposed model, DeepLabv3+, extends DeepLabv3 by adding a simple yet effective decoder module to refine the segmentation results especially along object boundaries. We further explore the Xception model and apply the depthwise separable convolution to both Atrous Spatial Pyramid Pooling and decoder modules, resulting in a faster and stronger encoder-decoder network. We demonstrate the effectiveness of the proposed model on PASCAL VOC 2012 and Cityscapes datasets, achieving the test set performance of 89.0\\% and 82.1\\% without any post-processing. Our paper is accompanied with a publicly available reference implementation of the proposed models in Tensorflow at \\url{this https URL}.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1802.02611v3",
                "caption": "Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation",
                "credit": "Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, Hartwig Adam"
            }
        },
        {
            "start_date": {
                "year": 2018,
                "month": 2,
                "day": 5
            },
            "text": {
                "headline": "IMPALA",
                "text": "<p>In this work we aim to solve a large collection of tasks using a single reinforcement learning agent with a single set of parameters. A key challenge is to handle the increased amount of data and extended training time. We have developed a new distributed agent IMPALA (Importance Weighted Actor-Learner Architecture) that not only uses resources more efficiently in single-machine training but also scales to thousands of machines without sacrificing data efficiency or resource utilisation. We achieve stable learning at high throughput by combining decoupled acting and learning with a novel off-policy correction method called V-trace. We demonstrate the effectiveness of IMPALA for multi-task reinforcement learning on DMLab-30 (a set of 30 tasks from the DeepMind Lab environment (Beattie et al., 2016)) and Atari-57 (all available Atari games in Arcade Learning Environment (Bellemare et al., 2013a)). Our results show that IMPALA is able to achieve better performance than previous agents with less data, and crucially exhibits positive transfer between tasks as a result of its multi-task approach.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1802.01561",
                "caption": "IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures",
                "credit": "Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Volodymir Mnih, Tom Ward, Yotam Doron, Vlad Firoiu, Tim Harley, Iain Dunning, Shane Legg, Koray Kavukcuoglu"
            }
        },
        {
            "start_date": {
                "year": 2018,
                "month": 2,
                "day": 5
            },
            "text": {
                "headline": "AmoebaNet-A (F=448)",
                "text": "<p>The effort devoted to hand-crafting neural network image classifiers has motivated the use of architecture search to discover them automatically. Although evolutionary algorithms have been repeatedly applied to neural network topologies, the image classifiers thus discovered have remained inferior to human-crafted ones. Here, we evolve an image classifier---AmoebaNet-A---that surpasses hand-designs for the first time. To do this, we modify the tournament selection evolutionary algorithm by introducing an age property to favor the younger genotypes. Matching size, AmoebaNet-A has comparable accuracy to current state-of-the-art ImageNet models discovered with more complex architecture-search methods. Scaled to larger size, AmoebaNet-A sets a new state-of-the-art 83.9% / 96.6% top-5 ImageNet accuracy. In a controlled comparison against a well known reinforcement learning algorithm, we give evidence that evolution can obtain results faster with the same hardware, especially at the earlier stages of the search. This is relevant when fewer compute resources are available. Evolution is, thus, a simple method to effectively discover high-quality architectures.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1802.01548",
                "caption": "Regularized Evolution for Image Classifier Architecture Search",
                "credit": "Esteban Real, Alok Aggarwal, Yanping Huang, Quoc V Le"
            }
        },
        {
            "start_date": {
                "year": 2018,
                "month": 2,
                "day": 5
            },
            "text": {
                "headline": "AmoebaNet-A (F=190)",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1802.01548",
                "caption": "Regularized Evolution for Image Classifier Architecture Search",
                "credit": "E Real, A Aggarwal, Y Huang, QV Le"
            }
        },
        {
            "start_date": {
                "year": 2018,
                "month": 2,
                "day": 1
            },
            "text": {
                "headline": "QRNN",
                "text": "<p>Word-level language modeling (WLM) is one the foundational tasks of unsupervised natural language processing. Most modern architectures for WLM use several LSTM layers, followed by a softmax layer. Even with larger batch sizes and a multi-GPU setup, training of these networks on large-vocabulary corpora is slow due to increased computation involving the softmax and the high cost of recurrence computation. We propose a model architecture and training strategy that enables us to achieve state-of-the-art performance on the WikiText-103 data set using a single GPU while being substantially faster than an NVIDIA cuDNN LSTM-based model by utilizing the Quasi-Recurrent Neural Network (QRNN), an adaptive softmax with weight tying, and longer sequences within batches.</p>"
            },
            "media": {
                "url": "https://mlsys.org/Conferences/doc/2018/50.pdf",
                "caption": "Scalable Language Modeling: WikiText-103 on a Single GPU in 12 hours",
                "credit": "Stephen Merity, Nitish Shirish Keskar, James Bradbury, Richard Socher"
            }
        },
        {
            "start_date": {
                "year": 2018,
                "month": 2,
                "day": 1
            },
            "text": {
                "headline": "ELMo",
                "text": "<p>We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1802.05365",
                "caption": "Deep contextualized word representations",
                "credit": "Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, Luke Zettlemoyer"
            }
        },
        {
            "start_date": {
                "year": 2018,
                "month": 1,
                "day": 18
            },
            "text": {
                "headline": "ULM-FiT",
                "text": "<p>Inductive transfer learning has greatly impacted computer vision, but existing approaches in NLP still require task-specific modifications and training from scratch. We propose Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for fine-tuning a language model. Our method significantly outperforms the state-of-the-art on six text classification tasks, reducing the error by 18-24% on the majority of datasets. Furthermore, with only 100 labeled examples, it matches the performance of training from scratch on 100x more data. We open-source our pretrained models and code.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1801.06146",
                "caption": "Universal Language Model Fine-tuning for Text Classification",
                "credit": "Jeremy Howard, Sebastian Ruder"
            }
        },
        {
            "start_date": {
                "year": 2018,
                "month": 1,
                "day": 9
            },
            "text": {
                "headline": "Refined Part Pooling",
                "text": "<p>Employing part-level features for pedestrian image description offers fine-grained information and has been verified as beneficial for person retrieval in very recent literature. A prerequisite of part discovery is that each part should be well located. Instead of using external cues, e.g., pose estimation, to directly locate parts, this paper lays emphasis on the content consistency within each part.\nSpecifically, we target at learning discriminative part-informed features for person retrieval and make two contributions. (i) A network named Part-based Convolutional Baseline (PCB). Given an image input, it outputs a convolutional descriptor consisting of several part-level features. With a uniform partition strategy, PCB achieves competitive results with the state-of-the-art methods, proving itself as a strong convolutional baseline for person retrieval.\n(ii) A refined part pooling (RPP) method. Uniform partition inevitably incurs outliers in each part, which are in fact more similar to other parts. RPP re-assigns these outliers to the parts they are closest to, resulting in refined parts with enhanced within-part consistency. Experiment confirms that RPP allows PCB to gain another round of performance boost. For instance, on the Market-1501 dataset, we achieve (77.4+4.2)% mAP and (92.3+1.5)% rank-1 accuracy, surpassing the state of the art by a large margin.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1711.09349",
                "caption": "Beyond Part Models: Person Retrieval with Refined Part Pooling (and a Strong Convolutional Baseline)",
                "credit": "Yifan Sun, Liang Zheng, Yi Yang, Qi Tian, Shengjin Wang"
            }
        },
        {
            "start_date": {
                "year": 2017,
                "month": 12,
                "day": 19
            },
            "text": {
                "headline": "Tacotron 2",
                "text": "<p>This paper describes Tacotron 2, a neural network architecture for speech synthesis directly from text. The system is composed of a recurrent sequence-to-sequence feature prediction network that maps character embeddings to mel-scale spectrograms, followed by a modified WaveNet model acting as a vocoder to synthesize timedomain waveforms from those spectrograms. Our model achieves a mean opinion score (MOS) of 4.53 comparable to a MOS of 4.58 for professionally recorded speech. To validate our design choices, we present ablation studies of key components of our system and evaluate the impact of using mel spectrograms as the input to WaveNet instead of linguistic, duration, and F0 features. We further demonstrate that using a compact acoustic intermediate representation enables significant simplification of the WaveNet architecture.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1712.05884",
                "caption": "Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Prediction",
                "credit": "Jonathan Shen, Ruoming Pang, Ron J. Weiss, Mike Schuster, Navdeep Jaitly, Zongheng Yang, Zhifeng Chen, Yu Zhang, Yuxuan Wang, RJ Skerry-Ryan, Rif A. Saurous, Yannis Agiomyrgiannakis, Yonghui Wu"
            }
        },
        {
            "start_date": {
                "year": 2017,
                "month": 12,
                "day": 5
            },
            "text": {
                "headline": "AlphaZero",
                "text": "<p>The game of chess is the most widely-studied domain in the history of artificial intelligence. The strongest programs are based on a combination of sophisticated search techniques, domain-specific adaptations, and handcrafted evaluation functions that have been refined by human experts over several decades. In contrast, the AlphaGo Zero program recently achieved superhuman performance in the game of Go, by tabula rasa reinforcement learning from games of self-play. In this paper, we generalise this approach into a single AlphaZero algorithm that can achieve, tabula rasa, superhuman performance in many challenging domains. Starting from random play, and given no domain knowledge except the game rules, AlphaZero achieved within 24 hours a superhuman level of play in the games of chess and shogi (Japanese chess) as well as Go, and convincingly defeated a world-champion program in each case.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1712.01815",
                "caption": "Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm",
                "credit": "David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, Timothy Lillicrap, Karen Simonyan, Demis Hassabis"
            }
        },
        {
            "start_date": {
                "year": 2017,
                "month": 12,
                "day": 5
            },
            "text": {
                "headline": "2-layer-LSTM+Deep-Gradient-Compression",
                "text": "<p>Large-scale distributed training requires significant communication bandwidth for gradient exchange that limits the scalability of multi-node training, and requires expensive high-bandwidth network infrastructure. The situation gets even worse with distributed training on mobile devices (federated learning), which suffers from higher latency, lower throughput, and intermittent poor connections. In this paper, we find 99.9% of the gradient exchange in distributed SGD is redundant, and propose Deep Gradient Compression (DGC) to greatly reduce the communication bandwidth. To preserve accuracy during compression, DGC employs four methods: momentum correction, local gradient clipping, momentum factor masking, and warm-up training. We have applied Deep Gradient Compression to image classification, speech recognition, and language modeling with multiple datasets including Cifar10, ImageNet, Penn Treebank, and Librispeech Corpus. On these scenarios, Deep Gradient Compression achieves a gradient compression ratio from 270x to 600x without losing accuracy, cutting the gradient size of ResNet-50 from 97MB to 0.35MB, and for DeepSpeech from 488MB to 0.74MB. Deep gradient compression enables large-scale distributed training on inexpensive commodity 1Gbps Ethernet and facilitates distributed training on mobile. Code is available at: this https URL.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1712.01887",
                "caption": "Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training",
                "credit": "Yujun Lin, Song Han, Huizi Mao, Yu Wang, William J. Dally"
            }
        },
        {
            "start_date": {
                "year": 2017,
                "month": 12,
                "day": 2
            },
            "text": {
                "headline": "PNASNet-5",
                "text": "<p>We propose a new method for learning the structure of convolutional neural networks (CNNs) that is more efficient than recent state-of-the-art methods based on reinforcement learning and evolutionary algorithms. Our approach uses a sequential model-based optimization (SMBO) strategy, in which we search for structures in order of increasing complexity, while simultaneously learning a surrogate model to guide the search through structure space. Direct comparison under the same search space shows that our method is up to 5 times more efficient than the RL method of Zoph et al. (2018) in terms of number of models evaluated, and 8 times faster in terms of total compute. The structures we discover in this way achieve state of the art classification accuracies on CIFAR-10 and ImageNet.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1712.00559",
                "caption": "Progressive Neural Architecture Search",
                "credit": "C Liu, B Zoph, M Neumann, J Shlens"
            }
        },
        {
            "start_date": {
                "year": 2017,
                "month": 12,
                "day": 2
            },
            "text": {
                "headline": "PNAS-net",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1712.00559",
                "caption": "Progressive Neural Architecture Search",
                "credit": "C Liu, B Zoph, M Neumann, J Shlens"
            }
        },
        {
            "start_date": {
                "year": 2017,
                "month": 11,
                "day": 21
            },
            "text": {
                "headline": "TriNet",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1703.07737",
                "caption": "In Defense of the Triplet Loss for Person Re-Identification",
                "credit": "Alexander Hermans, Lucas Beyer, Bastian Leibe"
            }
        },
        {
            "start_date": {
                "year": 2017,
                "month": 11,
                "day": 10
            },
            "text": {
                "headline": "AWD-LSTM-MoS + dynamic evaluation (WT2, 2017)",
                "text": "<p>We formulate language modeling as a matrix factorization problem, and show that the expressiveness of Softmax-based models (including the majority of neural language models) is limited by a Softmax bottleneck. Given that natural language is highly context-dependent, this further implies that in practice Softmax with distributed word embeddings does not have enough capacity to model natural language. We propose a simple and effective method to address this issue, and improve the state-of-the-art perplexities on Penn Treebank and WikiText-2 to 47.69 and 40.68 respectively. The proposed method also excels on the large-scale 1B Word dataset, outperforming the baseline by over 5.6 points in perplexity.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1711.03953",
                "caption": "Breaking the Softmax Bottleneck: A High-Rank RNN Language Model",
                "credit": "Zhilin Yang, Zihang Dai, Ruslan Salakhutdinov, William W. Cohen"
            }
        },
        {
            "start_date": {
                "year": 2017,
                "month": 10,
                "day": 31
            },
            "text": {
                "headline": "Fraternal dropout + AWD-LSTM 3-layer (WT2)",
                "text": "<p>Recurrent neural networks (RNNs) are important class of architectures among neural networks useful for language modeling and sequential prediction. However, optimizing RNNs is known to be harder compared to feed-forward neural networks. A number of techniques have been proposed in literature to address this problem. In this paper we propose a simple technique called fraternal dropout that takes advantage of dropout to achieve this goal. Specifically, we propose to train two identical copies of an RNN (that share parameters) with different dropout masks while minimizing the difference between their (pre-softmax) predictions. In this way our regularization encourages the representations of RNNs to be invariant to dropout mask, thus being robust. We show that our regularization term is upper bounded by the expectation-linear dropout objective which has been shown to address the gap due to the difference between the train and inference phases of dropout. We evaluate our model and achieve state-of-the-art results in sequence modeling tasks on two benchmark datasets - Penn Treebank and Wikitext-2. We also show that our approach leads to performance improvement by a significant margin in image captioning (Microsoft COCO) and semi-supervised (CIFAR-10) tasks.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1711.00066",
                "caption": "Fraternal Dropout",
                "credit": "Konrad Zolna, Devansh Arpit, Dendi Suhubdy, Yoshua Bengio"
            }
        },
        {
            "start_date": {
                "year": 2017,
                "month": 10,
                "day": 31
            },
            "text": {
                "headline": "DCN+",
                "text": "<p>Traditional models for question answering optimize using cross entropy loss, which encourages exact answers at the cost of penalizing nearby or overlapping answers that are sometimes equally accurate. We propose a mixed objective that combines cross entropy loss with self-critical policy learning. The objective uses rewards derived from word overlap to solve the misalignment between evaluation metric and optimization objective. In addition to the mixed objective, we improve dynamic coattention networks (DCN) with a deep residual coattention encoder that is inspired by recent work in deep self-attention and residual networks. Our proposals improve model performance across question types and input lengths, especially for long questions that requires the ability to capture long-term dependencies. On the Stanford Question Answering Dataset, our model achieves state-of-the-art results with 75.1% exact match accuracy and 83.1% F1, while the ensemble obtains 78.9% exact match accuracy and 86.0% F1. </p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1711.00106v2",
                "caption": "DCN+: Mixed Objective and Deep Residual Coattention for Question Answering",
                "credit": "Caiming Xiong, Victor Zhong, Richard Socher"
            }
        },
        {
            "start_date": {
                "year": 2017,
                "month": 10,
                "day": 29
            },
            "text": {
                "headline": "S-Norm",
                "text": "<p>We consider the problem of adapting neural paragraph-level question answering models to the case where entire documents are given as input. Our proposed solution trains models to produce well calibrated confidence scores for their results on individual paragraphs. We sample multiple paragraphs from the documents during training, and use a shared-normalization training objective that encourages the model to produce globally correct output. We combine this method with a state-of-the-art pipeline for training models on document QA data. Experiments demonstrate strong performance on several document QA datasets. Overall, we are able to achieve a score of 71.3 F1 on the web portion of TriviaQA, a large improvement from the 56.7 F1 of the previous best system.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1710.10723v2",
                "caption": "Simple and Effective Multi-Paragraph Reading Comprehension",
                "credit": "Christopher Clark, Matt Gardner"
            }
        },
        {
            "start_date": {
                "year": 2017,
                "month": 10,
                "day": 28
            },
            "text": {
                "headline": "PhraseCond",
                "text": "<p>Attention models have been intensively studied to improve NLP tasks such as machine comprehension via both question-aware passage attention model and self-matching attention model. Our research proposes phase conductor (PhaseCond) for attention models in two meaningful ways. First, PhaseCond, an architecture of multi-layered attention models, consists of multiple phases each implementing a stack of attention layers producing passage representations and a stack of inner or outer fusion layers regulating the information flow. Second, we extend and improve the dot-product attention function for PhaseCond by simultaneously encoding multiple question and passage embedding layers from different perspectives. We demonstrate the effectiveness of our proposed model PhaseCond on the SQuAD dataset, showing that our model significantly outperforms both state-of-the-art single-layered and multiple-layered attention models. We deepen our results with new findings via both detailed qualitative analysis and visualized examples showing the dynamic changes through multi-layered attention models.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1710.10504v2",
                "caption": "Phase Conductor on Multi-layered Attentions for Machine Comprehension",
                "credit": "Rui Liu, Wei Wei, Weiguang Mao, Maria Chikina"
            }
        },
        {
            "start_date": {
                "year": 2017,
                "month": 10,
                "day": 27
            },
            "text": {
                "headline": "ProgressiveGAN",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1710.10196",
                "caption": "Progressive Growing of GANs for Improved Quality, Stability, and Variation",
                "credit": "Tero Karras, Timo Aila, Samuli Laine, Jaakko Lehtinen"
            }
        },
        {
            "start_date": {
                "year": 2017,
                "month": 10,
                "day": 26
            },
            "text": {
                "headline": "CapsNet (MultiMNIST)",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1710.09829",
                "caption": "Dynamic Routing Between Capsules",
                "credit": "S Sabour, N Frosst, GE Hinton"
            }
        },
        {
            "start_date": {
                "year": 2017,
                "month": 10,
                "day": 26
            },
            "text": {
                "headline": "CapsNet (MNIST)",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1710.09829",
                "caption": "Dynamic Routing Between Capsules",
                "credit": "S Sabour, N Frosst, GE Hinton"
            }
        },
        {
            "start_date": {
                "year": 2017,
                "month": 10,
                "day": 22
            },
            "text": {
                "headline": "LRSO-GAN",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1701.07717",
                "caption": "Unlabeled Samples Generated by GAN Improve the Person Re-identification Baseline in vitro",
                "credit": "Zhedong Zheng, Liang Zheng, Yi Yang"
            }
        },
        {
            "start_date": {
                "year": 2017,
                "month": 10,
                "day": 19
            },
            "text": {
                "headline": "AlphaGo Master",
                "text": "<p>A long-standing goal of artificial intelligence is an algorithm that learns, tabula rasa, superhuman proficiency in challenging domains. Recently, AlphaGo became the first program to defeat a world champion in the game of Go. The tree search in AlphaGo evaluated positions and selected moves using deep neural networks. These neural networks were trained by supervised learning from human expert moves, and by reinforcement learning from self-play. Here we introduce an algorithm based solely on reinforcement learning, without human data, guidance or domain knowledge beyond game rules. AlphaGo becomes its own teacher: a neural network is trained to predict AlphaGo's own move selections and also the winner of AlphaGo's games. This neural network improves the strength of the tree search, resulting in higher quality move selection and stronger self-play in the next iteration. Starting tabula rasa, our new program AlphaGo Zero achieved superhuman performance, winning 100-0 against the previously published, champion-defeating AlphaGo. © 2017 Macmillan Publishers Limited, part of Springer Nature. All rights reserved.</p>"
            },
            "media": {
                "url": "https://www.nature.com/articles/nature24270",
                "caption": "Mastering the game of Go without human knowledge",
                "credit": "D Silver, J Schrittwieser, K Simonyan, I Antonoglou"
            }
        },
        {
            "start_date": {
                "year": 2017,
                "month": 10,
                "day": 18
            },
            "text": {
                "headline": "AlphaGo Zero",
                "text": "<p>A long-standing goal of artificial intelligence is an algorithm that learns, tabula rasa, superhuman proficiency in challenging domains. Recently, AlphaGo became the first program to defeat a world champion in the game of Go. The tree search in AlphaGo evaluated positions and selected moves using deep neural networks. These neural networks were trained by supervised learning from human expert moves, and by reinforcement learning from self-play. Here we introduce an algorithm based solely on reinforcement learning, without human data, guidance or domain knowledge beyond game rules. AlphaGo becomes its own teacher: a neural network is trained to predict AlphaGo's own move selections and also the winner of AlphaGo's games. This neural network improves the strength of the tree search, resulting in higher quality move selection and stronger self-play in the next iteration. Starting tabula rasa, our new program AlphaGo Zero achieved superhuman performance, winning 100-0 against the previously published, champion-defeating AlphaGo. © 2017 Macmillan Publishers Limited, part of Springer Nature. All rights reserved.</p>"
            },
            "media": {
                "url": "https://www.nature.com/articles/nature24270",
                "caption": "Mastering the game of Go without human knowledge",
                "credit": "D Silver, J Schrittwieser, K Simonyan, I Antonoglou"
            }
        },
        {
            "start_date": {
                "year": 2017,
                "month": 9,
                "day": 26
            },
            "text": {
                "headline": "AWD-LSTM+WT+Cache+IOG (WT2)",
                "text": "<p>This paper proposes a reinforcing method that refines the output layers of existing Recurrent Neural Network (RNN) language models. We refer to our proposed method as Input-to-Output Gate (IOG). IOG has an extremely simple structure, and thus, can be easily combined with any RNN language models. Our experiments on the Penn Treebank and WikiText-2 datasets demonstrate that IOG consistently boosts the performance of several different types of current topline RNN language models.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1709.08907",
                "caption": "Input-to-Output Gate to Improve RNN Language Models",
                "credit": "Sho Takase, Jun Suzuki, Masaaki Nagata"
            }
        },
        {
            "start_date": {
                "year": 2017,
                "month": 9,
                "day": 21
            },
            "text": {
                "headline": "LSTM + dynamic eval",
                "text": "<p>We present methodology for using dynamic evaluation to improve neural sequence models. Models are adapted to recent history via a gradient descent based mechanism, causing them to assign higher probabilities to re-occurring sequential patterns. Dynamic evaluation outperforms existing adaptation approaches in our comparisons. Dynamic evaluation improves the state-of-the-art word-level perplexities on the Penn Treebank and WikiText-2 datasets to 51.1 and 44.3 respectively, and the state-of-the-art character-level cross-entropies on the text8 and Hutter Prize datasets to 1.19 bits/char and 1.08 bits/char respectively.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1709.07432",
                "caption": "Dynamic Evaluation of Neural Sequence Models",
                "credit": "Ben Krause, Emmanuel Kahembwe, Iain Murray, Steve Renals"
            }
        },
        {
            "start_date": {
                "year": 2017,
                "month": 9,
                "day": 15
            },
            "text": {
                "headline": "ISS",
                "text": "<p>Model compression is significant for the wide adoption of Recurrent Neural Networks (RNNs) in both user devices possessing limited resources and business clusters requiring quick responses to large-scale service requests. This work aims to learn structurally-sparse Long Short-Term Memory (LSTM) by reducing the sizes of basic structures within LSTM units, including input updates, gates, hidden states, cell states and outputs. Independently reducing the sizes of basic structures can result in inconsistent dimensions among them, and consequently, end up with invalid LSTM units. To overcome the problem, we propose Intrinsic Sparse Structures (ISS) in LSTMs. Removing a component of ISS will simultaneously decrease the sizes of all basic structures by one and thereby always maintain the dimension consistency. By learning ISS within LSTM units, the obtained LSTMs remain regular while having much smaller basic structures. Based on group Lasso regularization, our method achieves 10.59x speedup without losing any perplexity of a language modeling of Penn TreeBank dataset. It is also successfully evaluated through a compact model with only 2.69M weights for machine Question Answering of SQuAD dataset. Our approach is successfully extended to non- LSTM RNNs, like Recurrent Highway Networks (RHNs). Our source code is publicly available at this https URL</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1709.05027",
                "caption": "Learning Intrinsic Sparse Structures within Long Short-Term Memory",
                "credit": "Wei Wen, Yuxiong He, Samyam Rajbhandari, Minjia Zhang, Wenhan Wang, Fang Liu, Bin Hu, Yiran Chen, Hai Li"
            }
        },
        {
            "start_date": {
                "year": 2017,
                "month": 9,
                "day": 6
            },
            "text": {
                "headline": "PyramidNet",
                "text": "<p>Deep convolutional neural networks (DCNNs) have shown remarkable performance in image classification tasks in recent years. Generally, deep neural network architectures are stacks consisting of a large number of convolutional layers, and they perform downsampling along the spatial dimension via pooling to reduce memory usage. Concurrently, the feature map dimension (i.e., the number of channels) is sharply increased at downsampling locations, which is essential to ensure effective performance because it increases the diversity of high-level attributes. This also applies to residual networks and is very closely related to their performance. In this research, instead of sharply increasing the feature map dimension at units that perform downsampling, we gradually increase the feature map dimension at all units to involve as many locations as possible. This design, which is discussed in depth together with our new insights, has proven to be an effective means of improving generalization ability. Furthermore, we propose a novel residual unit capable of further improving the classification accuracy with our new network architecture. Experiments on benchmark CIFAR-10, CIFAR-100, and ImageNet datasets have shown that our network architecture has superior generalization ability compared to the original residual networks. Code is available at this https URL}</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1610.02915v4",
                "caption": "Deep Pyramidal Residual Networks",
                "credit": "Dongyoon Han, Jiwhan Kim, Junmo Kim"
            }
        },
        {
            "start_date": {
                "year": 2017,
                "month": 9,
                "day": 5
            },
            "text": {
                "headline": "SENet (ImageNet)",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1709.01507",
                "caption": "Squeeze-and-Excitation Networks",
                "credit": "Jie Hu, Li Shen, Samuel Albanie, Gang Sun, Enhua Wu"
            }
        },
        {
            "start_date": {
                "year": 2017,
                "month": 8,
                "day": 29
            },
            "text": {
                "headline": "GL-LWGC-AWD-MoS-LSTM + dynamic evaluation (WT2)",
                "text": "<p>Recurrent Neural Networks (RNNs) achieve state-of-the-art results in many sequence-to-sequence modeling tasks. However, RNNs are difficult to train and tend to suffer from overfitting. Motivated by the Data Processing Inequality (DPI), we formulate the multi-layered network as a Markov chain, introducing a training method that comprises training the network gradually and using layer-wise gradient clipping. We found that applying our methods, combined with previously introduced regularization and optimization methods, resulted in improvements in state-of-the-art architectures operating in language modeling tasks.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1708.08863",
                "caption": "Gradual Learning of Recurrent Neural Networks",
                "credit": "Ziv Aharoni, Gal Rattner, Haim Permuter"
            }
        },
        {
            "start_date": {
                "year": 2017,
                "month": 8,
                "day": 19
            },
            "text": {
                "headline": "Libratus",
                "text": "<p>No-limit Texas Hold’em is the most popular variant of poker in the world. Heads-up no-limit Texas Hold’em is the main benchmark challenge for AI in imperfect-information games. We present Libratus, the first—and so far only—AI to defeat top human professionals in that game. Libratus’s architecture features three main modules, each of which has new algorithms: pre-computing a solution to an abstraction of the game which provides a high-level blueprint for the strategy of the AI, a new nested subgame-solving algorithm which repeatedly calculates a more detailed strategy as play progresses, and a self-improving module which augments the pre-computed blueprint over time.</p>"
            },
            "media": {
                "url": "https://www.ijcai.org/proceedings/2017/0772.pdf",
                "caption": "Libratus: The Superhuman AI for No-Limit Poker",
                "credit": "N Brown, T Sandholm, S Machine"
            }
        },
        {
            "start_date": {
                "year": 2017,
                "month": 8,
                "day": 17
            },
            "text": {
                "headline": "Adversarial Joint Adaptation Network (ResNet)",
                "text": "<p>Deep networks have been successfully applied to learn transferable features for adapting models from a source domain to a different target domain. In this paper, we present joint adaptation networks (JAN), which learn a transfer network by aligning the joint distributions of multiple domain-specific layers across domains based on a joint maximum mean discrepancy (JMMD) criterion. Adversarial training strategy is adopted to maximize JMMD such that the distributions of the source and target domains are made more distinguishable. Learning can be performed by stochastic gradient descent with the gradients computed by back-propagation in linear-time. Experiments testify that our model yields state of the art results on standard datasets.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1605.06636",
                "caption": "Deep Transfer Learning with Joint Adaptation Networks",
                "credit": "Mingsheng Long, Han Zhu, Jianmin Wang, Michael I. Jordan"
            }
        },
        {
            "start_date": {
                "year": 2017,
                "month": 8,
                "day": 16
            },
            "text": {
                "headline": "NeuMF (Pinterest)",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1708.05031",
                "caption": "Neural Collaborative Filtering",
                "credit": "X He, L Liao, H Zhang, L Nie, X Hu"
            }
        },
        {
            "start_date": {
                "year": 2017,
                "month": 8,
                "day": 15
            },
            "text": {
                "headline": "Cutout-regularized net",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1708.04552",
                "caption": "Improved Regularization of Convolutional Neural Networks with Cutout",
                "credit": " Terrance DeVries, Graham W. Taylor"
            }
        },
        {
            "start_date": {
                "year": 2017,
                "month": 8,
                "day": 14
            },
            "text": {
                "headline": "EI-REHN-1000D",
                "text": "<p>To model time-varying nonlinear temporal dynamics in sequential data, a recurrent network capable of varying and adjusting the recurrence depth between input intervals is examined. The recurrence depth is extended by several intermediate hidden state units, and the weight parameters involved in determining these units are dynamically calculated. The motivation behind the paper lies on overcoming a deficiency in Recurrent Highway Networks and improving their performances which are currently at the forefront of RNNs: 1) Determining the appropriate number of recurrent depth in RHN for different tasks is a huge burden and just setting it to a large number is computationally wasteful with possible repercussion in terms of performance degradation and high latency. Expanding on the idea of adaptive computation time (ACT), with the use of an elastic gate in the form of a rectified exponentially decreasing function taking on as arguments as previous hidden state and input, the proposed model is able to evaluate the appropriate recurrent depth for each input. The rectified gating function enables the most significant intermediate hidden state updates to come early such that significant performance gain is achieved early. 2) Updating the weights from that of previous intermediate layer offers a richer representation than the use of shared weights across all intermediate recurrence layers. The weight update procedure is just an expansion of the idea underlying hypernetworks. To substantiate the effectiveness of the proposed network, we conducted three experiments: regression on synthetic data, human activity recognition, and language modeling on the Penn Treebank dataset. The proposed networks showed better performance than other state-of-the-art recurrent networks in all three experiments.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1708.04116",
                "caption": "Early Improving Recurrent Elastic Highway Network",
                "credit": "Hyunsin Park, Chang D. Yoo"
            }
        },
        {
            "start_date": {
                "year": 2017,
                "month": 8,
                "day": 11
            },
            "text": {
                "headline": "OpenAI TI7 DOTA 1v1",
                "text": "<p>We’ve created a bot which beats the world’s top professionals at 1v1 matches of Dota 2 under standard tournament rules. The bot learned the game from scratch by self-play, and does not use imitation learning or tree search. This is a step towards building AI systems which accomplish well-defined goals in messy, complicated situations involving real humans.</p>"
            },
            "media": {
                "url": "https://openai.com/research/dota-2",
                "caption": "Dota 2",
                "credit": ""
            }
        },
        {
            "start_date": {
                "year": 2017,
                "month": 8,
                "day": 7
            },
            "text": {
                "headline": "RetinaNet-R101",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1708.02002",
                "caption": "Focal loss for dense object detection",
                "credit": "TY Lin, P Goyal, R Girshick, K He, P Dollar"
            }
        },
        {
            "start_date": {
                "year": 2017,
                "month": 8,
                "day": 7
            },
            "text": {
                "headline": "RetinaNet-R50",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1708.02002",
                "caption": "Focal loss for dense object detection",
                "credit": "TY Lin, P Goyal, R Girshick, K He"
            }
        },
        {
            "start_date": {
                "year": 2017,
                "month": 8,
                "day": 7
            },
            "text": {
                "headline": "AWD-LSTM - 3-layer LSTM (tied) + continuous cache pointer (WT2)",
                "text": "<p>Recurrent neural networks (RNNs), such as long short-term memory networks (LSTMs), serve as a fundamental building block for many sequence learning tasks, including machine translation, language modeling, and question answering. In this paper, we consider the specific problem of word-level language modeling and investigate strategies for regularizing and optimizing LSTM-based models. We propose the weight-dropped LSTM which uses DropConnect on hidden-to-hidden weights as a form of recurrent regularization. Further, we introduce NT-ASGD, a variant of the averaged stochastic gradient method, wherein the averaging trigger is determined using a non-monotonic condition as opposed to being tuned by the user. Using these and other regularization strategies, we achieve state-of-the-art word level perplexities on two data sets: 57.3 on Penn Treebank and 65.8 on WikiText-2. In exploring the effectiveness of a neural cache in conjunction with our proposed model, we achieve an even lower state-of-the-art perplexity of 52.8 on Penn Treebank and 52.0 on WikiText-2.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1708.02182",
                "caption": "Regularizing and Optimizing LSTM Language Models",
                "credit": "Stephen Merity, Nitish Shirish Keskar, Richard Socher"
            }
        },
        {
            "start_date": {
                "year": 2017,
                "month": 7,
                "day": 30
            },
            "text": {
                "headline": "GSM",
                "text": "<p>In this paper, we present the gated self-matching networks for reading comprehension style question answering, which aims to answer questions from a given passage. We first match the question and passage with gated attention-based recurrent networks to obtain the question-aware passage representation. Then we propose a self-matching attention mechanism to refine the representation by matching the passage against itself, which effectively encodes information from the whole passage. We finally employ the pointer networks to locate the positions of answers from the passages. We conduct extensive experiments on the SQuAD dataset. The single model achieves 71.3% on the evaluation metrics of exact match on the hidden test set, while the ensemble model further boosts the results to 75.9%. At the time of submission of the paper, our model holds the first place on the SQuAD leaderboard for both single and ensemble model.</p>"
            },
            "media": {
                "url": "https://aclanthology.org/P17-1018/",
                "caption": "Gated Self-Matching Networks for Reading Comprehension and Question Answering",
                "credit": "Wenhui Wang, Nan Yang, Furu Wei, Baobao Chang, Ming Zhou"
            }
        },
        {
            "start_date": {
                "year": 2017,
                "month": 7,
                "day": 25
            },
            "text": {
                "headline": "ConvS2S (ensemble of 8 models)",
                "text": "<p>The prevalent approach to sequence to sequence learning maps an input sequence to a variable length output sequence via recurrent neural networks. We introduce an architecture based entirely on convolutional neural networks. Compared to recurrent models, computations over all elements can be fully parallelized during training and optimization is easier since the number of non-linearities is fixed and independent of the input length. Our use of gated linear units eases gradient propagation and we equip each decoder layer with a separate attention module. We outperform the accuracy of the deep LSTM setup of Wu et al. (2016) on both WMT'14 English-German and WMT'14 English-French translation at an order of magnitude faster speed, both on GPU and CPU.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1705.03122",
                "caption": "Convolutional Sequence to Sequence Learning",
                "credit": "Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, Yann N. Dauphin"
            }
        },
        {
            "start_date": {
                "year": 2017,
                "month": 7,
                "day": 21
            },
            "text": {
                "headline": "PSPNet",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://ieeexplore.ieee.org/document/8100143",
                "caption": "Pyramid Scene Parsing Network",
                "credit": "Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, Jiaya Jia"
            }
        },
        {
            "start_date": {
                "year": 2017,
                "month": 7,
                "day": 21
            },
            "text": {
                "headline": "NASNet-A",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1707.07012",
                "caption": "Learning Transferable Architectures for Scalable Image Recognition",
                "credit": "B Zoph, V Vasudevan, J Shlens"
            }
        },
        {
            "start_date": {
                "year": 2017,
                "month": 7,
                "day": 18
            },
            "text": {
                "headline": "AWD-LSTM",
                "text": "<p>Ongoing innovations in recurrent neural network architectures have provided a steady influx of apparently state-of-the-art results on language modelling benchmarks. However, these have been evaluated using differing code bases and limited computational resources, which represent uncontrolled sources of experimental variation. We reevaluate several popular architectures and regularisation methods with large-scale automatic black-box hyperparameter tuning and arrive at the somewhat surprising conclusion that standard LSTM architectures, when properly regularised, outperform more recent models. We establish a new state of the art on the Penn Treebank and Wikitext-2 corpora, as well as strong baselines on the Hutter Prize dataset.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1707.05589",
                "caption": "On the State of the Art of Evaluation in Neural Language Models",
                "credit": "Gábor Melis, Chris Dyer, Phil Blunsom"
            }
        },
        {
            "start_date": {
                "year": 2017,
                "month": 7,
                "day": 10
            },
            "text": {
                "headline": "JFT",
                "text": "<p>The success of deep learning in vision can be attributed to: (a) models with high capacity; (b) increased computational power; and (c) availability of large-scale labeled data. Since 2012, there have been significant advances in representation capabilities of the models and computational capabilities of GPUs. But the size of the biggest dataset has surprisingly remained constant. What will happen if we increase the dataset size by 10x or 100x? This paper takes a step towards clearing the clouds of mystery surrounding the relationship between `enormous data' and visual deep learning. By exploiting the JFT-300M dataset which has more than 375M noisy labels for 300M images, we investigate how the performance of current vision tasks would change if this data was used for representation learning. Our paper delivers some surprising (and some expected) findings. First, we find that the performance on vision tasks increases logarithmically based on volume of training data size. Second, we show that representation learning (or pre-training) still holds a lot of promise. One can improve performance on many vision tasks by just training a better base model. Finally, as expected, we present new state-of-the-art results for different vision tasks including image classification, object detection, semantic segmentation and human pose estimation. Our sincere hope is that this inspires vision community to not undervalue the data and develop collective efforts in building larger datasets.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1707.02968",
                "caption": "Revisiting Unreasonable Effectiveness of Data in Deep Learning Era.",
                "credit": "Chen Sun, Abhinav Shrivastava, Saurabh Singh, Abhinav Gupta"
            }
        },
        {
            "start_date": {
                "year": 2017,
                "month": 7,
                "day": 3
            },
            "text": {
                "headline": "ShuffleNet v1",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1707.01083",
                "caption": "ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices",
                "credit": "X Zhang, X Zhou, M Lin, J Sun"
            }
        },
        {
            "start_date": {
                "year": 2017,
                "month": 6,
                "day": 30
            },
            "text": {
                "headline": "NoisyNet-Dueling",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1706.10295v3",
                "caption": "Noisy Networks for Exploration",
                "credit": "M Fortunato, MG Azar, B Piot, J Menick"
            }
        },
        {
            "start_date": {
                "year": 2017,
                "month": 6,
                "day": 17
            },
            "text": {
                "headline": "DeepLabV3",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1706.05587",
                "caption": "Rethinking Atrous Convolution for Semantic Image Segmentation",
                "credit": "Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, Hartwig Adam"
            }
        },
        {
            "start_date": {
                "year": 2017,
                "month": 6,
                "day": 13
            },
            "text": {
                "headline": "HRA",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1706.04208",
                "caption": "Hybrid Reward Architecture for Reinforcement Learning",
                "credit": "H Van Seijen, M Fatemi, J Romoff, R Laroche"
            }
        },
        {
            "start_date": {
                "year": 2017,
                "month": 6,
                "day": 12
            },
            "text": {
                "headline": "Transformer",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf",
                "caption": "Attention Is All You Need",
                "credit": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin"
            }
        },
        {
            "start_date": {
                "year": 2017,
                "month": 6,
                "day": 10
            },
            "text": {
                "headline": "EDSR",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1707.02921",
                "caption": "Enhanced Deep Residual Networks for Single Image Super-Resolution",
                "credit": "Bee Lim, Sanghyun Son, Heewon Kim, Seungjun Nah, Kyoung Mu Lee"
            }
        },
        {
            "start_date": {
                "year": 2017,
                "month": 6,
                "day": 8
            },
            "text": {
                "headline": "Reading Twice for NLU",
                "text": "<p>Common-sense and background knowledge is required to understand natural language, but in most neural natural language understanding (NLU) systems, this knowledge must be acquired from training corpora during learning, and then it is static at test time. We introduce a new architecture for the dynamic integration of explicit background knowledge in NLU models. A general-purpose reading module reads background knowledge in the form of free-text statements (together with task-specific text inputs) and yields refined word representations to a task-specific NLU architecture that reprocesses the task inputs with these representations. Experiments on document question answering (DQA) and recognizing textual entailment (RTE) demonstrate the effectiveness and flexibility of the approach. Analysis shows that our model learns to exploit knowledge in a semantically appropriate way. </p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1706.02596v3",
                "caption": "Dynamic Integration of Background Knowledge in Neural NLU Systems",
                "credit": "Dirk Weissenborn, Tomáš Kočiský, Chris Dyer"
            }
        },
        {
            "start_date": {
                "year": 2017,
                "month": 6,
                "day": 7
            },
            "text": {
                "headline": "PointNet++",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1706.02413",
                "caption": "PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space",
                "credit": "Charles R. Qi, Li Yi, Hao Su, Leonidas J. Guibas"
            }
        },
        {
            "start_date": {
                "year": 2017,
                "month": 6,
                "day": 1
            },
            "text": {
                "headline": "Inflated 3D ConvNet",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1705.07750",
                "caption": "Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset",
                "credit": "Joao Carreira, Andrew Zisserman"
            }
        },
        {
            "start_date": {
                "year": 2017,
                "month": 5,
                "day": 25
            },
            "text": {
                "headline": "SRGAN",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://openaccess.thecvf.com/content_cvpr_2017/html/Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper.html",
                "caption": "Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network",
                "credit": "Christian Ledig, Lucas Theis, Ferenc Huszar, Jose Caballero, Andrew Cunningham, Alejandro Acosta, Andrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, Wenzhe Shi"
            }
        },
        {
            "start_date": {
                "year": 2017,
                "month": 5,
                "day": 8
            },
            "text": {
                "headline": "Mnemonic Reader",
                "text": "<p>In this paper, we introduce the Reinforced Mnemonic Reader for machine reading comprehension tasks, which enhances previous attentive readers in two aspects. First, a reattention mechanism is proposed to refine current attentions by directly accessing to past attentions that are temporally memorized in a multi-round alignment architecture, so as to avoid the problems of attention redundancy and attention deficiency. Second, a new optimization approach, called dynamic-critical reinforcement learning, is introduced to extend the standard supervised method. It always encourages to predict a more acceptable answer so as to address the convergence suppression problem occurred in traditional reinforcement learning algorithms. Extensive experiments on the Stanford Question Answering Dataset (SQuAD) show that our model achieves state-of-the-art results. Meanwhile, our model outperforms previous systems by over 6% in terms of both Exact Match and F1 metrics on two adversarial SQuAD datasets. </p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1705.02798v6",
                "caption": "Reinforced Mnemonic Reader for Machine Reading Comprehension",
                "credit": "Minghao Hu, Yuxing Peng, Zhen Huang, Xipeng Qiu, Furu Wei, Ming Zhou"
            }
        },
        {
            "start_date": {
                "year": 2017,
                "month": 4,
                "day": 27
            },
            "text": {
                "headline": "DeepLab (2017)",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://ieeexplore.ieee.org/abstract/document/7913730",
                "caption": "DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs",
                "credit": "Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, Alan L. Yuille"
            }
        },
        {
            "start_date": {
                "year": 2017,
                "month": 4,
                "day": 17
            },
            "text": {
                "headline": "MobileNet",
                "text": "<p>We present a class of efficient models called MobileNets for mobile and embedded vision applications. MobileNets are based on a streamlined architecture that uses depth-wise separable convolutions to build light weight deep neural networks. We introduce two simple global hyper-parameters that efficiently trade off between latency and accuracy. These hyper-parameters allow the model builder to choose the right sized model for their application based on the constraints of the problem. We present extensive experiments on resource and accuracy tradeoffs and show strong performance compared to other popular models on ImageNet classification. We then demonstrate the effectiveness of MobileNets across a wide range of applications and use cases including object detection, finegrain classification, face attributes and large scale geo-localization.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1704.04861",
                "caption": "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications",
                "credit": "AG Howard, M Zhu, B Chen, D Kalenichenko"
            }
        },
        {
            "start_date": {
                "year": 2017,
                "month": 3,
                "day": 31
            },
            "text": {
                "headline": "WGAN-GP",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1704.00028",
                "caption": "Improved Training of Wasserstein GANs",
                "credit": "Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, Aaron Courville"
            }
        },
        {
            "start_date": {
                "year": 2017,
                "month": 3,
                "day": 30
            },
            "text": {
                "headline": "Mask R-CNN",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1703.06870",
                "caption": "Mask R-CNN",
                "credit": "Kaiming He, Georgia Gkioxari, Piotr Dollár, Ross Girshick"
            }
        },
        {
            "start_date": {
                "year": 2017,
                "month": 3,
                "day": 15
            },
            "text": {
                "headline": "Prototypical networks",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1703.05175",
                "caption": "Prototypical Networks for Few-shot Learning",
                "credit": " Jake Snell, Kevin Swersky, Richard S. Zemel"
            }
        },
        {
            "start_date": {
                "year": 2017,
                "month": 2,
                "day": 1
            },
            "text": {
                "headline": "DnCNN",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://ieeexplore.ieee.org/abstract/document/7839189",
                "caption": "Beyond a Gaussian Denoiser: Residual Learning of Deep CNN for Image Denoising",
                "credit": "Kai Zhang, Wangmeng Zuo, Yunjin Chen, Deyu Meng, Lei Zhang"
            }
        },
        {
            "start_date": {
                "year": 2017,
                "month": 1,
                "day": 23
            },
            "text": {
                "headline": "MoE-Multi",
                "text": "<p>The capacity of a neural network to absorb information is limited by its number of parameters. Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation. In practice, however, there are significant algorithmic and performance challenges. In this work, we address these challenges and finally realize the promise of conditional computation, achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters. We introduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks. A trainable gating network determines a sparse combination of these experts to use for each example. We apply the MoE to the tasks of language modeling and machine translation, where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora. We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers. On large language modeling and machine translation benchmarks, these models achieve significantly better results than state-of-the-art at lower computational cost.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1701.06538",
                "caption": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer",
                "credit": "N Shazeer, A Mirhoseini, K Maziarz, A Davis"
            }
        },
        {
            "start_date": {
                "year": 2017,
                "month": 1,
                "day": 7
            },
            "text": {
                "headline": "OR-WideResNet",
                "text": "<p>Deep Convolution Neural Networks (DCNNs) are capable of learning unprecedentedly effective image representations. However, their ability in handling significant local and global image rotations remains limited. In this paper, we propose Active Rotating Filters (ARFs) that actively rotate during convolution and produce feature maps with location and orientation explicitly encoded. An ARF acts as a virtual filter bank containing the filter itself and its multiple unmaterialised rotated versions. During back-propagation, an ARF is collectively updated using errors from all its rotated versions. DCNNs using ARFs, referred to as Oriented Response Networks (ORNs), can produce within-class rotation-invariant deep features while maintaining inter-class discrimination for classification tasks. The oriented response produced by ORNs can also be used for image and object orientation estimation tasks. Over multiple state-of-the-art DCNN architectures, such as VGG, ResNet, and STN, we consistently observe that replacing regular filters with the proposed ARFs leads to significant reduction in the number of network parameters and improvement in classification performance. We report the best results on several commonly used benchmarks.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1701.01833v2",
                "caption": "Oriented Response Networks",
                "credit": "Yanzhao Zhou, Qixiang Ye, Qiang Qiu and Jianbin Jiao"
            }
        },
        {
            "start_date": {
                "year": 2017,
                "month": 1,
                "day": 6
            },
            "text": {
                "headline": "DeepStack",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1701.01724",
                "caption": "DeepStack: Expert-Level Artificial Intelligence in No-Limit Poker",
                "credit": "Matej Moravčík, Martin Schmid, Neil Burch, Viliam Lisý, Dustin Morrill, Nolan Bard, Trevor Davis, Kevin Waugh, Michael Johanson, Michael Bowling"
            }
        },
        {
            "start_date": {
                "year": 2016,
                "month": 12,
                "day": 25
            },
            "text": {
                "headline": "YOLOv2",
                "text": "<p>We introduce YOLO9000, a state-of-the-art, real-time object detection system that can detect over 9000 object categories. First we propose various improvements to the YOLO detection method, both novel and drawn from prior work. The improved model, YOLOv2, is state-of-the-art on standard detection tasks like PASCAL VOC and COCO. At 67 FPS, YOLOv2 gets 76.8 mAP on VOC 2007. At 40 FPS, YOLOv2 gets 78.6 mAP, outperforming state-of-the-art methods like Faster RCNN with ResNet and SSD while still running significantly faster. Finally we propose a method to jointly train on object detection and classification. Using this method we train YOLO9000 simultaneously on the COCO detection dataset and the ImageNet classification dataset. Our joint training allows YOLO9000 to predict detections for object classes that don't have labelled detection data. We validate our approach on the ImageNet detection task. YOLO9000 gets 19.7 mAP on the ImageNet detection validation set despite only having detection data for 44 of the 200 classes. On the 156 classes not in COCO, YOLO9000 gets 16.0 mAP. But YOLO can detect more than just 200 classes; it predicts detections for more than 9000 different object categories. And it still runs in real-time.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1612.08242",
                "caption": "YOLO9000: Better, Faster, Stronger",
                "credit": "Joseph Redmon, Ali Farhadi"
            }
        },
        {
            "start_date": {
                "year": 2016,
                "month": 12,
                "day": 23
            },
            "text": {
                "headline": "GCNN-14",
                "text": "<p>The pre-dominant approach to language modeling to date is based on recurrent neural networks. Their success on this task is often linked to their ability to capture unbounded context. In this paper we develop a finite context approach through stacked convolutions, which can be more efficient since they allow parallelization over sequential tokens. We propose a novel simplified gating mechanism that outperforms Oord et al (2016) and investigate the impact of key architectural decisions. The proposed approach achieves state-of-the-art on the WikiText-103 benchmark, even though it features long-term dependencies, as well as competitive results on the Google Billion Words benchmark. Our model reduces the latency to score a sentence by an order of magnitude compared to a recurrent baseline. To our knowledge, this is the first time a non-recurrent approach is competitive with strong recurrent models on these large scale language tasks.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1612.08083",
                "caption": "Language Modeling with Gated Convolutional Networks, Language Modeling with Gated Convolutional Networks",
                "credit": "Yann N. Dauphin, Angela Fan, Michael Auli, David Grangier, Yann N. Dauphin, Angela Fan, Michael Auli, David Grangier"
            }
        },
        {
            "start_date": {
                "year": 2016,
                "month": 12,
                "day": 13
            },
            "text": {
                "headline": "Diabetic Retinopathy Detection Net",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://jamanetwork.com/journals/jama/article-abstract/2588763",
                "caption": "Development and Validation of a Deep Learning Algorithm for Detection of Diabetic Retinopathy in Retinal Fundus Photographs",
                "credit": "V Gulshan, L Peng, M Coram, MC Stumpe, D Wu"
            }
        },
        {
            "start_date": {
                "year": 2016,
                "month": 12,
                "day": 5
            },
            "text": {
                "headline": "GAN-Advancer",
                "text": "<p>We present a variety of new architectural features and training procedures that we apply to the generative adversarial networks (GANs) framework. Using our new techniques, we achieve state-of-the-art results in semi-supervised classification on MNIST, CIFAR-10 and SVHN. The generated images are of high quality as confirmed by a visual Turing test: our model generates MNIST samples that humans cannot distinguish from real data, and CIFAR-10 samples that yield a human error rate of 21.3%. We also present ImageNet samples with unprecedented resolution and show that our methods enable the model to learn recognizable features of ImageNet classes.</p>"
            },
            "media": {
                "url": "https://dl.acm.org/doi/10.5555/3157096.3157346",
                "caption": "Improved Techniques for Training GANs",
                "credit": "Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, Xi Chen"
            }
        },
        {
            "start_date": {
                "year": 2016,
                "month": 12,
                "day": 2
            },
            "text": {
                "headline": "PointNet",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1612.00593",
                "caption": "PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation",
                "credit": "CR Qi, H Su, K Mo, LJ Guibas"
            }
        },
        {
            "start_date": {
                "year": 2016,
                "month": 12,
                "day": 2
            },
            "text": {
                "headline": "Elastic weight consolidation",
                "text": "<p>The ability to learn tasks in a sequential fashion is crucial to the development of artificial intelligence. Neural networks are not, in general, capable of this and it has been widely thought that catastrophic forgetting is an inevitable feature of connectionist models. We show that it is possible to overcome this limitation and train networks that can maintain expertise on tasks which they have not experienced for a long time. Our approach remembers old tasks by selectively slowing down learning on the weights important for those tasks. We demonstrate our approach is scalable and effective by solving a set of classification tasks based on the MNIST hand written digit dataset and by learning several Atari 2600 games sequentially.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1612.00796",
                "caption": "Overcoming catastrophic forgetting in neural networks",
                "credit": "James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A. Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, Demis Hassabis, Claudia Clopath, Dharshan Kumaran, Raia Hadsell"
            }
        },
        {
            "start_date": {
                "year": 2016,
                "month": 11,
                "day": 21
            },
            "text": {
                "headline": "Image-to-image cGAN",
                "text": "<p>We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. We demonstrate that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks. Indeed, since the release of the pix2pix software associated with this paper, a large number of internet users (many of them artists) have posted their own experiments with our system, further demonstrating its wide applicability and ease of adoption without the need for parameter tweaking. As a community, we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without hand-engineering our loss functions either.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1611.07004",
                "caption": "Image-to-Image Translation with Conditional Adversarial Networks",
                "credit": "Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, Alexei A. Efros"
            }
        },
        {
            "start_date": {
                "year": 2016,
                "month": 11,
                "day": 20
            },
            "text": {
                "headline": "RefineNet",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1611.06612v3",
                "caption": "RefineNet: Multi-Path Refinement Networks for High-Resolution Semantic Segmentation",
                "credit": "Guosheng Lin, Anton Milan, Chunhua Shen, Ian Reid"
            }
        },
        {
            "start_date": {
                "year": 2016,
                "month": 11,
                "day": 17
            },
            "text": {
                "headline": "PolyNet",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1611.05725",
                "caption": "PolyNet: A Pursuit of Structural Diversity in Very Deep Networks",
                "credit": "X Zhang, Z Li, C Change Loy"
            }
        },
        {
            "start_date": {
                "year": 2016,
                "month": 11,
                "day": 16
            },
            "text": {
                "headline": "ResNeXt-50",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1611.05431",
                "caption": "Aggregated Residual Transformations for Deep Neural Networks",
                "credit": "Saining Xie, Ross Girshick, Piotr Dollár, Zhuowen Tu, Kaiming He"
            }
        },
        {
            "start_date": {
                "year": 2016,
                "month": 11,
                "day": 11
            },
            "text": {
                "headline": "Deeply-recursive ConvNet",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1511.04491",
                "caption": "Deeply-Recursive Convolutional Network for Image Super-Resolution",
                "credit": "Jiwon Kim, Jung Kwon Lee, Kyoung Mu Lee"
            }
        },
        {
            "start_date": {
                "year": 2016,
                "month": 11,
                "day": 5
            },
            "text": {
                "headline": "NASv3 (CIFAR-10)",
                "text": "<p>Neural networks are powerful and flexible models that work well for many difficult learning tasks in image, speech and natural language understanding. Despite their success, neural networks are still hard to design. In this paper, we use a recurrent network to generate the model descriptions of neural networks and train this RNN with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set. On the CIFAR-10 dataset, our method, starting from scratch, can design a novel network architecture that rivals the best human-invented architecture in terms of test set accuracy. Our CIFAR-10 model achieves a test error rate of 3.65, which is 0.09 percent better and 1.05x faster than the previous state-of-the-art model that used a similar architectural scheme. On the Penn Treebank dataset, our model can compose a novel recurrent cell that outperforms the widely-used LSTM cell, and other state-of-the-art baselines. Our cell achieves a test set perplexity of 62.4 on the Penn Treebank, which is 3.6 perplexity better than the previous state-of-the-art model. The cell can also be transferred to the character language modeling task on PTB and achieves a state-of-the-art perplexity of 1.214.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1611.01578",
                "caption": "Neural Architecture Search with Reinforcement Learning",
                "credit": "Barret Zoph, Quoc V. Le"
            }
        },
        {
            "start_date": {
                "year": 2016,
                "month": 11,
                "day": 5
            },
            "text": {
                "headline": "NAS with base 8 and shared embeddings",
                "text": "<p>Neural networks are powerful and flexible models that work well for many difficult learning tasks in image, speech and natural language understanding. Despite their success, neural networks are still hard to design. In this paper, we use a recurrent network to generate the model descriptions of neural networks and train this RNN with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set. On the CIFAR-10 dataset, our method, starting from scratch, can design a novel network architecture that rivals the best human-invented architecture in terms of test set accuracy. Our CIFAR-10 model achieves a test error rate of 3.65, which is 0.09 percent better and 1.05x faster than the previous state-of-the-art model that used a similar architectural scheme. On the Penn Treebank dataset, our model can compose a novel recurrent cell that outperforms the widely-used LSTM cell, and other state-of-the-art baselines. Our cell achieves a test set perplexity of 62.4 on the Penn Treebank, which is 3.6 perplexity better than the previous state-of-the-art model. The cell can also be transferred to the character language modeling task on PTB and achieves a state-of-the-art perplexity of 1.214.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1611.01578",
                "caption": "Neural Architecture Search with Reinforcement Learning",
                "credit": "Barret Zoph, Quoc V. Le"
            }
        },
        {
            "start_date": {
                "year": 2016,
                "month": 11,
                "day": 5
            },
            "text": {
                "headline": "BIDAF",
                "text": "<p>Machine comprehension (MC), answering a query about a given context paragraph, requires modeling complex interactions between the context and the query. Recently, attention mechanisms have been successfully extended to MC. Typically these methods use attention to focus on a small portion of the context and summarize it with a fixed-size vector, couple attentions temporally, and/or often form a uni-directional attention. In this paper we introduce the Bi-Directional Attention Flow (BIDAF) network, a multi-stage hierarchical process that represents the context at different levels of granularity and uses bi-directional attention flow mechanism to obtain a query-aware context representation without early summarization. Our experimental evaluations show that our model achieves the state-of-the-art results in Stanford Question Answering Dataset (SQuAD) and CNN/DailyMail cloze test. </p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1611.01603v6",
                "caption": "Bidirectional Attention Flow for Machine Comprehension",
                "credit": "Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, Hannaneh Hajishirzi"
            }
        },
        {
            "start_date": {
                "year": 2016,
                "month": 11,
                "day": 4
            },
            "text": {
                "headline": "VD-LSTM+REAL Large",
                "text": "<p>Recurrent neural networks have been very successful at predicting sequences of words in tasks such as language modeling. However, all such models are based on the conventional classification framework, where the model is trained against one-hot targets, and each word is represented both as an input and as an output in isolation. This causes inefficiencies in learning both in terms of utilizing all of the information and in terms of the number of parameters needed to train. We introduce a novel theoretical framework that facilitates better learning in language modeling, and show that our framework leads to tying together the input embedding and the output projection matrices, greatly reducing the number of trainable variables. Our framework leads to state of the art performance on the Penn Treebank with a variety of network models.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1611.01462",
                "caption": "Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling",
                "credit": "Hakan Inan, Khashayar Khosravi, Richard Socher"
            }
        },
        {
            "start_date": {
                "year": 2016,
                "month": 10,
                "day": 28
            },
            "text": {
                "headline": "SPIDER2",
                "text": "<p>Predicting one-dimensional structure properties has played an important role to improve prediction of protein three-dimensional structures and functions. The most commonly predicted properties are secondary structure and accessible surface area (ASA) representing local and nonlocal structural characteristics, respectively. Secondary structure prediction is further complemented by prediction of continuous main-chain torsional angles. Here we describe a newly developed method SPIDER2 that utilizes three iterations of deep learning neural networks to improve the prediction accuracy of several structural properties simultaneously. For an independent test set of 1199 proteins SPIDER2 achieves 82 % accuracy for secondary structure prediction, 0.76 for the correlation coefficient between predicted and actual solvent accessible surface area, 19° and 30° for mean absolute errors of backbone φ and ψ angles, respectively, and 8° and 32° for mean absolute errors of Cα-based θ and τ angles, respectively. The method provides state-of-the-art, all-in-one accurate prediction of local structure and solvent accessible surface area. The method is implemented, as a webserver along with a standalone package that are available in our website: http://sparks-lab.org.</p>"
            },
            "media": {
                "url": "https://link.springer.com/protocol/10.1007/978-1-4939-6406-2_6",
                "caption": "SPIDER2: A Package to Predict Secondary Structure, Accessible Surface Area, and Main-Chain Torsional Angles by Deep Neural Networks",
                "credit": "Yuedong Yang, Rhys Heffernan, Kuldip Paliwal, James Lyons, Abdollah Dehzangi, Alok Sharma, Jihua Wang, Abdul Sattar, and Yaoqi Zhou"
            }
        },
        {
            "start_date": {
                "year": 2016,
                "month": 10,
                "day": 12
            },
            "text": {
                "headline": "Differentiable neural computer",
                "text": "<p>Artificial neural networks are remarkably adept at sensory processing, sequence learning and reinforcement learning, but are limited in their ability to represent variables and data structures and to store data over long timescales, owing to the lack of an external memory. Here we introduce a machine learning model called a differentiable neural computer (DNC), which consists of a neural network that can read from and write to an external memory matrix, analogous to the random-access memory in a conventional computer. Like a conventional computer, it can use its memory to represent and manipulate complex data structures, but, like a neural network, it can learn to do so from data. When trained with supervised learning, we demonstrate that a DNC can successfully answer synthetic questions designed to emulate reasoning and inference problems in natural language. We show that it can learn tasks such as finding the shortest path between specified points and inferring the missing links in randomly generated graphs, and then generalize these tasks to specific graphs such as transport networks and family trees. When trained with reinforcement learning, a DNC can complete a moving blocks puzzle in which changing goals are specified by sequences of symbols. Taken together, our results demonstrate that DNCs have the capacity to solve complex, structured tasks that are inaccessible to neural networks without external read–write memory.</p>"
            },
            "media": {
                "url": "https://www.nature.com/articles/nature20101",
                "caption": "Hybrid computing using a neural network with dynamic external memory",
                "credit": "Alex Graves, Greg Wayne, Malcolm Reynolds, Tim Harley, Ivo Danihelka, Agnieszka Grabska-Barwińska, Sergio Gómez Colmenarejo, Edward Grefenstette, Tiago Ramalho, John Agapiou, Adrià Puigdomènech Badia, Karl Moritz Hermann, Yori Zwols, Georg Ostrovski, Adam Cain, Helen King, Christopher Summerfield, Phil Blunsom, Koray Kavukcuoglu & Demis Hassabis"
            }
        },
        {
            "start_date": {
                "year": 2016,
                "month": 10,
                "day": 7
            },
            "text": {
                "headline": "Xception",
                "text": "<p>We present an interpretation of Inception modules in convolutional neural networks as being an intermediate step in-between regular convolution and the depthwise separable convolution operation (a depthwise convolution followed by a pointwise convolution). In this light, a depthwise separable convolution can be understood as an Inception module with a maximally large number of towers. This observation leads us to propose a novel deep convolutional neural network architecture inspired by Inception, where Inception modules have been replaced with depthwise separable convolutions. We show that this architecture, dubbed Xception, slightly outperforms Inception V3 on the ImageNet dataset (which Inception V3 was designed for), and significantly outperforms Inception V3 on a larger image classification dataset comprising 350 million images and 17,000 classes. Since the Xception architecture has the same number of parameters as Inception V3, the performance gains are not due to increased capacity but rather to a more efficient use of model parameters.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1610.02357",
                "caption": "Xception: Deep Learning with Depthwise Separable Convolutions",
                "credit": "François Chollet"
            }
        },
        {
            "start_date": {
                "year": 2016,
                "month": 9,
                "day": 26
            },
            "text": {
                "headline": "Zoneout + Variational LSTM (WT2)",
                "text": "<p>Recent neural network sequence models with softmax classifiers have achieved their best language modeling performance only with very large hidden states and large vocabularies. Even then they struggle to predict rare or unseen words even if the context makes the prediction unambiguous. We introduce the pointer sentinel mixture architecture for neural sequence models which has the ability to either reproduce a word from the recent context or produce a word from a standard softmax classifier. Our pointer sentinel-LSTM model achieves state of the art language modeling performance on the Penn Treebank (70.9 perplexity) while using far fewer parameters than a standard softmax LSTM. In order to evaluate how well language models can exploit longer contexts and deal with more realistic vocabularies and larger corpora we also introduce the freely available WikiText corpus.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1609.07843",
                "caption": "Pointer Sentinel Mixture Models",
                "credit": "Stephen Merity, Caiming Xiong, James Bradbury, Richard Socher"
            }
        },
        {
            "start_date": {
                "year": 2016,
                "month": 9,
                "day": 26
            },
            "text": {
                "headline": "Pointer Sentinel-LSTM (medium)",
                "text": "<p>Recent neural network sequence models with softmax classifiers have achieved their best language modeling performance only with very large hidden states and large vocabularies. Even then they struggle to predict rare or unseen words even if the context makes the prediction unambiguous. We introduce the pointer sentinel mixture architecture for neural sequence models which has the ability to either reproduce a word from the recent context or produce a word from a standard softmax classifier. Our pointer sentinel-LSTM model achieves state of the art language modeling performance on the Penn Treebank (70.9 perplexity) while using far fewer parameters than a standard softmax LSTM. In order to evaluate how well language models can exploit longer contexts and deal with more realistic vocabularies and larger corpora we also introduce the freely available WikiText corpus.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1609.07843",
                "caption": "Pointer Sentinel Mixture Models",
                "credit": "Stephen Merity, Caiming Xiong, James Bradbury, Richard Socher"
            }
        },
        {
            "start_date": {
                "year": 2016,
                "month": 9,
                "day": 26
            },
            "text": {
                "headline": "GNMT",
                "text": "<p>Neural Machine Translation (NMT) is an end-to-end learning approach for automated translation, with the potential to overcome many of the weaknesses of conventional phrase-based translation systems. Unfortunately, NMT systems are known to be computationally expensive both in training and in translation inference. Also, most NMT systems have difficulty with rare words. These issues have hindered NMT's use in practical deployments and services, where both accuracy and speed are essential. In this work, we present GNMT, Google's Neural Machine Translation system, which attempts to address many of these issues. Our model consists of a deep LSTM network with 8 encoder and 8 decoder layers using attention and residual connections. To improve parallelism and therefore decrease training time, our attention mechanism connects the bottom layer of the decoder to the top layer of the encoder. To accelerate the final translation speed, we employ low-precision arithmetic during inference computations. To improve handling of rare words, we divide words into a limited set of common sub-word units (\"wordpieces\") for both input and output. This method provides a good balance between the flexibility of \"character\"-delimited models and the efficiency of \"word\"-delimited models, naturally handles translation of rare words, and ultimately improves the overall accuracy of the system. Our beam search technique employs a length-normalization procedure and uses a coverage penalty, which encourages generation of an output sentence that is most likely to cover all the words in the source sentence. On the WMT'14 English-to-French and English-to-German benchmarks, GNMT achieves competitive results to state-of-the-art. Using a human side-by-side evaluation on a set of isolated simple sentences, it reduces translation errors by an average of 60% compared to Google's phrase-based production system.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1609.08144",
                "caption": "Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation",
                "credit": "Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff Klingner, Apurva Shah, Melvin Johnson, Xiaobing Liu, Łukasz Kaiser, Stephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith Stevens, George Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals, Greg Corrado, Macduff Hughes, Jeffrey Dean"
            }
        },
        {
            "start_date": {
                "year": 2016,
                "month": 9,
                "day": 19
            },
            "text": {
                "headline": "Wide Residual Network",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1605.07146",
                "caption": "Wide Residual Networks",
                "credit": "Sergey Zagoruyko, Nikos Komodakis"
            }
        },
        {
            "start_date": {
                "year": 2016,
                "month": 9,
                "day": 17
            },
            "text": {
                "headline": "TSN",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://link.springer.com/chapter/10.1007/978-3-319-46484-8_2",
                "caption": "Temporal Segment Networks: Towards Good Practices for Deep Action Recognition",
                "credit": "Limin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao, Dahua Lin, Xiaoou Tang, Luc Van Gool"
            }
        },
        {
            "start_date": {
                "year": 2016,
                "month": 9,
                "day": 17
            },
            "text": {
                "headline": "Stacked hourglass network",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://link.springer.com/chapter/10.1007/978-3-319-46484-8_29",
                "caption": "Stacked Hourglass Networks for Human Pose Estimation",
                "credit": "Alejandro Newell, Kaiyu Yang, Jia Deng"
            }
        },
        {
            "start_date": {
                "year": 2016,
                "month": 9,
                "day": 17
            },
            "text": {
                "headline": "ResNet-1001",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://link.springer.com/chapter/10.1007/978-3-319-46493-0_38",
                "caption": "Identity Mappings in Deep Residual Networks",
                "credit": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun"
            }
        },
        {
            "start_date": {
                "year": 2016,
                "month": 9,
                "day": 17
            },
            "text": {
                "headline": "ResNet-200",
                "text": "<p>Deep residual networks have emerged as a family of extremely deep architectures showing compelling accuracy and nice convergence behaviors. In this paper, we analyze the propagation formulations behind the residual building blocks, which suggest that the forward and backward signals can be directly propagated from one block to any other block, when using identity mappings as the skip connections and after-addition activation. A series of ablation experiments support the importance of these identity mappings. This motivates us to propose a new residual unit, which makes training easier and improves generalization. We report improved results using a 1001-layer ResNet on CIFAR10 (4.62 % error) and CIFAR-100, and a 200-layer ResNet on ImageNet.</p>"
            },
            "media": {
                "url": "https://link.springer.com/chapter/10.1007/978-3-319-46493-0_38",
                "caption": "Identity Mappings in Deep Residual Networks",
                "credit": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun"
            }
        },
        {
            "start_date": {
                "year": 2016,
                "month": 9,
                "day": 17
            },
            "text": {
                "headline": "MS-CNN",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://link.springer.com/chapter/10.1007/978-3-319-46493-0_22",
                "caption": "A Unified Multi-scale Deep Convolutional Neural Network for Fast Object Detection",
                "credit": "Zhaowei Cai, Quanfu Fan, Rogerio S. Feris, Nuno Vasconcelos"
            }
        },
        {
            "start_date": {
                "year": 2016,
                "month": 9,
                "day": 15
            },
            "text": {
                "headline": "Youtube recommendation model",
                "text": "<p>YouTube represents one of the largest scale and most sophisticated industrial recommendation systems in existence. In this paper, we describe the system at a high level and focus on the dramatic performance improvements brought by deep learning. The paper is split according to the classic two-stage information retrieval dichotomy: first, we detail a deep candidate generation model and then describe a separate deep ranking model. We also provide practical lessons and insights derived from designing, iterating and maintaining a massive recommendation system with enormous user-facing impact.</p>"
            },
            "media": {
                "url": "https://research.google/pubs/pub45530/",
                "caption": "Deep Neural Networks for YouTube Recommendations",
                "credit": "Paul Covington, Jay Adams, and Emre Sargin"
            }
        },
        {
            "start_date": {
                "year": 2016,
                "month": 9,
                "day": 12
            },
            "text": {
                "headline": "WaveNet",
                "text": "<p>This paper introduces WaveNet, a deep neural network for generating raw audio waveforms. The model is fully probabilistic and autoregressive, with the predictive distribution for each audio sample conditioned on all previous ones; nonetheless we show that it can be efficiently trained on data with tens of thousands of samples per second of audio. When applied to text-to-speech, it yields state-of-the-art performance, with human listeners rating it as significantly more natural sounding than the best parametric and concatenative systems for both English and Mandarin. A single WaveNet can capture the characteristics of many different speakers with equal fidelity, and can switch between them by conditioning on the speaker identity. When trained to model music, we find that it generates novel and often highly realistic musical fragments. We also show that it can be employed as a discriminative model, returning promising results for phoneme recognition.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1609.03499",
                "caption": "WaveNet: A Generative Model for Raw Audio",
                "credit": "A Oord, S Dieleman, H Zen, K Simonyan"
            }
        },
        {
            "start_date": {
                "year": 2016,
                "month": 8,
                "day": 26
            },
            "text": {
                "headline": "Multi-task Cascaded CNN",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1604.02878",
                "caption": "Joint Face Detection and Alignment using Multitask cascaded convolutional networks",
                "credit": "Kaipeng Zhang, Zhanpeng Zhang, Zhifeng Li, Yu Qiao"
            }
        },
        {
            "start_date": {
                "year": 2016,
                "month": 8,
                "day": 25
            },
            "text": {
                "headline": "DenseNet-264",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1608.06993",
                "caption": "Densely Connected Convolutional Networks",
                "credit": "G Huang, Z Liu, L Van Der Maaten"
            }
        },
        {
            "start_date": {
                "year": 2016,
                "month": 8,
                "day": 22
            },
            "text": {
                "headline": "SimpleNet",
                "text": "<p>Major winning Convolutional Neural Networks (CNNs), such as AlexNet, VGGNet, ResNet, GoogleNet, include tens to hundreds of millions of parameters, which impose considerable computation and memory overhead. This limits their practical use for training, optimization and memory efficiency. On the contrary, light-weight architectures, being proposed to address this issue, mainly suffer from low accuracy. These inefficiencies mostly stem from following an ad hoc procedure. We propose a simple architecture, called SimpleNet, based on a set of designing principles, with which we empirically show, a well-crafted yet simple and reasonably deep architecture can perform on par with deeper and more complex architectures. SimpleNet provides a good tradeoff between the computation/memory efficiency and the accuracy. Our simple 13-layer architecture outperforms most of the deeper and complex architectures to date such as VGGNet, ResNet, and GoogleNet on several well-known benchmarks while having 2 to 25 times fewer number of parameters and operations. This makes it very handy for embedded systems or systems with computational and memory limitations. We achieved state-of-theart result on CIFAR10 outperforming several heavier architectures, near state of the art on MNIST and highly competitive results on CIFAR100 and SVHN. We also outperformed the much larger and deeper architectures such as VGGNet and popular variants of ResNets among others on the ImageNet dataset. Models are made available at: https://github.com/Coderx7/SimpleNet</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1608.06037",
                "caption": "Lets keep it simple, Using simple architectures to outperform deeper and more complex architectures",
                "credit": "Seyyed Hossein Hasanpour, Mohammad Rouhani, Mohsen Fayyaz, Mohammad Sabokrou"
            }
        },
        {
            "start_date": {
                "year": 2016,
                "month": 7,
                "day": 15
            },
            "text": {
                "headline": "Character-enriched word2vec",
                "text": "<p>Continuous word representations, trained on large unlabeled corpora are useful for many natural language processing tasks. Popular models that learn such representations ignore the morphology of words, by assigning a distinct vector to each word. This is a limitation, especially for languages with large vocabularies and many rare words. In this paper, we propose a new approach based on the skipgram model, where each word is represented as a bag of character n-grams. A vector representation is associated to each character n-gram; words being represented as the sum of these representations. Our method is fast, allowing to train models on large corpora quickly and allows us to compute word representations for words that did not appear in the training data. We evaluate our word representations on nine different languages, both on word similarity and analogy tasks. By comparing to recently proposed morphological word representations, we show that our vectors achieve state-of-the-art performance on these tasks.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1607.04606",
                "caption": "Enriching Word Vectors with Subword Information",
                "credit": "Piotr Bojanowski, Edouard Grave, Armand Joulin, Tomas Mikolov"
            }
        },
        {
            "start_date": {
                "year": 2016,
                "month": 7,
                "day": 12
            },
            "text": {
                "headline": "VD-RHN",
                "text": "<p>Many sequential processing tasks require complex nonlinear transition functions from one step to the next. However, recurrent neural networks with 'deep' transition functions remain difficult to train, even when using Long Short-Term Memory (LSTM) networks. We introduce a novel theoretical analysis of recurrent networks based on Gersgorin's circle theorem that illuminates several modeling and optimization issues and improves our understanding of the LSTM cell. Based on this analysis we propose Recurrent Highway Networks, which extend the LSTM architecture to allow step-to-step transition depths larger than one. Several language modeling experiments demonstrate that the proposed architecture results in powerful and efficient models. On the Penn Treebank corpus, solely increasing the transition depth from 1 to 10 improves word-level perplexity from 90.6 to 65.4 using the same number of parameters. On the larger Wikipedia datasets for character prediction (text8 and enwik8), RHNs outperform all previous results and achieve an entropy of 1.27 bits per character.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1607.03474",
                "caption": "Recurrent Highway Networks",
                "credit": "Julian Georg Zilly, Rupesh Kumar Srivastava, Jan Koutník, Jürgen Schmidhuber"
            }
        },
        {
            "start_date": {
                "year": 2016,
                "month": 7,
                "day": 6
            },
            "text": {
                "headline": "fastText",
                "text": "<p>This paper explores a simple and efficient baseline for text classification. Our experiments show that our fast text classifier fastText is often on par with deep learning classifiers in terms of accuracy, and many orders of magnitude faster for training and evaluation. We can train fastText on more than one billion words in less than ten minutes using a standard multicore~CPU, and classify half a million sentences among~312K classes in less than a minute.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1607.01759",
                "caption": "Bag of Tricks for Efficient Text Classification",
                "credit": "A Joulin, E Grave, P Bojanowski, T Mikolov"
            }
        },
        {
            "start_date": {
                "year": 2016,
                "month": 6,
                "day": 24
            },
            "text": {
                "headline": "Wide & Deep",
                "text": "<p>Generalized linear models with nonlinear feature transformations are widely used for large-scale regression and classification problems with sparse inputs. Memorization of feature interactions through a wide set of cross-product feature transformations are effective and interpretable, while generalization requires more feature engineering effort. With less feature engineering, deep neural networks can generalize better to unseen feature combinations through low-dimensional dense embeddings learned for the sparse features. However, deep neural networks with embeddings can over-generalize and recommend less relevant items when the user-item interactions are sparse and high-rank. In this paper, we present Wide & Deep learning---jointly trained wide linear models and deep neural networks---to combine the benefits of memorization and generalization for recommender systems. We productionized and evaluated the system on Google Play, a commercial mobile app store with over one billion active users and over one million apps. Online experiment results show that Wide & Deep significantly increased app acquisitions compared with wide-only and deep-only models. We have also open-sourced our implementation in TensorFlow.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1606.07792",
                "caption": "Wide & Deep Learning for Recommender Systems",
                "credit": "HT Cheng, L Koc, J Harmsen, T Shaked"
            }
        },
        {
            "start_date": {
                "year": 2016,
                "month": 6,
                "day": 21
            },
            "text": {
                "headline": "R-FCN",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1605.06409",
                "caption": "R-fcn: Object detection via region-based fully convolutional networks.",
                "credit": "Jifeng Dai, Y. Li, Kaiming He, and Jian Sun"
            }
        },
        {
            "start_date": {
                "year": 2016,
                "month": 6,
                "day": 20
            },
            "text": {
                "headline": "DMN",
                "text": "<p>Most tasks in natural language processing can be cast into question answering (QA) problems over language input. We introduce the dynamic memory network (DMN), a neural network architecture which processes input sequences and questions, forms episodic memories, and generates relevant answers. Questions trigger an iterative attention process which allows the model to condition its attention on the inputs and the result of previous iterations. These results are then reasoned over in a hierarchical recurrent sequence model to generate answers. The DMN can be trained end-to-end and obtains state-of-the-art results on several types of tasks and datasets: question answering (Facebook's bAbI dataset), text classification for sentiment analysis (Stanford Sentiment Treebank) and sequence modeling for part-of-speech tagging (WSJ-PTB). The training for these different tasks relies exclusively on trained word vector representations and input-question-answer triplets.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1506.07285",
                "caption": "Ask Me Anything: Dynamic Memory Networks for Natural Language Processing",
                "credit": "Ankit Kumar, Ozan Irsoy, Peter Ondruska, Mohit Iyyer, James Bradbury, Ishaan Gulrajani, Victor Zhong, Romain Paulus, Richard Socher"
            }
        },
        {
            "start_date": {
                "year": 2016,
                "month": 6,
                "day": 16
            },
            "text": {
                "headline": "PixelCNN",
                "text": "<p>This work explores conditional image generation with a new image density model based on the PixelCNN architecture. The model can be conditioned on any vector, including descriptive labels or tags, or latent embeddings created by other networks. When conditioned on class labels from the ImageNet database, the model is able to generate diverse, realistic scenes representing distinct animals, objects, landscapes and structures. When conditioned on an embedding produced by a convolutional network given a single image of an unseen face, it generates a variety of new portraits of the same person with different facial expressions, poses and lighting conditions. We also show that conditional PixelCNN can serve as a powerful decoder in an image autoencoder. Additionally, the gated convolutional layers in the proposed model improve the log-likelihood of PixelCNN to match the state-ofthe-art performance of PixelRNN on ImageNet, with greatly reduced computational cost.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1606.05328",
                "caption": "Conditional Image Generation with PixelCNN Decoders\n",
                "credit": "Aaron van den Oord, Nal Kalchbrenner, Oriol Vinyals, Lasse Espeholt, Alex Graves, Koray Kavukcuoglu"
            }
        },
        {
            "start_date": {
                "year": 2016,
                "month": 6,
                "day": 1
            },
            "text": {
                "headline": "Spatiotemporal fusion ConvNet",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://openaccess.thecvf.com/content_cvpr_2016/html/Feichtenhofer_Convolutional_Two-Stream_Network_CVPR_2016_paper.html",
                "caption": "Convolutional Two-Stream Network Fusion for Video Action Recognition",
                "credit": "Christoph Feichtenhofer, Axel Pinz, Andrew Zisserman"
            }
        },
        {
            "start_date": {
                "year": 2016,
                "month": 5,
                "day": 29
            },
            "text": {
                "headline": "Part-of-sentence tagging model",
                "text": "<p>State-of-the-art sequence labeling systems traditionally require large amounts of task-specific knowledge in the form of handcrafted features and data pre-processing. In this paper, we introduce a novel neutral network architecture that benefits from both word- and character-level representations automatically, by using combination of bidirectional LSTM, CNN and CRF. Our system is truly end-to-end, requiring no feature engineering or data preprocessing, thus making it applicable to a wide range of sequence labeling tasks. We evaluate our system on two data sets or two sequence labeling tasks — Penn Treebank WSJ corpus for part-of-speech (POS) tagging and CoNLL 2003 corpus for named entity recognition (NER). We obtain state-of-the-art performance on both datasets — 97.55% accuracy for POS tagging and 91.21% F1 for NER.\n</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1603.01354",
                "caption": "End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF",
                "credit": "Xuehe Ma, Eduard Hovy"
            }
        },
        {
            "start_date": {
                "year": 2016,
                "month": 5,
                "day": 29
            },
            "text": {
                "headline": "Named Entity Recognition model",
                "text": "<p>State-of-the-art sequence labeling systems traditionally require large amounts of task-specific knowledge in the form of handcrafted features and data pre-processing. In this paper, we introduce a novel neutral network architecture that benefits from both word- and character-level representations automatically, by using combination of bidirectional LSTM, CNN and CRF. Our system is truly end-to-end, requiring no feature engineering or data preprocessing, thus making it applicable to a wide range of sequence labeling tasks. We evaluate our system on two data sets or two sequence labeling tasks — Penn Treebank WSJ corpus for part-of-speech (POS) tagging and CoNLL 2003 corpus for named entity recognition (NER). We obtain state-of-the-art performance on both datasets — 97.55% accuracy for POS tagging and 91.21% F1 for NER.\n</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1603.01354",
                "caption": "Layer Normalization",
                "credit": "Xuezhe Ma, Eduard Hovy"
            }
        },
        {
            "start_date": {
                "year": 2016,
                "month": 4,
                "day": 30
            },
            "text": {
                "headline": "Gated HORNN (3rd order)",
                "text": "<p>In this paper, we study novel neural network structures to better model long term dependency in sequential data. We propose to use more memory units to keep track of more preceding states in recurrent neural networks (RNNs), which are all recurrently fed to the hidden layers as feedback through different weighted paths. By extending the popular recurrent structure in RNNs, we provide the models with better short-term memory mechanism to learn long term dependency in sequences. Analogous to digital filters in signal processing, we call these structures as higher order RNNs (HORNNs). Similar to RNNs, HORNNs can also be learned using the back-propagation through time method. HORNNs are generally applicable to a variety of sequence modelling tasks. In this work, we have examined HORNNs for the language modeling task using two popular data sets, namely the Penn Treebank (PTB) and English text8 data sets. Experimental results have shown that the proposed HORNNs yield the state-of-the-art performance on both data sets, significantly outperforming the regular RNNs as well as the popular LSTMs.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1605.00064",
                "caption": "Higher Order Recurrent Neural Networks",
                "credit": "Rohollah Soltani, Hui Jiang"
            }
        },
        {
            "start_date": {
                "year": 2016,
                "month": 3,
                "day": 30
            },
            "text": {
                "headline": "Symmetric Residual Encoder-Decoder Net",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1603.09056v2",
                "caption": "Image Restoration Using Very Deep Convolutional Encoder-Decoder Networks with Symmetric Skip Connections",
                "credit": "Xiao-Jiao Mao, Chunhua Shen, Yu-Bin Yang"
            }
        },
        {
            "start_date": {
                "year": 2016,
                "month": 3,
                "day": 17
            },
            "text": {
                "headline": "Binarized Neural Network (MNIST)",
                "text": "<p>We introduce a method to train Binarized Neural Networks (BNNs) - neural networks with binary weights and activations at run-time. At training-time the binary weights and activations are used for computing the parameters gradients. During the forward pass, BNNs drastically reduce memory size and accesses, and replace most arithmetic operations with bit-wise operations, which is expected to substantially improve power-efficiency. To validate the effectiveness of BNNs we conduct two sets of experiments on the Torch7 and Theano frameworks. On both, BNNs achieved nearly state-of-the-art results over the MNIST, CIFAR-10 and SVHN datasets. Last but not least, we wrote a binary matrix multiplication GPU kernel with which it is possible to run our MNIST BNN 7 times faster than with an unoptimized GPU kernel, without suffering any loss in classification accuracy. The code for training and running our BNNs is available on-line.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1602.02830",
                "caption": "Binarized Neural Networks: Training Neural Networks with Weights and Activations Constrained to +1 or −1",
                "credit": "Matthieu Courbariaux, Itay Hubara, Daniel Soudry, Ran El-Yaniv, Yoshua Bengio"
            }
        },
        {
            "start_date": {
                "year": 2016,
                "month": 2,
                "day": 24
            },
            "text": {
                "headline": "SqueezeNet",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1602.07360",
                "caption": "SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5MB model size",
                "credit": "Forrest N. Iandola, Song Han, Matthew W. Moskewicz, Khalid Ashraf, William J. Dally, Kurt Keutzer"
            }
        },
        {
            "start_date": {
                "year": 2016,
                "month": 2,
                "day": 23
            },
            "text": {
                "headline": "Inceptionv4",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1602.07261",
                "caption": "Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning",
                "credit": "Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, Alex Alemi"
            }
        },
        {
            "start_date": {
                "year": 2016,
                "month": 2,
                "day": 23
            },
            "text": {
                "headline": "Inception-ResNet-V2",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1602.07261",
                "caption": "Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning",
                "credit": "Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, Alex Alemi"
            }
        },
        {
            "start_date": {
                "year": 2016,
                "month": 2,
                "day": 4
            },
            "text": {
                "headline": "A3C FF hs",
                "text": "<p></p>"
            },
            "media": {
                "url": "http://arxiv.org/abs/1602.01783v2",
                "caption": "Asynchronous Methods for Deep Reinforcement Learning",
                "credit": "V Mnih, AP Badia, M Mirza, A Graves"
            }
        },
        {
            "start_date": {
                "year": 2016,
                "month": 1,
                "day": 30
            },
            "text": {
                "headline": "Convolutional Pose Machines",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1602.00134",
                "caption": "Convolutional Pose Machines",
                "credit": "Shih-En Wei, Varun Ramakrishna, Takeo Kanade, Yaser Sheikh"
            }
        },
        {
            "start_date": {
                "year": 2016,
                "month": 1,
                "day": 27
            },
            "text": {
                "headline": "AlphaGo Lee",
                "text": "<p>The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses ‘value networks’ to evaluate board positions and ‘policy networks’ to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8% winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away.</p>"
            },
            "media": {
                "url": "https://www.nature.com/articles/nature16961",
                "caption": "Mastering the game of Go with deep neural networks and tree search",
                "credit": "David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, Sander Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy Lillicrap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, Demis Hassabis"
            }
        },
        {
            "start_date": {
                "year": 2015,
                "month": 12,
                "day": 16
            },
            "text": {
                "headline": "Variational (untied weights, MC) LSTM (Large)",
                "text": "<p>Recurrent neural networks (RNNs) stand at the forefront of many recent developments in deep learning. Yet a major difficulty with these models is their tendency to overfit, with dropout shown to fail when applied to recurrent layers. Recent results at the intersection of Bayesian modelling and deep learning offer a Bayesian interpretation of common deep learning techniques such as dropout. This grounding of dropout in approximate Bayesian inference suggests an extension of the theoretical results, offering insights into the use of dropout with RNN models. We apply this new variational inference based dropout technique in LSTM and GRU models, assessing it on language modelling and sentiment analysis tasks. The new approach outperforms existing techniques, and to the best of our knowledge improves on the single model state-of-the-art in language modelling with the Penn Treebank (73.4 test perplexity). This extends our arsenal of variational tools in deep learning.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1512.05287",
                "caption": "A Theoretically Grounded Application of Dropout in Recurrent Neural Networks",
                "credit": "Yarin Gal, Zoubin Ghahramani"
            }
        },
        {
            "start_date": {
                "year": 2015,
                "month": 12,
                "day": 15
            },
            "text": {
                "headline": "Advantage Learning",
                "text": "<p>This paper introduces new optimality-preserving operators on Q-functions. We first describe an operator for tabular representations, the consistent Bellman operator, which incorporates a notion of local policy consistency. We show that this local consistency leads to an increase in the action gap at each state; increasing this gap, we argue, mitigates the undesirable effects of approximation and estimation errors on the induced greedy policies. This operator can also be applied to discretized continuous space and time problems, and we provide empirical results evidencing superior performance in this context. Extending the idea of a locally consistent operator, we then derive sufficient conditions for an operator to preserve optimality, leading to a family of operators which includes our consistent Bellman operator. As corollaries we provide a proof of optimality for Baird's advantage learning algorithm and derive other gap-increasing operators with interesting properties. We conclude with an empirical study on 60 Atari 2600 games illustrating the strong potential of these new operators.</p>"
            },
            "media": {
                "url": "http://arxiv.org/abs/1512.04860v1",
                "caption": "Increasing the Action Gap: New Operators for Reinforcement Learning",
                "credit": "MG Bellemare, G Ostrovski, A Guez"
            }
        },
        {
            "start_date": {
                "year": 2015,
                "month": 12,
                "day": 11
            },
            "text": {
                "headline": "BPL",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://science.sciencemag.org/content/350/6266/1332/",
                "caption": "Human-level concept learning through probabilistic program induction",
                "credit": "BM Lake, R Salakhutdinov, JB Tenenbaum"
            }
        },
        {
            "start_date": {
                "year": 2015,
                "month": 12,
                "day": 10
            },
            "text": {
                "headline": "ResNet-152 (ImageNet)",
                "text": "<p>Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1512.03385",
                "caption": "Deep Residual Learning for Image Recognition",
                "credit": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun"
            }
        },
        {
            "start_date": {
                "year": 2015,
                "month": 12,
                "day": 10
            },
            "text": {
                "headline": "ResNet-110 (CIFAR-10)",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1512.03385",
                "caption": "Deep Residual Learning for Image Recognition",
                "credit": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun"
            }
        },
        {
            "start_date": {
                "year": 2015,
                "month": 12,
                "day": 8
            },
            "text": {
                "headline": "SSD",
                "text": "<p>We present a method for detecting objects in images using a single deep neural network. Our approach, named SSD, discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location. At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape. Additionally, the network combines predictions from multiple feature maps with different resolutions to naturally handle objects of various sizes. Our SSD model is simple relative to methods that require object proposals because it completely eliminates proposal generation and subsequent pixel or feature resampling stage and encapsulates all computation in a single network. This makes SSD easy to train and straightforward to integrate into systems that require a detection component. Experimental results on the PASCAL VOC, MS COCO, and ILSVRC datasets confirm that SSD has comparable accuracy to methods that utilize an additional object proposal step and is much faster, while providing a unified framework for both training and inference. Compared to other single stage methods, SSD has much better accuracy, even with a smaller input image size. For 300×300 input, SSD achieves 72.1% mAP on VOC2007 test at 58 FPS on a Nvidia Titan X and for 500×500 input, SSD achieves 75.1% mAP, outperforming a comparable state of the art Faster R-CNN model. Code is available at this https URL</p>"
            },
            "media": {
                "url": "https://arxiv.org/pdf/1512.02325",
                "caption": "SSD: Single Shot MultiBox Detector",
                "credit": "Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, Alexander C. Berg"
            }
        },
        {
            "start_date": {
                "year": 2015,
                "month": 12,
                "day": 8
            },
            "text": {
                "headline": "DeepSpeech2 (English)",
                "text": "<p>We show that an end-to-end deep learning approach can be used to recognize either English or Mandarin Chinese speech--two vastly different languages. Because it replaces entire pipelines of hand-engineered components with neural networks, end-to-end learning allows us to handle a diverse variety of speech including noisy environments, accents and different languages. Key to our approach is our application of HPC techniques, resulting in a 7x speedup over our previous system. Because of this efficiency, experiments that previously took weeks now run in days. This enables us to iterate more quickly to identify superior architectures and algorithms. As a result, in several cases, our system is competitive with the transcription of human workers when benchmarked on standard datasets. Finally, using a technique called Batch Dispatch with GPUs in the data center, we show that our system can be inexpensively deployed in an online setting, delivering low latency when serving users at scale.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1512.02595",
                "caption": "Deep Speech 2: End-to-End Speech Recognition in English and Mandarin",
                "credit": "Dario Amodei, Rishita Anubhai, Eric Battenberg, Carl Case, Jared Casper, Bryan Catanzaro, Jingdong Chen, Mike Chrzanowski, Adam Coates, Greg Diamos, Erich Elsen, Jesse Engel, Linxi Fan, Christopher Fougner, Tony Han, Awni Hannun, Billy Jun, Patrick LeGresley, Libby Lin, Sharan Narang, Andrew Ng, Sherjil Ozair, Ryan Prenger, Jonathan Raiman, Sanjeev Satheesh, David Seetapun, Shubho Sengupta, Yi Wang, Zhiqian Wang, Chong Wang, Bo Xiao, Dani Yogatama, Jun Zhan, Zhenyao Zhu"
            }
        },
        {
            "start_date": {
                "year": 2015,
                "month": 12,
                "day": 2
            },
            "text": {
                "headline": "Inception v3",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1512.00567",
                "caption": "Rethinking the inception architecture for computer vision.",
                "credit": "Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, Zbigniew Wojna"
            }
        },
        {
            "start_date": {
                "year": 2015,
                "month": 12,
                "day": 1
            },
            "text": {
                "headline": "Netflix Recommender System",
                "text": "<p>This article discusses the various algorithms that make up the Netflix recommender system, and describes its business purpose. We also describe the role of search and related algorithms, which for us turns into a\nrecommendations problem as well. We explain the motivations behind and review the approach that we use to improve the recommendation algorithms, combining A/B testing focused on improving member retention and medium term engagement, as well as offline experimentation using historical member engagement data. We discuss some of the issues in designing and interpreting A/B tests. Finally, we describe some current areas of focused innovation, which include making our recommender system global and language aware.</p>"
            },
            "media": {
                "url": "https://dl.acm.org/doi/pdf/10.1145/2843948",
                "caption": "The Netflix Recommender System: Algorithms, Business Value, and Innovation",
                "credit": "CA Gomez-Uribe, N Hunt"
            }
        },
        {
            "start_date": {
                "year": 2015,
                "month": 11,
                "day": 23
            },
            "text": {
                "headline": "Multi-scale Dilated CNN",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1511.07122",
                "caption": "Multi-Scale Context Aggregation by Dilated Convolutions",
                "credit": "Fisher Yu, Vladlen Koltun"
            }
        },
        {
            "start_date": {
                "year": 2015,
                "month": 10,
                "day": 1
            },
            "text": {
                "headline": "AlphaGo Fan",
                "text": "<p>A long-standing goal of artificial intelligence is an algorithm that learns, tabula rasa, superhuman proficiency in challenging domains. Recently, AlphaGo became the first program to defeat a world champion in the game of Go. The tree search in AlphaGo evaluated positions and selected moves using deep neural networks. These neural networks were trained by supervised learning from human expert moves, and by reinforcement learning from self-play. Here we introduce an algorithm based solely on reinforcement learning, without human data, guidance or domain knowledge beyond game rules. AlphaGo becomes its own teacher: a neural network is trained to predict AlphaGo’s own move selections and also the winner of AlphaGo’s games. This neural network improves the strength of the tree search, resulting in higher quality move selection and stronger self-play in the next iteration. Starting tabula rasa, our new program AlphaGo Zero achieved superhuman performance, winning 100–0 against the previously published, champion-defeating AlphaGo.</p>"
            },
            "media": {
                "url": "https://www.nature.com/articles/nature24270.epdf?author_access_token=VJXbVjaSHxFoctQQ4p2k4tRgN0jAjWel9jnR3ZoTv0PVW4gB86EEpGqTRDtpIz-2rmo8-KG06gqVobU5NSCFeHILHcVFUeMsbvwS-lxjqQGg98faovwjxeTUgZAUMnRQ",
                "caption": "Mastering the game of Go with deep neural networks and tree search",
                "credit": "David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, Sander Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy Lillicrap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, Demis Hassabis"
            }
        },
        {
            "start_date": {
                "year": 2015,
                "month": 9,
                "day": 9
            },
            "text": {
                "headline": "Deep Deterministic Policy Gradients",
                "text": "<p>We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies end-to-end: directly from raw pixel inputs.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1509.02971",
                "caption": "Continuous control with deep reinforcement learning",
                "credit": "TP Lillicrap, JJ Hunt, A Pritzel, N Heess, T Erez"
            }
        },
        {
            "start_date": {
                "year": 2015,
                "month": 8,
                "day": 31
            },
            "text": {
                "headline": "BPE",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1508.07909",
                "caption": "Neural Machine Translation of Rare Words with Subword Units",
                "credit": "R Sennrich, B Haddow, A Birch"
            }
        },
        {
            "start_date": {
                "year": 2015,
                "month": 8,
                "day": 26
            },
            "text": {
                "headline": "LSTM-Char-Large",
                "text": "<p>We describe a simple neural language model that relies only on character-level inputs. Predictions are still made at the word-level. Our model employs a convolutional neural network (CNN) and a highway network over characters, whose output is given to a long short-term memory (LSTM) recurrent neural network language model (RNN-LM). On the English Penn Treebank the model is on par with the existing state-of-the-art despite having 60% fewer parameters. On languages with rich morphology (Arabic, Czech, French, German, Spanish, Russian), the model outperforms word-level/morpheme-level LSTM baselines, again with fewer parameters. The results suggest that on many languages, character inputs are sufficient for language modeling. Analysis of word representations obtained from the character composition part of the model reveals that the model is able to encode, from characters only, both semantic and orthographic information.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1508.06615",
                "caption": "Character-Aware Neural Language Models",
                "credit": "Yoon Kim, Yacine Jernite, David Sontag, Alexander M. Rush"
            }
        },
        {
            "start_date": {
                "year": 2015,
                "month": 8,
                "day": 20
            },
            "text": {
                "headline": "Listen, Attend and Spell",
                "text": "<p>We present Listen, Attend and Spell (LAS), a neural speech recognizer that transcribes speech utterances directly to characters without pronunciation models, HMMs or other components of traditional speech recognizers. In LAS, the neural network architecture subsumes the acoustic, pronunciation and language models making it not only an end-to-end trained system but an end-to-end model. In contrast to DNN-HMM, CTC and most other models, LAS makes no independence assumptions about the probability distribution of the output character sequences given the acoustic sequence. Our system has two components: a listener and a speller. The listener is a pyramidal recurrent network encoder that accepts filter bank spectra as inputs. The speller is an attention-based recurrent network decoder that emits each character conditioned on all previous characters, and the entire acoustic sequence. On a Google voice search task, LAS achieves a WER of 14.1% without a dictionary or an external language model and 10.3% with language model rescoring over the top 32 beams. In comparison, the state-of-the-art CLDNN-HMM model achieves a WER of 8.0% on the same set.</p>"
            },
            "media": {
                "url": "https://ieeexplore.ieee.org/document/7472621",
                "caption": "Listen, attend and spell: A neural network for large vocabulary conversational speech recognition",
                "credit": "William Chan, Navdeep Jaitly, Quoc Le, Oriol Vinyals"
            }
        },
        {
            "start_date": {
                "year": 2015,
                "month": 7,
                "day": 6
            },
            "text": {
                "headline": "Search-Proven Best LSTM",
                "text": "<p>The Recurrent Neural Network (RNN) is an extremely powerful sequence model that is often difficult to train. The Long Short-Term Memory (LSTM) is a specific RNN architecture whose design makes it much easier to train. While wildly successful in practice, the LSTM’s architecture appears to be ad-hoc so it is not clear if it is optimal, and the significance of its individual components is unclear. In this work, we aim to determine whether the LSTM architecture is optimal or whether much better architectures exist. We conducted a thorough architecture search where we evaluated over ten thousand different RNN architectures, and identified an architecture that outperforms both the LSTM and the recently-introduced Gated Recurrent Unit (GRU) on some but not all tasks. We found that adding a bias of 1 to the LSTM’s forget gate closes the gap between the LSTM and the GRU.</p>"
            },
            "media": {
                "url": "https://proceedings.mlr.press/v37/jozefowicz15.pdf",
                "caption": "An Empirical Exploration of Recurrent Network Architectures",
                "credit": "R. Józefowicz, Wojciech Zaremba, Ilya Sutskever"
            }
        },
        {
            "start_date": {
                "year": 2015,
                "month": 6,
                "day": 15
            },
            "text": {
                "headline": "BatchNorm",
                "text": "<p>Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9% top-5 validation error (and 4.8% test error), exceeding the accuracy of human raters.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1502.03167",
                "caption": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift",
                "credit": "Sergey Ioffe, Christian Szegedy"
            }
        },
        {
            "start_date": {
                "year": 2015,
                "month": 6,
                "day": 8
            },
            "text": {
                "headline": "YOLO",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1506.02640",
                "caption": "You Only Look Once: Unified, Real-Time Object Detection",
                "credit": "Joseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadi"
            }
        },
        {
            "start_date": {
                "year": 2015,
                "month": 6,
                "day": 4
            },
            "text": {
                "headline": "Faster R-CNN",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1506.01497",
                "caption": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks",
                "credit": "S Ren, K He, R Girshick, J Sun"
            }
        },
        {
            "start_date": {
                "year": 2015,
                "month": 5,
                "day": 19
            },
            "text": {
                "headline": "Trajectory-pooled conv nets",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Wang_Action_Recognition_With_2015_CVPR_paper.html",
                "caption": "Action Recognition with Trajectory-Pooled Deep-Convolutional Descriptors",
                "credit": "Limin Wang, Yu Qiao, Xiaoou Tang"
            }
        },
        {
            "start_date": {
                "year": 2015,
                "month": 5,
                "day": 1
            },
            "text": {
                "headline": "Deep LSTM video classifier",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Ng_Beyond_Short_Snippets_2015_CVPR_paper.html",
                "caption": "Beyond Short Snippets: Deep Networks for Video Classification",
                "credit": "Joe Yue-Hei Ng, Matthew Hausknecht, Sudheendra Vijayanarasimhan, Oriol Vinyals, Rajat Monga, George Toderici"
            }
        },
        {
            "start_date": {
                "year": 2015,
                "month": 4,
                "day": 30
            },
            "text": {
                "headline": "Fast R-CNN",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1504.08083",
                "caption": "Fast R-CNN",
                "credit": "R Girshick"
            }
        },
        {
            "start_date": {
                "year": 2015,
                "month": 3,
                "day": 17
            },
            "text": {
                "headline": "genCNN + dyn eval",
                "text": "<p>We propose a convolutional neural network, named genCNN, for word sequence prediction. Different from previous work on neural networkbased language modeling and generation (e.g., RNN or LSTM), we choose\nnot to greedily summarize the history of words as a fixed length vector. Instead, we use a convolutional neural network to predict the next word with\nthe history of words of variable length. Also different from the existing feedforward networks for language modeling, our model can effectively fuse\nthe local correlation and global correlation in the word sequence, with\na convolution-gating strategy specifically designed for the task. We argue\nthat our model can give adequate representation of the history, and therefore can naturally exploit both the short and long range dependencies. Our\nmodel is fast, easy to train, and readily parallelized. Our extensive experiments on text generation and n-best re-ranking in machine translation show\nthat genCNN outperforms the state-ofthe-arts with big margins.</p>"
            },
            "media": {
                "url": "https://aclanthology.org/P15-1151/",
                "caption": "genCNN: A Convolutional Architecture for Word Sequence Prediction",
                "credit": "Mingxuan Wang, Zhengdong Lu, Hang Li, Wenbin Jiang, Qun Liu"
            }
        },
        {
            "start_date": {
                "year": 2015,
                "month": 2,
                "day": 28
            },
            "text": {
                "headline": "Constituency-Tree LSTM",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1503.00075",
                "caption": "Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks",
                "credit": "KS Tai, R Socher, CD Manning"
            }
        },
        {
            "start_date": {
                "year": 2015,
                "month": 2,
                "day": 25
            },
            "text": {
                "headline": "DQN-2015",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://www.nature.com/articles/nature14236",
                "caption": "Human-level control through deep reinforcement learning",
                "credit": "Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, Demis Hassabis "
            }
        },
        {
            "start_date": {
                "year": 2015,
                "month": 2,
                "day": 19
            },
            "text": {
                "headline": "TRPO",
                "text": "<p>We describe an iterative procedure for optimizing policies, with guaranteed monotonic improvement. By making several approximations to the theoretically-justified procedure, we develop a practical algorithm, called Trust Region Policy Optimization (TRPO). This algorithm is similar to natural policy gradient methods and is effective for optimizing large nonlinear policies such as neural networks. Our experiments demonstrate its robust performance on a wide variety of tasks: learning simulated robotic swimming, hopping, and walking gaits; and playing Atari games using images of the screen as input. Despite its approximations that deviate from the theory, TRPO tends to give monotonic improvement, with little tuning of hyperparameters.</p>"
            },
            "media": {
                "url": "https://arxiv.org/pdf/1502.05477",
                "caption": "Trust Region Policy Optimization",
                "credit": "John Schulman, Sergey Levine, Philipp Moritz, Michael I. Jordan, Pieter Abbeel"
            }
        },
        {
            "start_date": {
                "year": 2015,
                "month": 2,
                "day": 11
            },
            "text": {
                "headline": "CRF-RNN",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1502.03240",
                "caption": "Conditional Random Fields as Recurrent Neural Networks",
                "credit": "Shuai Zheng, Sadeep Jayasumana, Bernardino Romera-Paredes, Vibhav Vineet, Zhizhong Su, Dalong Du, Chang Huang, Philip H. S. Torr"
            }
        },
        {
            "start_date": {
                "year": 2015,
                "month": 2,
                "day": 6
            },
            "text": {
                "headline": "MSRA (C, PReLU)",
                "text": "<p>Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra computational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on our PReLU networks (PReLU-nets), we achieve 4.94% top-5 test error on the ImageNet 2012 classification dataset. This is a 26% relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66%). To our knowledge, our result is the first to surpass human-level performance (5.1%, Russakovsky et al.) on this visual recognition challenge.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1502.01852",
                "caption": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification",
                "credit": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun"
            }
        },
        {
            "start_date": {
                "year": 2014,
                "month": 12,
                "day": 22
            },
            "text": {
                "headline": "DeepLab",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1412.7062",
                "caption": "Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs",
                "credit": "Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, Alan L. Yuille"
            }
        },
        {
            "start_date": {
                "year": 2014,
                "month": 12,
                "day": 22
            },
            "text": {
                "headline": "ADAM (CIFAR-10)",
                "text": "<p>We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1412.6980",
                "caption": "Adam: A Method for Stochastic Optimization",
                "credit": "DP Kingma, J Ba"
            }
        },
        {
            "start_date": {
                "year": 2014,
                "month": 12,
                "day": 18
            },
            "text": {
                "headline": "Fractional Max-Pooling",
                "text": "<p>Convolutional networks almost always incorporate some form of spatial pooling, and very often it is alpha times alpha max-pooling with alpha=2. Max-pooling act on the hidden layers of the network, reducing their size by an integer multiplicative factor alpha. The amazing by-product of discarding 75% of your data is that you build into the network a degree of invariance with respect to translations and elastic distortions. However, if you simply alternate convolutional layers with max-pooling layers, performance is limited due to the rapid reduction in spatial size, and the disjoint nature of the pooling regions. We have formulated a fractional version of max-pooling where alpha is allowed to take non-integer values. Our version of max-pooling is stochastic as there are lots of different ways of constructing suitable pooling regions. We find that our form of fractional max-pooling reduces overfitting on a variety of datasets: for instance, we improve on the state-of-the art for CIFAR-100 without even using dropout.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1412.6071v4",
                "caption": "Fractional Max-Pooling",
                "credit": "Benjamin Graham"
            }
        },
        {
            "start_date": {
                "year": 2014,
                "month": 12,
                "day": 10
            },
            "text": {
                "headline": "NTM",
                "text": "<p>We extend the capabilities of neural networks by coupling them to external memory resources, which they can interact with by attentional processes. The combined system is analogous to a Turing Machine or Von Neumann architecture but is differentiable end-to-end, allowing it to be efficiently trained with gradient descent. Preliminary results demonstrate that Neural Turing Machines can infer simple algorithms such as copying, sorting, and associative recall from input and output examples.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1410.5401",
                "caption": "Neural Turing Machines",
                "credit": "Alex Graves, Greg Wayne, Ivo Danihelka"
            }
        },
        {
            "start_date": {
                "year": 2014,
                "month": 12,
                "day": 3
            },
            "text": {
                "headline": "SNM-skip",
                "text": "<p>We present a novel family of language model (LM) estimation techniques named Sparse Non-negative Matrix (SNM) estimation. A first set of experiments empirically evaluating it on the One Billion Word Benchmark shows that SNM n-gram LMs perform almost as well as the well-established Kneser-Ney (KN) models. When using skip-gram features the models are able to match the state-of-the-art recurrent neural network (RNN) LMs; combining the two modeling techniques yields the best known result on the benchmark. The computational advantages of SNM over both maximum entropy and RNN LM estimation are probably its main strength, promising an approach that has the same flexibility in combining arbitrary features effectively and yet should scale to very large amounts of data as gracefully as n-gram LMs do. </p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1412.1454",
                "caption": "Skip-gram Language Modeling Using Sparse Non-negative Matrix Probability Estimation",
                "credit": "Noam Shazeer, Joris Pelemans, Ciprian Chelba"
            }
        },
        {
            "start_date": {
                "year": 2014,
                "month": 11,
                "day": 28
            },
            "text": {
                "headline": "Cascaded LNet-ANet",
                "text": "<p>Predicting face attributes in the wild is challenging due to complex face variations. We propose a novel deep learning framework for attribute prediction in the wild. It cascades two CNNs, LNet and ANet, which are fine-tuned jointly with attribute tags, but pre-trained differently. LNet is pre-trained by massive general object categories for face localization, while ANet is pre-trained by massive face identities for attribute prediction. This framework not only outperforms the state-of-the-art with a large margin, but also reveals valuable facts on learning face representation.\n(1) It shows how the performances of face localization (LNet) and attribute prediction (ANet) can be improved by different pre-training strategies.\n(2) It reveals that although the filters of LNet are fine-tuned only with image-level attribute tags, their response maps over entire images have strong indication of face locations. This fact enables training LNet for face localization with only image-level annotations, but without face bounding boxes or landmarks, which are required by all attribute recognition works.\n(3) It also demonstrates that the high-level hidden neurons of ANet automatically discover semantic concepts after pre-training with massive face identities, and such concepts are significantly enriched after fine-tuning with attribute tags. Each attribute can be well explained with a sparse linear combination of these concepts.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1411.7766",
                "caption": "Deep Learning Face Attributes in the Wild",
                "credit": "Ziwei Liu, Ping Luo, Xiaogang Wang, Xiaoou Tang"
            }
        },
        {
            "start_date": {
                "year": 2014,
                "month": 11,
                "day": 14
            },
            "text": {
                "headline": "Fully Convolutional Networks",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1411.4038",
                "caption": "Fully Convolutional Networks for Semantic Segmentation",
                "credit": "J Long, E Shelhamer, T Darrell"
            }
        },
        {
            "start_date": {
                "year": 2014,
                "month": 11,
                "day": 10
            },
            "text": {
                "headline": "SC-NLM",
                "text": "<p>Inspired by recent advances in multimodal learning and machine translation, we introduce an encoder-decoder pipeline that learns (a): a multimodal joint embedding space with images and text and (b): a novel language model for decoding distributed representations from our space. Our pipeline effectively unifies joint image-text embedding models with multimodal neural language models. We introduce the structure-content neural language model that disentangles the structure of a sentence to its content, conditioned on representations produced by the encoder. The encoder allows one to rank images and sentences while the decoder can generate novel descriptions from scratch. Using LSTM to encode sentences, we match the state-of-the-art performance on Flickr8K and Flickr30K without using object detections. We also set new best results when using the 19-layer Oxford convolutional network. Furthermore we show that with linear encoders, the learned embedding space captures multimodal regularities in terms of vector space arithmetic e.g. *image of a blue car* - \"blue\" + \"red\" is near images of red cars. Sample captions generated for 800 images are made available for comparison.</p>"
            },
            "media": {
                "url": "https://www.semanticscholar.org/paper/Unifying-Visual-Semantic-Embeddings-with-Multimodal-Kiros-Salakhutdinov/2e36ea91a3c8fbff92be2989325531b4002e2afc",
                "caption": "Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models",
                "credit": "Ryan Kiros, R. Salakhutdinov, R. Zemel"
            }
        },
        {
            "start_date": {
                "year": 2014,
                "month": 11,
                "day": 7
            },
            "text": {
                "headline": "LRCN",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1411.4389",
                "caption": "Long-term Recurrent Convolutional Networks for Visual Recognition and Description",
                "credit": "Jeff Donahue, Lisa Anne Hendricks, Marcus Rohrbach, Subhashini Venugopalan, Sergio Guadarrama, Kate Saenko, Trevor Darrell"
            }
        },
        {
            "start_date": {
                "year": 2014,
                "month": 9,
                "day": 23
            },
            "text": {
                "headline": "Spatially-Sparse CNN",
                "text": "<p>Convolutional neural networks (CNNs) perform well on problems such as handwriting recognition and image classification. However, the performance of the networks is often limited by budget and time constraints, particularly when trying to train deep networks.\nMotivated by the problem of online handwriting recognition, we developed a CNN for processing spatially-sparse inputs; a character drawn with a one-pixel wide pen on a high resolution grid looks like a sparse matrix. Taking advantage of the sparsity allowed us more efficiently to train and test large, deep CNNs. On the CASIA-OLHWDB1.1 dataset containing 3755 character classes we get a test error of 3.82%.\nAlthough pictures are not sparse, they can be thought of as sparse by adding padding. Applying a deep convolutional network using sparsity has resulted in a substantial reduction in test error on the CIFAR small picture datasets: 6.28% on CIFAR-10 and 24.30% for CIFAR-100.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1409.6070v1",
                "caption": "Spatially-sparse convolutional neural networks",
                "credit": "Benjamin Graham"
            }
        },
        {
            "start_date": {
                "year": 2014,
                "month": 9,
                "day": 18
            },
            "text": {
                "headline": "Deeply-supervised nets",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1409.5185",
                "caption": "Deeply-Supervised Nets",
                "credit": "Chen-Yu Lee, Saining Xie, Patrick Gallagher, Zhengyou Zhang, Zhuowen Tu"
            }
        },
        {
            "start_date": {
                "year": 2014,
                "month": 9,
                "day": 17
            },
            "text": {
                "headline": "GoogLeNet / InceptionV1",
                "text": "<p>We propose a deep convolutional neural network architecture codenamed \"Inception\", which was responsible for setting the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC 2014). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. This was achieved by a carefully crafted design that allows for increasing the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC 2014 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1409.4842",
                "caption": "Going deeper with convolutions",
                "credit": "Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich"
            }
        },
        {
            "start_date": {
                "year": 2014,
                "month": 9,
                "day": 14
            },
            "text": {
                "headline": "SPN-4+KN5",
                "text": "<p>Sum product networks (SPNs) are a new class of deep probabilistic models. They can contain multiple hidden layers while keeping their inference and training times tractable. An SPN consists of interleaving layers of sum nodes and product nodes. A sum node can be interpreted as a hidden variable, and a product node can be viewed as a feature capturing rich interactions among an SPN’s inputs. We show that the ability of SPN to use hidden layers to model complex dependencies among words, and its tractable inference and learning times, make it a suitable framework for a language model. Even though SPNs have been applied to a variety of vision problems [1, 2], we are the first to use it for language modeling. Our empirical comparisons with\nsix previous language models indicate that our SPN has superior performance.</p>"
            },
            "media": {
                "url": "https://www.comp.nus.edu.sg/~skok/papers/is14.pdf",
                "caption": "Language modeling with sum-product networks",
                "credit": "W. Cheng, Stanley Kok, Hoai Vu Pham, Hai Leong Chieu, K. M. A. Chai"
            }
        },
        {
            "start_date": {
                "year": 2014,
                "month": 9,
                "day": 10
            },
            "text": {
                "headline": "Seq2Seq LSTM",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1409.3215",
                "caption": "Sequence to Sequence Learning with Neural Networks",
                "credit": "I Sutskever, O Vinyals, QV Le"
            }
        },
        {
            "start_date": {
                "year": 2014,
                "month": 9,
                "day": 8
            },
            "text": {
                "headline": "Large regularized LSTM",
                "text": "<p>We present a simple regularization technique for Recurrent Neural Networks (RNNs) with Long Short-Term Memory (LSTM) units. Dropout, the most successful technique for regularizing neural networks, does not work well with RNNs and LSTMs. In this paper, we show how to correctly apply dropout to LSTMs, and show that it substantially reduces overfitting on a variety of tasks. These tasks include language modeling, speech recognition, image caption generation, and machine translation.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1409.2329",
                "caption": "Recurrent Neural Network Regularization",
                "credit": "Wojciech Zaremba, Ilya Sutskever, Oriol Vinyals"
            }
        },
        {
            "start_date": {
                "year": 2014,
                "month": 9,
                "day": 4
            },
            "text": {
                "headline": "VGG19",
                "text": "<p>In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1409.1556",
                "caption": "Very Deep Convolutional Networks for Large-Scale Image Recognition",
                "credit": "Karen Simonyan, Andrew Zisserman"
            }
        },
        {
            "start_date": {
                "year": 2014,
                "month": 9,
                "day": 4
            },
            "text": {
                "headline": "VGG16",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1409.1556",
                "caption": "Very Deep Convolutional Networks for Large-Scale Image Recognition",
                "credit": "Karen Simonyan; Andrew Zisserman"
            }
        },
        {
            "start_date": {
                "year": 2014,
                "month": 9,
                "day": 1
            },
            "text": {
                "headline": "RNNsearch-50*",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1409.0473",
                "caption": "Neural Machine Translation by Jointly Learning to Align and Translate",
                "credit": "D Bahdanau, K Cho, Y Bengio"
            }
        },
        {
            "start_date": {
                "year": 2014,
                "month": 7,
                "day": 1
            },
            "text": {
                "headline": "SmooCT",
                "text": "<p>Self-play reinforcement learning has proved to be successful in many perfect information two-player games.\nHowever, research carrying over its theoretical guarantees and practical success to games of imperfect information has been lacking. In this paper, we evaluate selfplay Monte-Carlo Tree Search (MCTS) in limit Texas Hold’em and Kuhn poker. We introduce a variant of the established UCB algorithm and provide first empirical results demonstrating its ability to find approximate Nash equilibria.</p>"
            },
            "media": {
                "url": "https://www.semanticscholar.org/paper/Self-play-Monte-Carlo-tree-search-in-computer-poker-Heinrich-Silver/7b687599b4425aa959036071030e1212a3b359c7",
                "caption": "Self-Play Monte-Carlo Tree Search in Computer Poker",
                "credit": "Johannes Heinrich, David Silver"
            }
        },
        {
            "start_date": {
                "year": 2014,
                "month": 6,
                "day": 23
            },
            "text": {
                "headline": "Multiresolution CNN",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://ieeexplore.ieee.org/document/6909619",
                "caption": "Large-Scale Video Classification with Convolutional Neural Networks",
                "credit": "A Karpathy, G Toderici, S Shetty, T Leung"
            }
        },
        {
            "start_date": {
                "year": 2014,
                "month": 6,
                "day": 23
            },
            "text": {
                "headline": "DeepFace",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://ieeexplore.ieee.org/document/6909616",
                "caption": "DeepFace: Closing the Gap to Human-Level Performance in Face Verification",
                "credit": "Y Taigman, M Yang, MA Ranzato"
            }
        },
        {
            "start_date": {
                "year": 2014,
                "month": 6,
                "day": 22
            },
            "text": {
                "headline": "RNN-WER",
                "text": "<p>This paper presents a speech recognition system that directly transcribes audio data with text, without requiring an intermediate phonetic representation. The system is based on a combination of the deep bidirectional LSTM recurrent neural network architecture and the Connectionist Temporal Classification objective function. A modification to the objective function is introduced that trains the network to minimise the expectation of an arbitrary transcription loss function. This allows a direct optimisation of the word error rate, even in the absence of a lexicon or language model. The system achieves a word error rate of 27.3% on the Wall Street Journal corpus with no prior linguistic information, 21.9% with only a lexicon of allowed words, and 8.2% with a trigram language model. Combining the network with a baseline system further reduces the error rate to 6.7%.</p>"
            },
            "media": {
                "url": "https://proceedings.mlr.press/v32/graves14.html",
                "caption": "Towards End-To-End Speech Recognition with Recurrent Neural Networks",
                "credit": "Alex Graves, Navdeep Jaitly"
            }
        },
        {
            "start_date": {
                "year": 2014,
                "month": 6,
                "day": 21
            },
            "text": {
                "headline": "Fragment embedding",
                "text": "<p>We introduce a model for bidirectional retrieval of images and sentences through a deep, multi-modal embedding of visual and natural language data. Unlike previous models that directly map images or sentences into a common embedding space, our model works on a finer level and embeds fragments of images (objects) and fragments of sentences (typed dependency tree relations) into a common space. We then introduce a structured max-margin objective that allows our model to explicitly associate these fragments across modalities. Extensive experimental evaluation shows that reasoning on both the global level of images and sentences and the finer level of their respective fragments improves performance on image-sentence retrieval tasks. Additionally, our model provides interpretable predictions for the image-sentence retrieval task since the inferred inter-modal alignment of fragments is explicit.</p>"
            },
            "media": {
                "url": "https://www.semanticscholar.org/paper/Deep-Fragment-Embeddings-for-Bidirectional-Image-Karpathy-Joulin/7f1b111f0bb703b0bd97aba505728a9b0d9b2a54",
                "caption": "Deep Fragment Embeddings for Bidirectional Image Sentence Mapping",
                "credit": "A. Karpathy, Armand Joulin, Li Fei-Fei"
            }
        },
        {
            "start_date": {
                "year": 2014,
                "month": 6,
                "day": 18
            },
            "text": {
                "headline": "SPPNet",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1406.4729",
                "caption": "Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition",
                "credit": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun"
            }
        },
        {
            "start_date": {
                "year": 2014,
                "month": 6,
                "day": 10
            },
            "text": {
                "headline": "GANs",
                "text": "<p>We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1406.2661",
                "caption": "Generative Adversarial Networks",
                "credit": "Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio"
            }
        },
        {
            "start_date": {
                "year": 2014,
                "month": 6,
                "day": 9
            },
            "text": {
                "headline": "Two-stream ConvNets for action recognition",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1406.2199",
                "caption": "Two-Stream Convolutional Networks for Action Recognition in Videos",
                "credit": "Karen Simonyan, Andrew Zisserman"
            }
        },
        {
            "start_date": {
                "year": 2014,
                "month": 6,
                "day": 3
            },
            "text": {
                "headline": "GRUs",
                "text": "<p>In this paper, we propose a novel neural network model called RNN Encoder-Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixed-length vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1406.1078",
                "caption": "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation",
                "credit": "Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, Yoshua Bengio"
            }
        },
        {
            "start_date": {
                "year": 2014,
                "month": 6,
                "day": 1
            },
            "text": {
                "headline": "AdaRNN",
                "text": "<p>We propose Adaptive Recursive Neural Network (AdaRNN) for target-dependent Twitter sentiment classification. AdaRNN adaptively propagates the sentiments of words to target depending on the context and syntactic relationships between them. It consists of more than one composition functions, and we model the adaptive sentiment propagations as distributions over these composition functions. The experimental studies illustrate that AdaRNN improves the baseline methods. Furthermore, we introduce a manually annotated dataset for target-dependent Twitter sentiment analysis.</p>"
            },
            "media": {
                "url": "https://www.semanticscholar.org/paper/Adaptive-Recursive-Neural-Network-for-Twitter-Dong-Wei/06e122f475a21d92dba137609c40f35690217475",
                "caption": "Adaptive Recursive Neural Network for Target-dependent Twitter Sentiment Classification",
                "credit": "Li Dong, Furu Wei, Chuanqi Tan, Duyu Tang, M. Zhou, Ke Xu"
            }
        },
        {
            "start_date": {
                "year": 2014,
                "month": 5,
                "day": 14
            },
            "text": {
                "headline": "Paragraph Vector",
                "text": "<p>Many machine learning algorithms require the input to be represented as a fixed-length feature vector. When it comes to texts, one of the most common fixed-length features is bag-of-words. Despite their popularity, bag-of-words features have two major weaknesses: they lose the ordering of the words and they also ignore semantics of the words. For example, \"powerful,\" \"strong\" and \"Paris\" are equally distant. In this paper, we propose Paragraph Vector, an unsupervised algorithm that learns fixed-length feature representations from variable-length pieces of texts, such as sentences, paragraphs, and documents. Our algorithm represents each document by a dense vector which is trained to predict words in the document. Its construction gives our algorithm the potential to overcome the weaknesses of bag-of-words models. Empirical results show that Paragraph Vectors outperforms bag-of-words models as well as other techniques for text representations. Finally, we achieve new state-of-the-art results on several text classification and sentiment analysis tasks.</p>"
            },
            "media": {
                "url": "https://www.semanticscholar.org/paper/Distributed-Representations-of-Sentences-and-Le-Mikolov/f3de86aeb442216a8391befcacb49e58b478f512",
                "caption": "Distributed Representations of Sentences and Documents",
                "credit": "Quoc V. Le, Tomas Mikolov"
            }
        },
        {
            "start_date": {
                "year": 2014,
                "month": 3,
                "day": 5
            },
            "text": {
                "headline": "HyperNEAT",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://ieeexplore.ieee.org/abstract/document/6756960",
                "caption": "A Neuroevolution Approach to General Atari Game Playing",
                "credit": "M Hausknecht, J Lehman"
            }
        },
        {
            "start_date": {
                "year": 2014,
                "month": 1,
                "day": 1
            },
            "text": {
                "headline": "GloVe (32B)",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://nlp.stanford.edu/projects/glove/",
                "caption": "GloVe: Global Vectors for Word Representation",
                "credit": "J Pennington, R Socher, CD Manning"
            }
        },
        {
            "start_date": {
                "year": 2014,
                "month": 1,
                "day": 1
            },
            "text": {
                "headline": "GloVe (6B)",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://nlp.stanford.edu/projects/glove/",
                "caption": "GloVe: Global Vectors for Word Representation",
                "credit": "J Pennington, R Socher, CD Manning"
            }
        },
        {
            "start_date": {
                "year": 2013,
                "month": 12,
                "day": 21
            },
            "text": {
                "headline": "OverFeat",
                "text": "<p>We present an integrated framework for using Convolutional Networks for classification, localization and detection. We show how a multiscale and sliding window approach can be efficiently implemented within a ConvNet. We also introduce a novel deep learning approach to localization by learning to predict object boundaries. Bounding boxes are then accumulated rather than suppressed in order to increase detection confidence. We show that different tasks can be learned simultaneously using a single shared network. This integrated framework is the winner of the localization task of the ImageNet Large Scale Visual Recognition Challenge 2013 (ILSVRC2013) and obtained very competitive results for the detection and classifications tasks. In post-competition work, we establish a new state of the art for the detection task. Finally, we release a feature extractor from our best model called OverFeat.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1312.6229",
                "caption": "OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks",
                "credit": "Pierre Sermanet, David Eigen, Xiang Zhang, Michael Mathieu, Rob Fergus, Yann LeCun"
            }
        },
        {
            "start_date": {
                "year": 2013,
                "month": 12,
                "day": 20
            },
            "text": {
                "headline": "Image generation",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1312.6114",
                "caption": "Auto-Encoding Variational Bayes",
                "credit": "DP Kingma, M Welling"
            }
        },
        {
            "start_date": {
                "year": 2013,
                "month": 12,
                "day": 20
            },
            "text": {
                "headline": "DOT(S)-RNN",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1312.6026",
                "caption": "How to Construct Deep Recurrent Neural Networks",
                "credit": "Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, Yoshua Bengio"
            }
        },
        {
            "start_date": {
                "year": 2013,
                "month": 12,
                "day": 19
            },
            "text": {
                "headline": "DQN",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1312.5602",
                "caption": "Playing Atari with Deep Reinforcement Learning",
                "credit": "V Mnih, K Kavukcuoglu, D Silver, A Graves"
            }
        },
        {
            "start_date": {
                "year": 2013,
                "month": 12,
                "day": 16
            },
            "text": {
                "headline": "Network in Network",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1312.4400",
                "caption": "Network In Network",
                "credit": "M Lin, Q Chen, S Yan"
            }
        },
        {
            "start_date": {
                "year": 2013,
                "month": 12,
                "day": 11
            },
            "text": {
                "headline": "RNN for 1B words",
                "text": "<p>We propose a new benchmark corpus to be used for measuring progress in statistical language modeling. With almost one billion words of training data, we hope this benchmark will be useful to quickly evaluate novel language modeling techniques, and to compare their contribution when combined with other advanced techniques. We show performance of several well-known types of language models, with the best results achieved with a recurrent neural network based language model. The baseline unpruned Kneser-Ney 5-gram model achieves perplexity 67.6; a combination of techniques leads to 35% reduction in perplexity, or 10% reduction in cross-entropy (bits), over that baseline.\nThe benchmark is available as a this http URL project; besides the scripts needed to rebuild the training/held-out data, it also makes available log-probability values for each word in each of ten held-out data sets, for each of the baseline n-gram models. </p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1312.3005",
                "caption": "One Billion Word Benchmark for Measuring Progress in Statistical Language Modeling",
                "credit": "Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, Tony Robinson"
            }
        },
        {
            "start_date": {
                "year": 2013,
                "month": 12,
                "day": 8
            },
            "text": {
                "headline": "DBLSTM",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://ieeexplore.ieee.org/document/6707742",
                "caption": "Hybrid speech recognition with Deep Bidirectional LSTM",
                "credit": "A Graves, N Jaitly, A Mohamed"
            }
        },
        {
            "start_date": {
                "year": 2013,
                "month": 12,
                "day": 5
            },
            "text": {
                "headline": "TransE",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://papers.nips.cc/paper/2013/hash/1cecc7a77928ca8133fa24680a88d2f9-Abstract.html",
                "caption": "Translating Embeddings for Modeling Multi- relational Data",
                "credit": "Antoine Bordes, Nicolas Usunier, Alberto Garcia- Duran, Jason Weston, and Oksana Yakhnenko"
            }
        },
        {
            "start_date": {
                "year": 2013,
                "month": 12,
                "day": 5
            },
            "text": {
                "headline": "DeViSE",
                "text": "<p>Modern visual recognition systems are often limited in their ability to scale to large numbers of object categories. This limitation is in part due to the increasing difficulty of acquiring sufficient training data in the form of labeled images as the number of object categories grows. One remedy is to leverage data from other sources - such as text data - both to train visual models and to constrain their predictions. In this paper we present a new deep visual-semantic embedding model trained to identify visual objects using both labeled image data as well as semantic information gleaned from unannotated text. We demonstrate that this model matches state-of-the-art performance on the 1000-class ImageNet object recognition challenge while making more semantically reasonable errors, and also show that the semantic information can be exploited to make predictions about tens of thousands of image labels not observed during training. Semantic knowledge improves such zero-shot predictions achieving hit rates of up to 18% across thousands of novel labels never seen by the visual model.</p>"
            },
            "media": {
                "url": "https://www.semanticscholar.org/paper/DeViSE%3A-A-Deep-Visual-Semantic-Embedding-Model-Frome-Corrado/4aa4069693bee00d1b0759ca3df35e59284e9845",
                "caption": "DeViSE: A Deep Visual-Semantic Embedding Model",
                "credit": "Andrea Frome, G. Corrado, Jonathon Shlens, Samy Bengio, J. Dean, Marc'Aurelio Ranzato, Tomas Mikolov"
            }
        },
        {
            "start_date": {
                "year": 2013,
                "month": 12,
                "day": 1
            },
            "text": {
                "headline": "TensorReasoner",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://papers.nips.cc/paper/2013/hash/b337e84de8752b27eda3a12363109e80-Abstract.html",
                "caption": "Reasoning With Neural Tensor Networks for Knowledge Base Completion",
                "credit": "R Socher, D Chen, CD Manning, A Ng"
            }
        },
        {
            "start_date": {
                "year": 2013,
                "month": 11,
                "day": 12
            },
            "text": {
                "headline": "Visualizing CNNs",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1311.2901",
                "caption": "Visualizing and Understanding Convolutional Networks",
                "credit": "MD Zeiler, R Fergus"
            }
        },
        {
            "start_date": {
                "year": 2013,
                "month": 11,
                "day": 11
            },
            "text": {
                "headline": "R-CNN (T-net)",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1311.2524",
                "caption": "Rich feature hierarchies for accurate object detection and semantic segmentation",
                "credit": "Ross Girshick, Jeff Donahue, Trevor Darrell, Jitendra Malik"
            }
        },
        {
            "start_date": {
                "year": 2013,
                "month": 10,
                "day": 16
            },
            "text": {
                "headline": "Word2Vec (small)",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1310.4546",
                "caption": "Distributed Representations of Words and Phrases and their Compositionality",
                "credit": "T Mikolov, I Sutskever, K Chen, GS Corrado"
            }
        },
        {
            "start_date": {
                "year": 2013,
                "month": 10,
                "day": 16
            },
            "text": {
                "headline": "Word2Vec (large)",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1310.4546",
                "caption": "Distributed Representations of Words and Phrases and their Compositionality",
                "credit": "T Mikolov, I Sutskever, K Chen, GS Corrado"
            }
        },
        {
            "start_date": {
                "year": 2013,
                "month": 10,
                "day": 1
            },
            "text": {
                "headline": "RNTN",
                "text": "<p>Semantic word spaces have been very useful but cannot express the meaning of longer phrases in a principled way. Further progress towards understanding compositionality in tasks such as sentiment detection requires richer supervised training and evaluation resources and more powerful models of composition. To remedy this, we introduce a Sentiment Treebank. It includes fine grained sentiment labels for 215,154 phrases in the parse trees of 11,855 sentences and presents new challenges for sentiment compositionality. To address them, we introduce the Recursive Neural Tensor Network. When trained on the new treebank, this model outperforms all previous methods on several metrics. It pushes the state of the art in single sentence positive/negative classification from 80% up to 85.4%. The accuracy of predicting fine-grained sentiment labels for all phrases reaches 80.7%, an improvement of 9.7% over bag of features baselines. Lastly, it is the only model that can accurately capture the effects of negation and its scope at various tree levels for both positive and negative phrases.</p>"
            },
            "media": {
                "url": "https://www.semanticscholar.org/paper/Recursive-Deep-Models-for-Semantic-Compositionality-Socher-Perelygin/687bac2d3320083eb4530bf18bb8f8f721477600",
                "caption": "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank",
                "credit": "R. Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, A. Ng, Christopher Potts"
            }
        },
        {
            "start_date": {
                "year": 2013,
                "month": 10,
                "day": 1
            },
            "text": {
                "headline": "RCTM",
                "text": "<p>We introduce a class of probabilistic continuous translation models called Recurrent Continuous Translation Models that are purely based on continuous representations for words, phrases and sentences and do not rely on alignments or phrasal translation units. The models have a generation and a conditioning aspect. The generation of the translation is modelled with a target Recurrent Language Model, whereas the conditioning on the source sentence is modelled with a Convolutional Sentence Model. Through various experiments, we show first that our models obtain a perplexity with respect to gold translations that is > 43% lower than that of stateof-the-art alignment-based translation models. Secondly, we show that they are remarkably sensitive to the word order, syntax, and meaning of the source sentence despite lacking alignments. Finally we show that they match a state-of-the-art system when rescoring n-best lists of translations.</p>"
            },
            "media": {
                "url": "https://www.semanticscholar.org/paper/Recurrent-Continuous-Translation-Models-Kalchbrenner-Blunsom/944a1cfd79dbfb6fef460360a0765ba790f4027a",
                "caption": "Recurrent Continuous Translation Models",
                "credit": "Nal Kalchbrenner, Phil Blunsom"
            }
        },
        {
            "start_date": {
                "year": 2013,
                "month": 9,
                "day": 22
            },
            "text": {
                "headline": "Mitosis",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://link.springer.com/chapter/10.1007/978-3-642-40763-5_51",
                "caption": "Mitosis Detection in Breast Cancer Histology Images with Deep Neural Networks",
                "credit": "Dan C. Cireşan, Alessandro Giusti, Luca M. Gambardella, Jürgen Schmidhuber"
            }
        },
        {
            "start_date": {
                "year": 2013,
                "month": 8,
                "day": 4
            },
            "text": {
                "headline": "RNN+weight noise+dynamic eval",
                "text": "<p>This paper shows how Long Short-term Memory recurrent neural networks can be used to generate complex sequences with long-range structure, simply by predicting one data point at a time. The approach is demonstrated for text (where the data are discrete) and online handwriting (where the data are real-valued). It is then extended to handwriting synthesis by allowing the network to condition its predictions on a text sequence. The resulting system is able to generate highly realistic cursive handwriting in a wide variety of styles.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1308.0850",
                "caption": "Generating Sequences With Recurrent Neural Networks",
                "credit": "Alex Graves"
            }
        },
        {
            "start_date": {
                "year": 2013,
                "month": 6,
                "day": 12
            },
            "text": {
                "headline": "Fisher Vector image classifier",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://hal.inria.fr/hal-00830491v2/document",
                "caption": "Image Classification with the Fisher Vector: Theory and Practice",
                "credit": "orge Sanchez, Florent Perronnin, Thomas Mensink, Jakob Verbeek"
            }
        },
        {
            "start_date": {
                "year": 2013,
                "month": 6,
                "day": 9
            },
            "text": {
                "headline": "SemVec",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://www.aclweb.org/anthology/N13-1090/",
                "caption": "Linguistic Regularities in Continuous Space Word Representations",
                "credit": "T Mikolov, W Yih, G Zweig"
            }
        },
        {
            "start_date": {
                "year": 2013,
                "month": 5,
                "day": 26
            },
            "text": {
                "headline": "ReLU-Speech",
                "text": "<p>Deep neural networks have recently become the gold standard for acoustic modeling in speech recognition systems. The key computational unit of a deep network is a linear projection followed by a point-wise non-linearity, which is typically a logistic function. In this work, we show that we can improve generalization and make training of deep networks faster and simpler by substituting the logistic units with rectified linear units. These units are linear when their input is positive and zero otherwise. In a supervised setting, we can successfully train very deep nets from random initialization on a large vocabulary speech recognition task achieving lower word error rates than using a logistic network with the same topology. Similarly in an unsupervised setting, we show how we can learn sparse features that can be useful for discriminative tasks. All our experiments are executed in a distributed environment using several hundred machines and several hundred hours of speech data.</p>"
            },
            "media": {
                "url": "https://www.semanticscholar.org/paper/On-rectified-linear-units-for-speech-processing-Zeiler-Ranzato/64da1980714cfc130632c5b92b9d98c2f6763de6",
                "caption": "On rectified linear units for speech processing",
                "credit": "Matthew D. Zeiler, Marc'Aurelio Ranzato, R. Monga, Mark Z. Mao, K. Yang, Quoc V. Le, Patrick Nguyen, A. Senior, Vincent Vanhoucke, J. Dean, Geoffrey E. Hinton"
            }
        },
        {
            "start_date": {
                "year": 2013,
                "month": 5,
                "day": 26
            },
            "text": {
                "headline": "Multilingual DNN",
                "text": "<p>Today's speech recognition technology is mature enough to be useful for many practical applications. In this context, it is of paramount importance to train accurate acoustic models for many languages within given resource constraints such as data, processing power, and time. Multilingual training has the potential to solve the data issue and close the performance gap between resource-rich and resource-scarce languages. Neural networks lend themselves naturally to parameter sharing across languages, and distributed implementations have made it feasible to train large networks. In this paper, we present experimental results for cross- and multi-lingual network training of eleven Romance languages on 10k hours of data in total. The average relative gains over the monolingual baselines are 4%/2% (data-scarce/data-rich languages) for cross- and 7%/2% for multi-lingual training. However, the additional gain from jointly training the languages on all data comes at an increased training time of roughly four weeks, compared to two weeks (monolingual) and one week (crosslingual).</p>"
            },
            "media": {
                "url": "https://www.semanticscholar.org/paper/Multilingual-acoustic-models-using-distributed-deep-Heigold-Vanhoucke/a41b826d23957d6ad4e9e794d20a583a9b567c5d",
                "caption": "Multilingual acoustic models using distributed deep neural networks",
                "credit": "G. Heigold, Vincent Vanhoucke, A. Senior, Patrick Nguyen, Marc'Aurelio Ranzato, M. Devin, J. Dean"
            }
        },
        {
            "start_date": {
                "year": 2013,
                "month": 4,
                "day": 2
            },
            "text": {
                "headline": "Selective Search",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://link.springer.com/article/10.1007/s11263-013-0620-5",
                "caption": "Selective search for object recognition",
                "credit": "JRR Uijlings, KEA Van De Sande, T Gevers"
            }
        },
        {
            "start_date": {
                "year": 2013,
                "month": 3,
                "day": 22
            },
            "text": {
                "headline": "PreTrans-3L-250H",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1303.5778",
                "caption": "Speech Recognition with Deep Recurrent Neural Networks",
                "credit": "Alex Graves, Abdel-rahman Mohamed, Geoffrey Hinton"
            }
        },
        {
            "start_date": {
                "year": 2013,
                "month": 2,
                "day": 18
            },
            "text": {
                "headline": "Maxout Networks",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1302.4389",
                "caption": "Maxout Networks ",
                "credit": " Ian J. Goodfellow, David Warde-Farley, Mehdi Mirza, Aaron Courville, Yoshua Bengio"
            }
        },
        {
            "start_date": {
                "year": 2013,
                "month": 1,
                "day": 16
            },
            "text": {
                "headline": "Textual Imager",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1301.3666",
                "caption": "Zero-Shot Learning Through Cross-Modal Transfer",
                "credit": "R Socher, M Ganjoo, H Sridhar, O Bastani"
            }
        },
        {
            "start_date": {
                "year": 2013,
                "month": 1,
                "day": 16
            },
            "text": {
                "headline": "DistBelief NNLM",
                "text": "<p>We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1301.3781",
                "caption": "Efficient Estimation of Word Representations in Vector Space",
                "credit": "Tomas Mikolov, Kai Chen, G. Corrado, J. Dean"
            }
        },
        {
            "start_date": {
                "year": 2012,
                "month": 12,
                "day": 3
            },
            "text": {
                "headline": "DistBelief Vision",
                "text": "<p>Recent work in unsupervised feature learning and deep learning has shown that being able to train large models can dramatically improve performance. In this paper, we consider the problem of training a deep network with billions of parameters using tens of thousands of CPU cores. We have developed a software framework called DistBelief that can utilize computing clusters with thousands of machines to train large models. Within this framework, we have developed two algorithms for large-scale distributed training: (i) Downpour SGD, an asynchronous stochastic gradient descent procedure supporting a large number of model replicas, and (ii) Sandblaster, a framework that supports a variety of distributed batch optimization procedures, including a distributed implementation of L-BFGS. Downpour SGD and Sandblaster L-BFGS both increase the scale and speed of deep network training. We have successfully used our system to train a deep network 30x larger than previously reported in the literature, and achieves state-of-the-art performance on ImageNet, a visual object recognition task with 16 million images and 21k categories. We show that these same techniques dramatically accelerate the training of a more modestly- sized deep network for a commercial speech recognition service. Although we focus on and report performance of these methods as applied to training large neural networks, the underlying algorithms are applicable to any gradient-based machine learning algorithm.</p>"
            },
            "media": {
                "url": "https://www.semanticscholar.org/paper/Large-Scale-Distributed-Deep-Networks-Dean-Corrado/3127190433230b3dc1abd0680bb58dced4bcd90e",
                "caption": "Large Scale Distributed Deep Networks",
                "credit": "J. Dean, G. Corrado, R. Monga, Kai Chen, M. Devin, Quoc V. Le, Mark Z. Mao, Marc'Aurelio Ranzato, A. Senior, P. Tucker, Ke Yang, A. Ng"
            }
        },
        {
            "start_date": {
                "year": 2012,
                "month": 12,
                "day": 3
            },
            "text": {
                "headline": "DistBelief Speech",
                "text": "<p>Recent work in unsupervised feature learning and deep learning has shown that being able to train large models can dramatically improve performance. In this paper, we consider the problem of training a deep network with billions of parameters using tens of thousands of CPU cores. We have developed a software framework called DistBelief that can utilize computing clusters with thousands of machines to train large models. Within this framework, we have developed two algorithms for large-scale distributed training: (i) Downpour SGD, an asynchronous stochastic gradient descent procedure supporting a large number of model replicas, and (ii) Sandblaster, a framework that supports a variety of distributed batch optimization procedures, including a distributed implementation of L-BFGS. Downpour SGD and Sandblaster L-BFGS both increase the scale and speed of deep network training. We have successfully used our system to train a deep network 30x larger than previously reported in the literature, and achieves state-of-the-art performance on ImageNet, a visual object recognition task with 16 million images and 21k categories. We show that these same techniques dramatically accelerate the training of a more modestly- sized deep network for a commercial speech recognition service. Although we focus on and report performance of these methods as applied to training large neural networks, the underlying algorithms are applicable to any gradient-based machine learning algorithm.</p>"
            },
            "media": {
                "url": "https://www.semanticscholar.org/paper/Large-Scale-Distributed-Deep-Networks-Dean-Corrado/3127190433230b3dc1abd0680bb58dced4bcd90e",
                "caption": "Large Scale Distributed Deep Networks",
                "credit": "J. Dean, G. Corrado, R. Monga, Kai Chen, M. Devin, Quoc V. Le, Mark Z. Mao, Marc'Aurelio Ranzato, A. Senior, P. Tucker, Ke Yang, A. Ng"
            }
        },
        {
            "start_date": {
                "year": 2012,
                "month": 12,
                "day": 2
            },
            "text": {
                "headline": "Bayesian automated hyperparameter tuning",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1206.2944",
                "caption": "Practical Bayesian optimization of machine learning algorithms",
                "credit": "J Snoek, H Larochelle, RP Adams"
            }
        },
        {
            "start_date": {
                "year": 2012,
                "month": 12,
                "day": 1
            },
            "text": {
                "headline": "RNN+LDA+KN5+cache",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/rnn_ctxt.pdf",
                "caption": "Context dependent recurrent neural network language model",
                "credit": "Tomas Mikolov, Geoffrey Zweig"
            }
        },
        {
            "start_date": {
                "year": 2012,
                "month": 9,
                "day": 30
            },
            "text": {
                "headline": "AlexNet",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://proceedings.neurips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html",
                "caption": "ImageNet Classification with Deep Convolutional Neural Networks",
                "credit": "Alex Krizhevsky, Ilya Sutskever, Geoffrey Hinton"
            }
        },
        {
            "start_date": {
                "year": 2012,
                "month": 9,
                "day": 9
            },
            "text": {
                "headline": "LSTM LM",
                "text": "<p>Neural networks have become increasingly popular for the task of language modeling. Whereas feed-forward networks only exploit a fixed context length to predict the next word of a sequence, conceptually, standard recurrent neural networks can take into account all of the predecessor words. On the other hand, it is well known that recurrent networks are difficult to train and therefore are unlikely to show the full potential of recurrent models. These problems are addressed by a the Long Short-Term Memory neural network architecture. In this work, we analyze this type of network on an English and a large French language modeling task. Experiments show improvements of about 8 % relative in perplexity over standard recurrent neural network LMs. In addition, we gain considerable improvements in WER on top of a state-of-the-art speech recognition system.</p>"
            },
            "media": {
                "url": "https://www.semanticscholar.org/paper/LSTM-Neural-Networks-for-Language-Modeling-Sundermeyer-Schl%C3%BCter/f9a1b3850dfd837793743565a8af95973d395a4e",
                "caption": "LSTM Neural Networks for Language Modeling",
                "credit": "M. Sundermeyer, R. Schlüter, H. Ney"
            }
        },
        {
            "start_date": {
                "year": 2012,
                "month": 9,
                "day": 1
            },
            "text": {
                "headline": "LSTM-300units",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://www.isca-archive.org/interspeech_2012/sundermeyer12_interspeech.pdf",
                "caption": "LSTM Neural Networks for Language Modeling",
                "credit": "Martin Sundermeyer, Ralf Schlüter, Hermann Ney"
            }
        },
        {
            "start_date": {
                "year": 2012,
                "month": 7,
                "day": 27
            },
            "text": {
                "headline": "Context-dependent RNN",
                "text": "<p>Recurrent neural network language models (RNNLMs) have recently demonstrated state-of-the-art performance across a variety of tasks. In this paper, we improve their performance by providing a contextual real-valued input vector in association with each word. This vector is used to convey contextual information about the sentence being modeled. By performing Latent Dirichlet Allocation using a block of preceding text, we achieve a topic-conditioned RNNLM. This approach has the key advantage of avoiding the data fragmentation associated with building multiple topic models on different data subsets. We report perplexity results on the Penn Treebank data, where we achieve a new state-of-the-art. We further apply the model to the Wall Street Journal speech recognition task, where we observe \nimprovements in word-error-rate.</p>"
            },
            "media": {
                "url": "https://www.microsoft.com/en-us/research/wp-content/uploads/2012/07/rnn_ctxt_TR.sav_.pdf",
                "caption": "Context Dependent Recurrent Neural Network Language Model",
                "credit": "Tomas Mikolov, Geoffrey Zweig"
            }
        },
        {
            "start_date": {
                "year": 2012,
                "month": 7,
                "day": 12
            },
            "text": {
                "headline": "Unsupervised High-level Feature Learner",
                "text": "<p>We consider the problem of building high-level, class-specific feature detectors from only unlabeled data. For example, is it possible to learn a face detector using only unlabeled images using unlabeled images? To answer this, we train a 9-layered locally connected sparse autoencoder with pooling and local contrast normalization on a large dataset of images (the model has 1 billion connections, the dataset has 10 million 200×200 pixel images downloaded from the Internet). We train this network using model parallelism and asynchronous SGD on a cluster with 1,000 machines (16,000 cores) for three days. Contrary to what appears to be a widely-held intuition, our experimental results reveal that it is possible to train a face detector without having to label images as containing a face or not. Control experiments show that this feature detector is robust not only to translation but also to scaling and out-of-plane rotation. We also find that the same network is sensitive to other high-level concepts such as cat faces and human bodies. Starting with these learned features, we trained our network to obtain 15.8% accuracy in recognizing 20,000 object categories from ImageNet, a leap of 70% relative improvement over the previous state-of-the-art.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1112.6209",
                "caption": "Building High-level Features Using Large Scale Unsupervised Learning",
                "credit": "Quoc V. Le, Marc'Aurelio Ranzato, Rajat Monga, Matthieu Devin, Kai Chen, Greg S. Corrado, Jeff Dean, Andrew Y. Ng"
            }
        },
        {
            "start_date": {
                "year": 2012,
                "month": 7,
                "day": 12
            },
            "text": {
                "headline": "MV-RNN",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://www.aclweb.org/anthology/D12-1110/",
                "caption": "Semantic Compositionality through Recursive Matrix-Vector Spaces",
                "credit": "R. Socher, B. Huval, C. D. Manning, and A. Y. Ng"
            }
        },
        {
            "start_date": {
                "year": 2012,
                "month": 6,
                "day": 3
            },
            "text": {
                "headline": "Dropout (TIMIT)",
                "text": "<p>Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different thinned networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1207.0580",
                "caption": "Improving neural networks by preventing co-adaptation of feature detectors",
                "credit": "GE Hinton, N Srivastava, A Krizhevsky"
            }
        },
        {
            "start_date": {
                "year": 2012,
                "month": 6,
                "day": 3
            },
            "text": {
                "headline": "Dropout (MNIST)",
                "text": "<p>Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different thinned networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1207.0580",
                "caption": "Improving neural networks by preventing co-adaptation of feature detectors",
                "credit": "GE Hinton, N Srivastava, A Krizhevsky"
            }
        },
        {
            "start_date": {
                "year": 2012,
                "month": 6,
                "day": 3
            },
            "text": {
                "headline": "Dropout (ImageNet)",
                "text": "<p>Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different thinned networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1207.0580",
                "caption": "Improving neural networks by preventing co-adaptation of feature detectors",
                "credit": "GE Hinton, N Srivastava, A Krizhevsky"
            }
        },
        {
            "start_date": {
                "year": 2012,
                "month": 6,
                "day": 3
            },
            "text": {
                "headline": "Dropout (CIFAR)",
                "text": "<p>Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different thinned networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.</p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1207.0580",
                "caption": "Improving neural networks by preventing co-adaptation of feature detectors",
                "credit": "GE Hinton, N Srivastava, A Krizhevsky"
            }
        },
        {
            "start_date": {
                "year": 2012,
                "month": 2,
                "day": 13
            },
            "text": {
                "headline": "MCDNN (MNIST)",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1202.2745v1",
                "caption": "Multi-column Deep Neural Networks for Image Classification",
                "credit": "D Ciregan, U Meier, J Schmidhuber"
            }
        },
        {
            "start_date": {
                "year": 2011,
                "month": 11,
                "day": 11
            },
            "text": {
                "headline": "HOGWILD!",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1106.5730",
                "caption": "HOGWILD!: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent",
                "credit": "F Niu, B Recht, C Ré, SJ Wright"
            }
        },
        {
            "start_date": {
                "year": 2011,
                "month": 11,
                "day": 8
            },
            "text": {
                "headline": "NLP from scratch",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://www.jmlr.org/papers/volume12/collobert11a/collobert11a.pdf",
                "caption": "Natural Language Processing (Almost) from Scratch",
                "credit": "Ronan Collobert, J. Weston, L. Bottou, Michael Karlen, K. Kavukcuoglu, P. Kuksa"
            }
        },
        {
            "start_date": {
                "year": 2011,
                "month": 11,
                "day": 6
            },
            "text": {
                "headline": "Domain Adaptation",
                "text": "<p></p>"
            },
            "media": {
                "url": "http://ftp.idiap.ch/pub/courses/EE-700/material/05-12-2012/2011_ICCV_DomainAdaptation.pdf",
                "caption": "Domain Adaptation for Object Recognition: An Unsupervised Approach",
                "credit": "Raghuraman Gopalan, Ruonan Li, Rama Chellappa"
            }
        },
        {
            "start_date": {
                "year": 2011,
                "month": 10,
                "day": 3
            },
            "text": {
                "headline": "Adaptive Subgrad",
                "text": "<p>We present a new family of subgradient methods that dynamically incorporate knowledge of the geometry of the data observed in earlier iterations to perform more informative gradient-based learning. Metaphorically, the adaptation allows us to find needles in haystacks in the form of very predictive but rarely seen features. Our paradigm stems from recent advances in stochastic optimization and online learning which employ proximal functions to control the gradient steps of the algorithm. We describe and analyze an apparatus for adaptively modifying the proximal function, which significantly simplifies setting a learning rate and results in regret guarantees that are provably as good as the best proximal function that can be chosen in hindsight. We give several efficient algorithms for empirical risk minimization problems with common and important regularization functions and domain constraints. We experimentally study our theoretical analysis and show that adaptive subgradient methods outperform state-of-the-art, yet non-adaptive, subgradient algorithms.</p>"
            },
            "media": {
                "url": "https://dl.acm.org/doi/10.5555/1953048.2021068",
                "caption": "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization",
                "credit": "J Duchi, E Hazan, Y Singer"
            }
        },
        {
            "start_date": {
                "year": 2011,
                "month": 7,
                "day": 1
            },
            "text": {
                "headline": "Recursive sentiment autoencoder",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://aclanthology.org/D11-1014/",
                "caption": "Semi-supervised recursive autoencoders for predicting sentiment distributions",
                "credit": "R. Socher, J. Pennington, E. H. Huang, A. Y. Ng, and C. D. Manning"
            }
        },
        {
            "start_date": {
                "year": 2011,
                "month": 6,
                "day": 28
            },
            "text": {
                "headline": "Recursive Neural Network",
                "text": "<p>Recursive structure is commonly found in the inputs of different modalities such as natural scene images or natural language sentences. Discovering this recursive structure helps us to not only identify the units that an image or sentence contains but also how they interact to form a whole. We introduce a max-margin structure prediction architecture based on recursive neural networks that can successfully recover such structure both in complex scene images as well as sentences. The same algorithm can be used both to provide a competitive syntactic parser for natural language sentences from the Penn Treebank and to outperform alternative approaches for semantic scene segmentation, annotation and classification. For segmentation and annotation our algorithm obtains a new level of state-of-the-art performance on the Stanford background dataset (78.1%). The features from the image parse tree outperform Gist descriptors for scene classification by 4%.</p>"
            },
            "media": {
                "url": "https://www.semanticscholar.org/paper/Parsing-Natural-Scenes-and-Natural-Language-with-Socher-Lin/9c0ddf74f87d154db88d79c640578c1610451eec",
                "caption": "Parsing natural scenes and natural language with recursive neural networks",
                "credit": "R. Socher, Cliff Chiung-Yu Lin, A. Ng, Christopher D. Manning"
            }
        },
        {
            "start_date": {
                "year": 2011,
                "month": 6,
                "day": 19
            },
            "text": {
                "headline": "Vector Space Model",
                "text": "<p>Unsupervised vector-based approaches to semantics can model rich lexical meanings, but they largely fail to capture sentiment information that is central to many word meanings and important for a wide range of NLP tasks. We present a model that uses a mix of unsupervised and supervised techniques to learn word vectors capturing semantic term--document information as well as rich sentiment content. The proposed model can leverage both continuous and multi-dimensional sentiment information as well as non-sentiment annotations. We instantiate the model to utilize the document-level sentiment polarity annotations present in many online documents (e.g. star ratings). We evaluate the model using small, widely used sentiment and subjectivity corpora and find it out-performs several previously introduced methods for sentiment classification. We also introduce a large dataset of movie reviews to serve as a more robust benchmark for work in this area.</p>"
            },
            "media": {
                "url": "https://www.semanticscholar.org/paper/Learning-Word-Vectors-for-Sentiment-Analysis-Maas-Daly/1c61f9ef06fe74505775a833ff849185757199e7",
                "caption": "Learning Word Vectors for Sentiment Analysis",
                "credit": "Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, A. Ng, Christopher Potts"
            }
        },
        {
            "start_date": {
                "year": 2011,
                "month": 6,
                "day": 19
            },
            "text": {
                "headline": "Cross-Lingual POS Tagger",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://aclanthology.org/P11-1061/",
                "caption": "Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections",
                "credit": "Dipanjan Das, Slav Petrov"
            }
        },
        {
            "start_date": {
                "year": 2011,
                "month": 5,
                "day": 22
            },
            "text": {
                "headline": "RNN-SpeedUp",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://ieeexplore.ieee.org/document/5947611",
                "caption": "Extensions of recurrent neural network language model",
                "credit": "T. Mikolov, S. Kombrink, L. Burget, J. Cernock ˇ y, and S. Khudanpur"
            }
        },
        {
            "start_date": {
                "year": 2011,
                "month": 4,
                "day": 29
            },
            "text": {
                "headline": "Deep Autoencoders",
                "text": "<p>We show how to learn many layers of features on color images and we use these features to initialize deep autoencoders. We then use the autoencoders to map images to short binary codes. Using semantic hashing [6], 28-bit codes can be used to retrieve images that are similar to a query image in a time that is independent of the size of the database. This extremely fast retrieval makes it possible to search using multiple di erent transformations of the query image. 256-bit binary codes allow much more accurate matching and can be used to prune the set of images found using the 28-bit codes.</p>"
            },
            "media": {
                "url": "https://www.semanticscholar.org/paper/Using-very-deep-autoencoders-for-content-based-Krizhevsky-Hinton/88080d28536f36588740737f3b7a1f6c1a409654",
                "caption": "Using very deep autoencoders for content-based image retrieval",
                "credit": "A. Krizhevsky, Geoffrey E. Hinton"
            }
        },
        {
            "start_date": {
                "year": 2011,
                "month": 4,
                "day": 13
            },
            "text": {
                "headline": "Deep rectifier networks",
                "text": "<p>While logistic sigmoid neurons are more biologically plausible than hyperbolic tangent neurons, the latter work better for training multi-layer neural networks. This paper shows that rectifying neurons are an even better model of biological neurons and yield equal or better performance than hyperbolic tangent networks in spite of the hard non-linearity and non-differentiability at zero, creating sparse representations with true zeros which seem remarkably suitable for naturally sparse data. Even though they can take advantage of semi-supervised setups with extra-unlabeled data, deep rectifier networks can reach their best performance without requiring any unsupervised pre-training on purely supervised tasks with large labeled datasets. Hence, these results can be seen as a new milestone in the attempts at understanding the difficulty in training deep but purely supervised neural networks, and closing the performance gap between neural networks learnt with and without unsupervised pre-training.</p>"
            },
            "media": {
                "url": "https://proceedings.mlr.press/v15/glorot11a.html",
                "caption": "Deep sparse rectifier neural networks",
                "credit": "Xavier Glorot, Antoine Bordes, Yoshua Bengio"
            }
        },
        {
            "start_date": {
                "year": 2011,
                "month": 4,
                "day": 11
            },
            "text": {
                "headline": "Optimized Single-layer Net",
                "text": "<p></p>"
            },
            "media": {
                "url": "http://proceedings.mlr.press/v15/coates11a.html",
                "caption": "An analysis of single-layer networks in unsupervised feature learning",
                "credit": "A Coates, A Ng, H Lee"
            }
        },
        {
            "start_date": {
                "year": 2010,
                "month": 9,
                "day": 26
            },
            "text": {
                "headline": "YouTube Video Recommendation System",
                "text": "<p>We discuss the video recommendation system in use at YouTube, the world's most popular online video community. The system recommends personalized sets of videos to users based on their activity on the site. We discuss some of the unique challenges that the system faces and how we address them. In addition, we provide details on the experimentation and evaluation framework used to test and tune new algorithms. We also present some of the findings from these experiments.</p>"
            },
            "media": {
                "url": "https://dl.acm.org/doi/10.1145/1864708.1864770",
                "caption": "The YouTube Video Recommendation System",
                "credit": "J Davidson, B Liebald, J Liu, P Nandy"
            }
        },
        {
            "start_date": {
                "year": 2010,
                "month": 9,
                "day": 26
            },
            "text": {
                "headline": "RNN LM",
                "text": "<p>A new recurrent neural network based language model (RNN LM) with applications to speech recognition is presented. Results indicate that it is possible to obtain around 50% reduction of perplexity by using mixture of several RNN LMs, compared to a state of the art backoff language model. Speech recognition experiments show around 18% reduction of word error rate on the Wall Street Journal task when comparing models trained on the same amount of data, and around 5% on the much harder NIST RT05 task, even when the backoff model is trained on much more data than the RNN LM. We provide ample empirical evidence to suggest that connectionist language models are superior to standard n-gram techniques, except their high computational (training) complexity. Index Terms: language modeling, recurrent neural networks, speech recognition</p>"
            },
            "media": {
                "url": "https://www.semanticscholar.org/paper/Recurrent-neural-network-based-language-model-Mikolov-Karafi%C3%A1t/9819b600a828a57e1cde047bbe710d3446b30da5",
                "caption": "Recurrent neural network based language model",
                "credit": "Tomas Mikolov, M. Karafiát, L. Burget, J. Černocký, S. Khudanpur"
            }
        },
        {
            "start_date": {
                "year": 2010,
                "month": 9,
                "day": 5
            },
            "text": {
                "headline": "Fisher-Boost",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://link.springer.com/chapter/10.1007/978-3-642-15561-1_11",
                "caption": "Improving the Fisher Kernel for Large-Scale Image Classification",
                "credit": "Florent PerronninJorge SánchezThomas Mensink"
            }
        },
        {
            "start_date": {
                "year": 2010,
                "month": 6,
                "day": 15
            },
            "text": {
                "headline": "ReLU (NORB)",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://dl.acm.org/doi/10.5555/3104322.3104425",
                "caption": "Rectified linear units improve restricted boltzmann machines",
                "credit": "Nair, V., Hinton, G. E."
            }
        },
        {
            "start_date": {
                "year": 2010,
                "month": 6,
                "day": 15
            },
            "text": {
                "headline": "ReLU (LFW)",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://dl.acm.org/doi/10.5555/3104322.3104425",
                "caption": "Rectified linear units improve restricted boltzmann machines",
                "credit": "Nair, V., Hinton, G. E."
            }
        },
        {
            "start_date": {
                "year": 2010,
                "month": 6,
                "day": 13
            },
            "text": {
                "headline": "Mid-level Features",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://ieeexplore.ieee.org/document/5539963",
                "caption": "Learning mid-level features for recognition",
                "credit": "YL Boureau, F Bach, Y LeCun, J Ponce"
            }
        },
        {
            "start_date": {
                "year": 2010,
                "month": 6,
                "day": 13
            },
            "text": {
                "headline": "Deconvolutional Network",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://ieeexplore.ieee.org/document/5539957",
                "caption": "Deconvolutional Networks",
                "credit": "Matthew D. Zeiler, Dilip Krishnan, Graham W. Taylor and Rob Fergus"
            }
        },
        {
            "start_date": {
                "year": 2010,
                "month": 6,
                "day": 1
            },
            "text": {
                "headline": "Word Representations",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://aclanthology.org/P10-1040.pdf",
                "caption": "Word Representations: A Simple and General Method for Semi-Supervised Learning",
                "credit": "Joseph Turian, Lev-Arie Ratinov, Yoshua Bengio"
            }
        },
        {
            "start_date": {
                "year": 2010,
                "month": 5,
                "day": 13
            },
            "text": {
                "headline": "Feedforward NN",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://proceedings.mlr.press/v9/glorot10a.html",
                "caption": "Understanding the difficulty of training deep feedforward neural networks",
                "credit": "X Glorot, Y Bengio"
            }
        },
        {
            "start_date": {
                "year": 2010,
                "month": 3,
                "day": 1
            },
            "text": {
                "headline": "6-layer MLP (MNIST)",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/1003.0358",
                "caption": "Deep Big Simple Neural Nets Excel on Handwritten Digit Recognition",
                "credit": "Dan Claudiu Ciresan, Ueli Meier, Luca Maria Gambardella, Juergen Schmidhuber"
            }
        },
        {
            "start_date": {
                "year": 2010,
                "month": 1,
                "day": 3
            },
            "text": {
                "headline": "Stacked Denoising Autoencoders",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://www.jmlr.org/papers/v11/vincent10a.html",
                "caption": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion",
                "credit": "P Vincent, H Larochelle, I Lajoie, Y Bengio"
            }
        },
        {
            "start_date": {
                "year": 2010,
                "month": 1,
                "day": 1
            },
            "text": {
                "headline": "Super-vector coding",
                "text": "<p></p>"
            },
            "media": {
                "url": "http://tongzhang-ml.org/papers/eccv10_supervect.pdf",
                "caption": "Image Classification using Super-Vector Coding of Local Image Descriptors",
                "credit": "Xi Zhou, Kai Yu, Tong Zhang, and Thomas S. Huang"
            }
        },
        {
            "start_date": {
                "year": 2009,
                "month": 9,
                "day": 29
            },
            "text": {
                "headline": "3D city reconstruction",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://grail.cs.washington.edu/rome/",
                "caption": "Building Rome in a Day",
                "credit": "Sameer Agarwal, Noah Snavely, Ian Simon, Steven M. Seitz and Richard Szeliski"
            }
        },
        {
            "start_date": {
                "year": 2009,
                "month": 9,
                "day": 21
            },
            "text": {
                "headline": "BellKor 2007",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://www.semanticscholar.org/paper/The-BellKor-solution-to-the-Netflix-Prize-Bell-Koren/f4ebb542c752a0dc423f94fd121e2edb8f6275ba",
                "caption": "The BellKor solution to the Netflix Prize",
                "credit": "RM Bell, Y Koren, C Volinsky"
            }
        },
        {
            "start_date": {
                "year": 2009,
                "month": 8,
                "day": 7
            },
            "text": {
                "headline": "MatrixFac for Recommenders",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://ieeexplore.ieee.org/document/5197422",
                "caption": "Matrix factorization techniques for recommender systems",
                "credit": "Yehuda Koren, Robert Bell, and Chris Volinsky"
            }
        },
        {
            "start_date": {
                "year": 2009,
                "month": 8,
                "day": 1
            },
            "text": {
                "headline": "Pragmatic Theory solution (Netflix 2009)",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://www.asc.ohio-state.edu/statistics/statgen/joul_aut2009/PragmaticTheory.pdf",
                "caption": "The Pragmatic Theory solution to the Netflix Grand Prize",
                "credit": "M Piotte, M Chabbert"
            }
        },
        {
            "start_date": {
                "year": 2009,
                "month": 8,
                "day": 1
            },
            "text": {
                "headline": "BigChaos OptiBlend",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://www.asc.ohio-state.edu/statistics/statgen/joul_aut2009/BigChaos.pdf",
                "caption": "The BigChaos Solution to the Netflix Grand Prize",
                "credit": "A Töscher, M Jahrer, RM Bell"
            }
        },
        {
            "start_date": {
                "year": 2009,
                "month": 8,
                "day": 1
            },
            "text": {
                "headline": "BellKor 2009",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://www2.seas.gwu.edu/~simhaweb/champalg/cf/papers/KorenBellKor2009.pdf",
                "caption": "The BellKor Solution to the Netflix Grand Prize",
                "credit": "Y Koren"
            }
        },
        {
            "start_date": {
                "year": 2009,
                "month": 8,
                "day": 1
            },
            "text": {
                "headline": "BellKor 2008",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://cseweb.ucsd.edu/classes/fa17/cse291-b/reading/ProgressPrize2008_BellKor.pdf",
                "caption": "The BellKor 2008 Solution to the Netflix Prize",
                "credit": "RM Bell, Y Koren, C Volinsky"
            }
        },
        {
            "start_date": {
                "year": 2009,
                "month": 6,
                "day": 15
            },
            "text": {
                "headline": "GPU DBNs",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://dl.acm.org/doi/abs/10.1145/1553374.1553486?utm_campaign=The+Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-95Z7-X4Dl-RJK6gYKvjyDIrYaGhBeqWoc0ldqyPEKni0Ip6UE7452hr-ygY52z00LBpYgM",
                "caption": "Large-scale Deep Unsupervised Learning using Graphics Processors",
                "credit": "R Raina, A Madhavan, AY Ng"
            }
        },
        {
            "start_date": {
                "year": 2009,
                "month": 6,
                "day": 14
            },
            "text": {
                "headline": "Conv-DBN",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://dl.acm.org/doi/10.1145/1553374.1553453",
                "caption": "Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations",
                "credit": "H Lee, R Grosse, R Ranganath, AY Ng"
            }
        },
        {
            "start_date": {
                "year": 2009,
                "month": 4,
                "day": 16
            },
            "text": {
                "headline": "Deep Boltzmann Machines",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://www.sciencedirect.com/topics/computer-science/deep-boltzmann-machine",
                "caption": "Deep Boltzmann Machines",
                "credit": "Ruslan Salakhutdinov, Geoffrey Hinton"
            }
        },
        {
            "start_date": {
                "year": 2009,
                "month": 4,
                "day": 8
            },
            "text": {
                "headline": "RBM Image Classifier",
                "text": "<p>Groups at MIT and NYU have collected a dataset of millions of tiny colour images from the web. It\nis, in principle, an excellent dataset for unsupervised training of deep generative models, but previous\nresearchers who have tried this have found it di\u001ecult to learn a good set of \u001clters from the images.\nWe show how to train a multi-layer generative model that learns to extract meaningful features which\nresemble those found in the human visual cortex. Using a novel parallelization algorithm to distribute\nthe work among multiple machines connected on a network, we show how training such a model can be\ndone in reasonable time.\nA second problematic aspect of the tiny images dataset is that there are no reliable class labels\nwhich makes it hard to use for object recognition experiments. We created two sets of reliable labels.\nThe CIFAR-10 set has 6000 examples of each of 10 classes and the CIFAR-100 set has 600 examples of\neach of 100 non-overlapping classes. Using these labels, we show that object recognition is signi\u001ccantly\nimproved by pre-training a layer of features on a large set of unlabeled tiny images.</p>"
            },
            "media": {
                "url": "https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf",
                "caption": "Learning Multiple Layers of Features from Tiny Images",
                "credit": "Alex Krizhevsky"
            }
        },
        {
            "start_date": {
                "year": 2008,
                "month": 12,
                "day": 10
            },
            "text": {
                "headline": "Semantic Hashing",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://www.cs.cmu.edu/~rsalakhu/papers/sdarticle.pdf",
                "caption": "Semantic Hashing",
                "credit": "R Salakhutdinov, G Hinton"
            }
        },
        {
            "start_date": {
                "year": 2008,
                "month": 12,
                "day": 8
            },
            "text": {
                "headline": "HLBL",
                "text": "<p>Neural probabilistic language models (NPLMs) have been shown to be competitive with and occasionally superior to the widely-used n-gram language models. The main drawback of NPLMs is their extremely long training and testing times. Morin and Bengio have proposed a hierarchical language model built around a binary tree of words, which was two orders of magnitude faster than the non-hierarchical model it was based on. However, it performed considerably worse than its non-hierarchical counterpart in spite of using a word tree created using expert knowledge. We introduce a fast hierarchical language model along with a simple feature-based algorithm for automatic construction of word trees from the data. We then show that the resulting models can outperform non-hierarchical neural models as well as the best n-gram models.</p>"
            },
            "media": {
                "url": "https://www.semanticscholar.org/paper/A-Scalable-Hierarchical-Distributed-Language-Model-Mnih-Hinton/a9fc84f8abe740cdc7ee82e69444d1d00dbe0ceb",
                "caption": "A Scalable Hierarchical Distributed Language Model",
                "credit": "A. Mnih, Geoffrey E. Hinton"
            }
        },
        {
            "start_date": {
                "year": 2008,
                "month": 12,
                "day": 8
            },
            "text": {
                "headline": "ADAPTIVE NLPM",
                "text": "<p>Neural probabilistic language models (NPLMs) have been shown to be competitive with and occasionally superior to the widely-used n-gram language models. The main drawback of NPLMs is their extremely long training and testing times. Morin and Bengio have proposed a hierarchical language model built around a binary tree of words, which was two orders of magnitude faster than the nonhierarchical model it was based on. However, it performed considerably worse than its non-hierarchical counterpart in spite of using a word tree created using expert knowledge. We introduce a fast hierarchical language model along with a simple feature-based algorithm for automatic construction of word trees from the data. We then show that the resulting models can outperform non-hierarchical neural models as well as the best n-gram models.</p>"
            },
            "media": {
                "url": "https://papers.nips.cc/paper_files/paper/2008/file/1e056d2b0ebd5c878c550da6ac5d3724-Paper.pdf",
                "caption": "A Scalable Hierarchical Distributed Language Model",
                "credit": "Andriy Mnih, Geoffrey Hinton"
            }
        },
        {
            "start_date": {
                "year": 2008,
                "month": 11,
                "day": 25
            },
            "text": {
                "headline": "BigChaos 2008",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://www.asc.ohio-state.edu/statistics/statgen/joul_aut2009/BigChaos.pdf",
                "caption": "The BigChaos Solution to the Netflix Prize 2008",
                "credit": "A Töscher, M Jahrer"
            }
        },
        {
            "start_date": {
                "year": 2008,
                "month": 11,
                "day": 19
            },
            "text": {
                "headline": "Sparse digit recognition SVM",
                "text": "<p>In this brief paper, we propose a method of feature extraction for digit recognition that is inspired by vision research: a sparse-coding strategy and a local maximum operation. We show that our method, despite its simplicity, yields state-of-the-art classification results on a highly competitive digit-recognition benchmark. We first employ the unsupervised Sparsenet algorithm to learn a basis for representing patches of handwritten digit images. We then use this basis to extract local coefficients. In a second step, we apply a local maximum operation to implement local shift invariance. Finally, we train a support vector machine (SVM) on the resulting feature vectors and obtain state-of-the-art classification performance in the digit recognition task defined by the MNIST benchmark. We compare the different classification performances obtained with sparse coding, Gabor wavelets, and principal component analysis (PCA). We conclude that the learning of a sparse representation of local image patches combined with a local maximum operation for feature extraction can significantly improve recognition performance.</p>"
            },
            "media": {
                "url": "https://pubmed.ncbi.nlm.nih.gov/19000969/",
                "caption": "Simple method for high-performance digit recognition based on sparse coding",
                "credit": "Kai Labusch, Erhadt Barth, Thomas Martinetz"
            }
        },
        {
            "start_date": {
                "year": 2008,
                "month": 7,
                "day": 23
            },
            "text": {
                "headline": "Boss (DARPA Urban Challenge)",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://onlinelibrary.wiley.com/doi/abs/10.1002/rob.20255",
                "caption": "Autonomous driving in urban environments: Boss and the Urban Challenge",
                "credit": "Chris Urmson, Joshua Anhalt, Drew Bagnell,Christopher Baker, Robert Bittner,M. N. Clark, John Dolan, Dave Duggins,Tugrul Galatali, Chris Geyer,Michele Gittleman, Sam Harbaugh,Martial Hebert, Thomas M. Howard,Sascha Kolski, Alonzo Kelly,Maxim Likhachev, Matt McNaughton,Nick Miller, Kevin Peterson, Brian Pilnick,Raj Rajkumar, Paul Rybski, Bryan Salesky,Young-Woo Seo, Sanjiv Singh, Jarrod Snider,Anthony Stentz, William “Red” Whittaker,Ziv Wolkowicki, and Jason Ziglar"
            }
        },
        {
            "start_date": {
                "year": 2008,
                "month": 7,
                "day": 5
            },
            "text": {
                "headline": "Semi-Supervised Embedding for DL",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://dl.acm.org/doi/10.1145/1390156.1390303",
                "caption": "Deep Learning via Semi-Supervised Embedding",
                "credit": "Jason Weston, Frederick, Ratle, Hossein Mobahi, Ronan Collobert"
            }
        },
        {
            "start_date": {
                "year": 2008,
                "month": 7,
                "day": 5
            },
            "text": {
                "headline": "Denoising Autoencoders",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://dl.acm.org/doi/10.1145/1390156.1390294",
                "caption": "Extracting and Composing Robust Features with Denoising Autoencoders",
                "credit": "Pascal Vincent, Hugo Larechelle, Yoshua Bengio, Pierre- Antoine Manzagol"
            }
        },
        {
            "start_date": {
                "year": 2008,
                "month": 7,
                "day": 5
            },
            "text": {
                "headline": "Deep Multitask NLP Network",
                "text": "<p>We describe a single convolutional neural network architecture that, given a sentence, outputs a host of language processing predictions: part-of-speech tags, chunks, named entity tags, semantic roles, semantically similar words and the likelihood that the sentence makes sense (grammatically and semantically) using a language model. The entire network is trained jointly on all these tasks using weight-sharing, an instance of multitask learning. All the tasks use labeled data except the language model which is learnt from unlabeled text and represents a novel form of semi-supervised learning for the shared tasks. We show how both multitask learning and semi-supervised learning improve the generalization of the shared tasks, resulting in stateof-the-art performance.</p>"
            },
            "media": {
                "url": "http://icml2008.cs.helsinki.fi/papers/391.pdf",
                "caption": "A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning\n",
                "credit": "Ronan Collobert, Jason Weston"
            }
        },
        {
            "start_date": {
                "year": 2008,
                "month": 6,
                "day": 23
            },
            "text": {
                "headline": "Multiscale deformable part model",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://ieeexplore.ieee.org/abstract/document/4587597",
                "caption": "A discriminatively trained, multiscale, deformable part model",
                "credit": "Pedro Felzenszwalb, David McAllester, Deva Ramanan"
            }
        },
        {
            "start_date": {
                "year": 2007,
                "month": 12,
                "day": 3
            },
            "text": {
                "headline": "BLSTM for handwriting (2)",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://proceedings.neurips.cc/paper/2007/hash/4b0250793549726d5c1ea3906726ebfe-Abstract.html",
                "caption": "Unconstrained online handwriting recognition with recurrent neural networks",
                "credit": "Alex Graves, Marcus Liwicki, Horst Bunke, Jürgen Schmidhuber, Santiago Fernández"
            }
        },
        {
            "start_date": {
                "year": 2007,
                "month": 10,
                "day": 28
            },
            "text": {
                "headline": "Enhanced Neighborhood-Based Filtering",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://ieeexplore.ieee.org/abstract/document/4470228",
                "caption": "Scalable Collaborative Filtering with Jointly Derived Neighborhood Interpolation Weights",
                "credit": "RM Bell, Y Koren"
            }
        },
        {
            "start_date": {
                "year": 2007,
                "month": 9,
                "day": 23
            },
            "text": {
                "headline": "BLSTM for handwriting (1)",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://people.idsia.ch//~juergen/icdar_2007.pdf",
                "caption": "A Novel Approach to On-Line Handwriting Recognition Based on Bidirectional Long Short-Term Memory Networks",
                "credit": "M Liwicki, A Graves, S Fernàndez"
            }
        },
        {
            "start_date": {
                "year": 2007,
                "month": 8,
                "day": 12
            },
            "text": {
                "headline": "Regularized SVD for Collaborative Filtering",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://www.semanticscholar.org/paper/Improving-regularized-singular-value-decomposition-Paterek/f732d0f69fe4e84a95c32706b28b9e4ef1753c61",
                "caption": "Improving regularized singular value decomposition for collaborative filtering",
                "credit": "A Paterek"
            }
        },
        {
            "start_date": {
                "year": 2007,
                "month": 7,
                "day": 16
            },
            "text": {
                "headline": "Fisher Kernel GMM",
                "text": "<p>Within the field of pattern classification, the Fisher kernel is a powerful framework which combines the strengths of generative and discriminative approaches. The idea is to\ncharacterize a signal with a gradient vector derived from a\ngenerative probability model and to subsequently feed this\nrepresentation to a discriminative classifier. We propose to\napply this framework to image categorization where the input signals are images and where the underlying generative\nmodel is a visual vocabulary: a Gaussian mixture model\nwhich approximates the distribution of low-level features in\nimages. We show that Fisher kernels can actually be understood as an extension of the popular bag-of-visterms. Our approach demonstrates excellent performance on two challenging databases: an in-house database of 19 object/scene categories and the recently released VOC 2006 database. It is also very practical: it has low computational needs both at training and test time and vocabularies trained on one set of categories can be applied to another set without any significant loss in performance.</p>"
            },
            "media": {
                "url": "https://ieeexplore.ieee.org/document/4270291",
                "caption": "Fisher kernels on visual vocabularies for image categorization",
                "credit": "Florent Perronnin, Christopher Dance"
            }
        },
        {
            "start_date": {
                "year": 2007,
                "month": 6,
                "day": 22
            },
            "text": {
                "headline": "SB-LM",
                "text": "<p>Systems, methods, and computer program products for machine translation are provided. In some implementations a system is provided. The system includes a language model including a collection of n-grams from a corpus, each n-gram having a corresponding relative frequency in the corpus and an order n corresponding to a number of tokens in the n-gram, each n-gram corresponding to a backoff n-gram having an order of n-1 and a collection of backoff scores, each backoff score associated with an n-gram, the backoff score determined as a function of a backoff factor and a relative frequency of a corresponding backoff n-gram in the corpus.</p>"
            },
            "media": {
                "url": "https://www.semanticscholar.org/paper/Large-Language-Models-in-Machine-Translation-Brants-Popat/ba786c46373892554b98df42df7af6f5da343c9d",
                "caption": "Large Language Models in Machine Translation",
                "credit": "T. Brants, Ashok Popat, P. Xu, F. Och, J. Dean"
            }
        },
        {
            "start_date": {
                "year": 2007,
                "month": 6,
                "day": 22
            },
            "text": {
                "headline": "KN-LM",
                "text": "<p>Systems, methods, and computer program products for machine translation are provided. In some implementations a system is provided. The system includes a language model including a collection of n-grams from a corpus, each n-gram having a corresponding relative frequency in the corpus and an order n corresponding to a number of tokens in the n-gram, each n-gram corresponding to a backoff n-gram having an order of n-1 and a collection of backoff scores, each backoff score associated with an n-gram, the backoff score determined as a function of a backoff factor and a relative frequency of a corresponding backoff n-gram in the corpus.</p>"
            },
            "media": {
                "url": "https://www.semanticscholar.org/paper/Large-Language-Models-in-Machine-Translation-Brants-Popat/ba786c46373892554b98df42df7af6f5da343c9d",
                "caption": "Large Language Models in Machine Translation",
                "credit": "T. Brants, Ashok Popat, P. Xu, F. Och, J. Dean"
            }
        },
        {
            "start_date": {
                "year": 2007,
                "month": 6,
                "day": 20
            },
            "text": {
                "headline": "Restricted Bolzmann machines",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://dl.acm.org/doi/abs/10.1145/1273496.1273596?casa_token=cfdkH2x12MwAAAAA:sEUzfllIGyPcOfzgUoDPHlpC1ukfCAo8ewocBXWBswIIF9eS5HdFo30nOtfmIV8gm-XpBpQJJ5zYVO8",
                "caption": "Restricted Boltzmann machines for collaborative filtering",
                "credit": "Russ Salukhutdinov, Andriy Mnih, GE Hinton"
            }
        },
        {
            "start_date": {
                "year": 2007,
                "month": 6,
                "day": 1
            },
            "text": {
                "headline": "λ-WASP",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://www.aclweb.org/anthology/P07-1121/",
                "caption": "Learning Synchronous Grammars for Semantic Parsing with Lambda Calculus",
                "credit": "YW Wong, R Mooney"
            }
        },
        {
            "start_date": {
                "year": 2007,
                "month": 6,
                "day": 1
            },
            "text": {
                "headline": "Empirical evaluation of deep architectures",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://dl.acm.org/doi/10.1145/1273496.1273556",
                "caption": "An empirical evaluation of deep architectures on problems with many factors of variation",
                "credit": "Hugo Larechelle, Dumithru Erhan, Aaron C Courville, James Bergsta, Yoshua Bengio"
            }
        },
        {
            "start_date": {
                "year": 2006,
                "month": 12,
                "day": 4
            },
            "text": {
                "headline": "Sparse Energy-Based Model",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://papers.nips.cc/paper/2006/hash/87f4d79e36d68c3031ccf6c55e9bbd39-Abstract.html",
                "caption": "Efficient Learning of Sparse Representations with an Energy-Based Model",
                "credit": "M Ranzato, C Poultney, S Chopra, Y Cun"
            }
        },
        {
            "start_date": {
                "year": 2006,
                "month": 12,
                "day": 4
            },
            "text": {
                "headline": "Greedy layer-wise DNN training",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://dl.acm.org/doi/10.5555/2976456.2976476",
                "caption": "Greedy layer-wise training of deep networks",
                "credit": "Y Bengio, P Lamblin, D Popovici"
            }
        },
        {
            "start_date": {
                "year": 2006,
                "month": 12,
                "day": 1
            },
            "text": {
                "headline": "Local Binary Patterns for facial recognition",
                "text": "<p></p>"
            },
            "media": {
                "url": "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.456.1094&rep=rep1&type=pdf",
                "caption": "Face Description with Local Binary Patterns: Application to Face Recognition",
                "credit": "Timo Ahonen, Abdenour Hadid, and Matti Pietikainen"
            }
        },
        {
            "start_date": {
                "year": 2006,
                "month": 11,
                "day": 1
            },
            "text": {
                "headline": "Sparse Vision Encoding",
                "text": "<p>Sparse coding provides a class of algorithms for finding succinct representations of stimuli; given only unlabeled input data, it discovers basis functions that capture higher-level features in the data. However, finding sparse codes remains a very difficult computational problem. In this paper, we present efficient sparse coding algorithms that are based on iteratively solving two convex optimization problems: an L1-regularized least squares problem and an L2-constrained least squares problem. We propose novel algorithms to solve both of these optimization problems. Our algorithms result in a significant speedup for sparse coding, allowing us to learn larger sparse codes than possible with previously described algorithms. We apply these algorithms to natural images and demonstrate that the inferred sparse codes exhibit end-stopping and non-classical receptive field surround suppression and, therefore, may provide a partial explanation for these two phenomena in V1 neurons.</p>"
            },
            "media": {
                "url": "https://proceedings.neurips.cc/paper_files/paper/2006/file/2d71b2ae158c7c5912cc0bbde2bb9d95-Paper.pdf",
                "caption": "Efficient sparse coding algorithms",
                "credit": "Honglak Lee, Alexis Battle, Rajat Raina, Andrew Y. Ng"
            }
        },
        {
            "start_date": {
                "year": 2006,
                "month": 7,
                "day": 18
            },
            "text": {
                "headline": "Dimensionality Reduction",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://www.cs.toronto.edu/~hinton/science.pdf",
                "caption": "Reducing the dimensionality of data with neural networks.",
                "credit": "GE Hinton, RR Salakhutdinov"
            }
        },
        {
            "start_date": {
                "year": 2006,
                "month": 7,
                "day": 18
            },
            "text": {
                "headline": "Deep Belief Nets",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://www.cs.toronto.edu/~hinton/absps/fastnc.pdf",
                "caption": "A fast learning algorithm for deep belief nets",
                "credit": "GE Hinton, S Osindero, YW Teh"
            }
        },
        {
            "start_date": {
                "year": 2006,
                "month": 6,
                "day": 25
            },
            "text": {
                "headline": "CTC-Trained LSTM",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://www.cs.toronto.edu/~graves/icml_2006.pdf",
                "caption": "Connectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks",
                "credit": "Alex Graves, Santiago Fernández, Faustino Gómez, Jürgen Schmidhuber"
            }
        },
        {
            "start_date": {
                "year": 2006,
                "month": 6,
                "day": 17
            },
            "text": {
                "headline": "Spatial Pyramid Matching",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://inc.ucsd.edu/mplab/users/marni/Igert/Lazebnik_06.pdf",
                "caption": "Beyond Bags of Features: Spatial Pyramid Matching for Recognizing Natural Scene Categories",
                "credit": "S Lazebnik, C Schmid, J Ponce"
            }
        },
        {
            "start_date": {
                "year": 2006,
                "month": 6,
                "day": 17
            },
            "text": {
                "headline": "DrLIM",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://ieeexplore.ieee.org/document/1640964",
                "caption": "Dimensionality Reduction by Learning an Invariant Mapping",
                "credit": "R. Hadsell; S. Chopra; Y. LeCun"
            }
        },
        {
            "start_date": {
                "year": 2006,
                "month": 5,
                "day": 7
            },
            "text": {
                "headline": "FAST",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://link.springer.com/chapter/10.1007/11744023_34",
                "caption": "Machine Learning for High-Speed Corner Detection",
                "credit": "Edward Rosten and Tom Drummond"
            }
        },
        {
            "start_date": {
                "year": 2006,
                "month": 2,
                "day": 2
            },
            "text": {
                "headline": "TFE SVM",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://hal.archives-ouvertes.fr/hal-00018426/en",
                "caption": "A trainable feature extractor for handwritten digit recognition",
                "credit": "Fabian Lauer, Ching Y Suen, Gerard Bloch"
            }
        },
        {
            "start_date": {
                "year": 2006,
                "month": 1,
                "day": 1
            },
            "text": {
                "headline": "Stanley (DARPA Grand Challenge 2)",
                "text": "<p></p>"
            },
            "media": {
                "url": "http://robots.stanford.edu/papers/thrun.stanley05.pdf",
                "caption": "Stanley: The Robot that Won the DARPA Grand Challenge",
                "credit": "S Thrun, M Montemerlo, H Dahlkamp"
            }
        },
        {
            "start_date": {
                "year": 2005,
                "month": 12,
                "day": 5
            },
            "text": {
                "headline": "Monocular Depth Prediction",
                "text": "<p>We consider the task of depth estimation from a single monocular image. We take a supervised learning approach to this problem, in which we begin by collecting a training set of monocular images (of unstructured outdoor environments which include forests, trees, buildings, etc.) and their corresponding ground-truth depthmaps. Then, we apply supervised learning to predict the depthmap as a function of the image. Depth estimation is a challenging problem, since local features alone are insufficient to estimate depth at a point, and one needs to consider the global context of the image. Our model uses a discriminatively-trained Markov Random Field (MRF) that incorporates multiscale local- and global-image features, and models both depths at individual points as well as the relation between depths at different points. We show that, even on unstructured scenes, our algorithm is frequently able to recover fairly accurate depthmaps.</p>"
            },
            "media": {
                "url": "https://papers.nips.cc/paper_files/paper/2005/file/17d8da815fa21c57af9829fb0a869602-Paper.pdf",
                "caption": "Learning Depth from Single Monocular Images",
                "credit": "Ashutosh Saxena, Sung H. Chung, and Andrew Y. Ng"
            }
        },
        {
            "start_date": {
                "year": 2005,
                "month": 8,
                "day": 7
            },
            "text": {
                "headline": "RankNet",
                "text": "<p>We investigate using gradient descent methods for learning ranking functions; we propose a simple probabilistic cost function, and we introduce RankNet, an implementation of these ideas using a neural network to model the underlying ranking function. We present test results on toy data and on data from a commercial internet search engine.</p>"
            },
            "media": {
                "url": "https://dl.acm.org/doi/abs/10.1145/1102351.1102363",
                "caption": "Learning to rank using gradient descent",
                "credit": "Chris Burges, Tal Shaked, Erin Renshaw, Ari Lazier, Matt Deeds, Nicole Hamilton, Greg HullenderAuthors Info & Claims"
            }
        },
        {
            "start_date": {
                "year": 2005,
                "month": 8,
                "day": 1
            },
            "text": {
                "headline": "BiLSTM for Speech",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://www.sciencedirect.com/science/article/abs/pii/S0893608005001206",
                "caption": "Framewise phoneme classification with bidirectional LSTM and other neural network architectures",
                "credit": "A Graves, J Schmidhuber"
            }
        },
        {
            "start_date": {
                "year": 2005,
                "month": 6,
                "day": 25
            },
            "text": {
                "headline": "Histograms of Oriented Gradients",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://ieeexplore.ieee.org/document/1467360",
                "caption": "Histograms of oriented gradients for human detection",
                "credit": "N Dalal, B Triggs"
            }
        },
        {
            "start_date": {
                "year": 2005,
                "month": 6,
                "day": 20
            },
            "text": {
                "headline": "ConvNet similarity metric",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://ieeexplore.ieee.org/document/1467314",
                "caption": "Learning a similarity metric discriminatively, with application to face verification",
                "credit": "S Chopra, R Hadsell, Y LeCun"
            }
        },
        {
            "start_date": {
                "year": 2005,
                "month": 6,
                "day": 1
            },
            "text": {
                "headline": "Hiero",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://aclanthology.org/P05-1033/",
                "caption": "A Hierarchical Phrase-Based Model for Statistical Machine Translation",
                "credit": "David Chiang"
            }
        },
        {
            "start_date": {
                "year": 2005,
                "month": 4,
                "day": 22
            },
            "text": {
                "headline": "SACHS",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://science.sciencemag.org/content/308/5721/523.long",
                "caption": "Causal Protein-Signaling Networks Derived from Multiparameter Single-Cell Data.",
                "credit": "K. Sachs, O. Perez, D. Pe'er, D. A. Lauffenburger and G. P. Nolan"
            }
        },
        {
            "start_date": {
                "year": 2005,
                "month": 1,
                "day": 6
            },
            "text": {
                "headline": "Hierarchical LM",
                "text": "<p>In recent years, variants of a neural network architecture for statistical language modeling have been proposed and successfully applied, e.g. in the language modeling component of speech recognizers. The main advantage of these architectures is that they learn an embedding for words (or other symbols) in a continuous space that helps to smooth the language model and provide good generalization even when the number of training examples is insufficient. However, these models are extremely slow in comparison to the more commonly used n-gram models, both for training and recognition. As an alternative to an importance sampling method proposed to speed-up training, we introduce a hierarchical decomposition of the conditional probabilities that yields a speed-up of about 200 both during training and recognition. The hierarchical decomposition is a binary hierarchical clustering constrained by the prior knowledge extracted from the WordNet semantic hierarchy.</p>"
            },
            "media": {
                "url": "https://www.semanticscholar.org/paper/Hierarchical-Probabilistic-Neural-Network-Language-Morin-Bengio/c19fbefdeead6a4154a22a9c8551a18b1530033a",
                "caption": "Hierarchical Probabilistic Neural Network Language Model",
                "credit": "Frederic Morin, Yoshua Bengio"
            }
        },
        {
            "start_date": {
                "year": 2004,
                "month": 12,
                "day": 1
            },
            "text": {
                "headline": "LMICA",
                "text": "<p>In this paper, linear multilayer ICA (LMICA) is proposed for extracting independent components from quite high-dimensional observed signals such as large-size natural scenes. There are two phases in each layer of LMICA. One is the mapping phase, where a one-dimensional mapping is formed by a stochastic gradient algorithm which makes more highly-correlated (non-independent) signals be nearer incrementally. Another is the local-ICA phase, where each neighbor (namely, highly-correlated) pair of signals in the mapping is separated by the MaxKurt algorithm. Because LMICA separates only the highly-correlated pairs instead of all ones, it can extract independent components quite efficiently from appropriate observed signals. In addition, it is proved that LMICA always converges. Some numerical experiments verify that LMICA is quite efficient and effective in large-size natural image processing.</p>"
            },
            "media": {
                "url": "https://www.semanticscholar.org/paper/Linear-Multilayer-Independent-Component-Analysis-Matsuda-Yamaguchi/7061b01572fbff2e223ce3abb59f397895b1ebf1",
                "caption": "Linear Multilayer Independent Component Analysis for Large Natural Scenes\n",
                "credit": "Yoshitatsu Matsuda, K. Yamaguchi"
            }
        },
        {
            "start_date": {
                "year": 2004,
                "month": 6,
                "day": 27
            },
            "text": {
                "headline": "Invariant CNN",
                "text": "<p>We assess the applicability of several popular learning methods for the problem of recognizing generic visual categories with invariance to pose, lighting, and surrounding clutter. A large dataset comprising stereo image pairs of 50 uniform-colored toys under 36 azimuths, 9 elevations, and 6 lighting conditions was collected (for a total of 194,400 individual images). The objects were 10 instances of 5 generic categories: four-legged animals, human figures, airplanes, trucks, and cars. Five instances of each category were used for training, and the other five for testing. Low-resolution grayscale images of the objects with various amounts of variability and surrounding clutter were used for training and testing. Nearest neighbor methods, support vector machines, and convolutional networks, operating on raw pixels or on PCA-derived features were tested. Test error rates for unseen object instances placed on uniform backgrounds were around 13% for SVM and 7% for convolutional nets. On a segmentation/recognition task with highly cluttered images, SVM proved impractical, while convolutional nets yielded 16/7% error. A real-time version of the system was implemented that can detect and classify objects in natural scenes at around 10 frames per second.</p>"
            },
            "media": {
                "url": "https://www.semanticscholar.org/paper/Learning-methods-for-generic-object-recognition-to-LeCun-Huang/f354310098e09c1e1dc88758fca36767fd9d084d",
                "caption": "Learning methods for generic object recognition with invariance to pose and lighting",
                "credit": "Yann LeCun, Fu Jie Huang, L. Bottou"
            }
        },
        {
            "start_date": {
                "year": 2004,
                "month": 3,
                "day": 1
            },
            "text": {
                "headline": "Max-Margin Markov Networks",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://papers.nips.cc/paper/2003/file/878d5691c824ee2aaf770f7d36c151d6-Paper.pdf",
                "caption": "Max-margin markov networks",
                "credit": "B. Taskar, C. Guestrin, and D. Koller"
            }
        },
        {
            "start_date": {
                "year": 2003,
                "month": 8,
                "day": 6
            },
            "text": {
                "headline": "CNN Best Practices",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://ieeexplore.ieee.org/document/1227801",
                "caption": "Best practices for convolutional neural networks applied to visual document analysis",
                "credit": "PY Simard, D Steinkraus, JC Platt"
            }
        },
        {
            "start_date": {
                "year": 2003,
                "month": 6,
                "day": 18
            },
            "text": {
                "headline": "Unsupervised Scale-Invariant Learning",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://ieeexplore.ieee.org/document/1211479",
                "caption": "Object Class Recognition by Unsupervised Scale-Invariant Learning",
                "credit": "R Fergus, P Perona, A Zisserman"
            }
        },
        {
            "start_date": {
                "year": 2003,
                "month": 5,
                "day": 1
            },
            "text": {
                "headline": "Phrase-based translation",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://dl.acm.org/doi/10.3115/1073445.1073462",
                "caption": "Statistical Phrase-Based Translation",
                "credit": "Philipp Koehn, Franz Josef Och, Daniel Marcu"
            }
        },
        {
            "start_date": {
                "year": 2003,
                "month": 3,
                "day": 15
            },
            "text": {
                "headline": "NPLM",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://dl.acm.org/doi/10.5555/944919.944966",
                "caption": "A Neural Probabilistic Language Model",
                "credit": "Yoshua Bengio, Réjean Ducharme, Pascal Vincent, Christian Jauvin"
            }
        },
        {
            "start_date": {
                "year": 2003,
                "month": 2,
                "day": 2
            },
            "text": {
                "headline": "LDA",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://jmlr.org/papers/volume3/blei03a/blei03a.pdf",
                "caption": "Latent Dirichlet Allocation",
                "credit": "David M. Blei, Andrew Y. Ng, Michael I. Jordan"
            }
        },
        {
            "start_date": {
                "year": 2003,
                "month": 1,
                "day": 1
            },
            "text": {
                "headline": "Statistical Shape Constellations",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://link.springer.com/content/pdf/10.1007/3-540-45054-8_2.pdf",
                "caption": "Unsupervised Learning of Models for Recognition",
                "credit": "M. Weber, M. Welling, and P. Perona"
            }
        },
        {
            "start_date": {
                "year": 2002,
                "month": 7,
                "day": 6
            },
            "text": {
                "headline": "Maximum Entropy Models for machine translation",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://aclanthology.org/P02-1038/",
                "caption": "Discriminative Training and Maximum Entropy Models for Statistical Machine Translation",
                "credit": "Franz Josef Och and Hermann Ney"
            }
        },
        {
            "start_date": {
                "year": 2002,
                "month": 6,
                "day": 1
            },
            "text": {
                "headline": "Tagging via Viterbi Decoding",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://dl.acm.org/doi/10.3115/1118693.1118694",
                "caption": "Discriminative Training Methods for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms",
                "credit": "Michael Collins"
            }
        },
        {
            "start_date": {
                "year": 2002,
                "month": 6,
                "day": 1
            },
            "text": {
                "headline": "NEAT",
                "text": "<p>An important question in neuroevolution is how to gain an advantage from evolving neural network topologies along with weights. We present a method, NeuroEvolution of Augmenting Topologies (NEAT), which outperforms the best fixed-topology method on a challenging benchmark reinforcement learning task. We claim that the increased efficiency is due to (1) employing a principled method of crossover of different topologies, (2) protecting structural innovation using speciation, and (3) incrementally growing from minimal structure. We test this claim through a series of ablation studies that demonstrate that each component is necessary to the system as a whole and to each other. What results is significantly faster learning. NEAT is also an important contribution to GAs because it shows how it is possible for evolution to both optimize and complexify solutions simultaneously, offering the possibility of evolving increasingly complex solutions over generations, and strengthening the analogy with biological evolution.</p>"
            },
            "media": {
                "url": "https://direct.mit.edu/evco/article/10/2/99/1123/Evolving-Neural-Networks-through-Augmenting",
                "caption": "Evolving Neural Networks through Augmenting Topologies ",
                "credit": "Justin Bayer, Daan Wierstra, Julian Togelius, Jürgen Schmidhuber"
            }
        },
        {
            "start_date": {
                "year": 2002,
                "month": 5,
                "day": 28
            },
            "text": {
                "headline": "Thumbs Up?",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://arxiv.org/abs/cs/0205070",
                "caption": "Thumbs up? Sentiment Classification using Machine Learning Techniques",
                "credit": "Bo Pang, Lillian Lee, Shivakumar Vaithyanathan"
            }
        },
        {
            "start_date": {
                "year": 2001,
                "month": 12,
                "day": 8
            },
            "text": {
                "headline": "Decision tree (classification)",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://www.cs.cmu.edu/~efros/courses/LBMV07/Papers/viola-cvpr-01.pdf",
                "caption": "Rapid object detection using a boosted cascade of simple features",
                "credit": "P. Viola, M. Jones"
            }
        },
        {
            "start_date": {
                "year": 2001,
                "month": 10,
                "day": 1
            },
            "text": {
                "headline": "Gradient Boosting Machine",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://projecteuclid.org/journals/annals-of-statistics/volume-29/issue-5/Greedy-function-approximation-A-gradient-boostingmachine/10.1214/aos/1013203451.full",
                "caption": "Greedy function approximation: A gradient boosting machine",
                "credit": "Jerome H. Friedman"
            }
        },
        {
            "start_date": {
                "year": 2001,
                "month": 7,
                "day": 6
            },
            "text": {
                "headline": "Immediate trihead",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://dl.acm.org/doi/10.3115/1073012.1073029",
                "caption": "Immediate-Head Parsing for Language Models",
                "credit": "Eugene Charniak"
            }
        },
        {
            "start_date": {
                "year": 2000,
                "month": 11,
                "day": 28
            },
            "text": {
                "headline": "PoE MNIST",
                "text": "<p>The product of experts learning procedure [1] can discover a set of stochastic binary features that constitute a non-linear generative model of handwritten images of digits. The quality of generative models learned in this way can be assessed by learning a separate model for each class of digit and then comparing the unnormalized probabilities of test images under the 10 different class-specific models. To improve discriminative performance, it is helpful to learn a hierarchy of separate models for each digit class. Each model in the hierarchy has one layer of hidden units and the nth level model is trained on data that consists of the activities of the hidden units in the already trained (n - 1)th level model. After training, each level produces a separate, unnormalized log probabilty score. With a three-level hierarchy for each of the 10 digit classes, a test image produces 30 scores which can be used as inputs to a supervised, logistic classification network that is trained on separate data. On the MNIST database, our system is comparable with current state-of-the-art discriminative methods, demonstrating that the product of experts learning procedure can produce effective generative models of high-dimensional data.</p>"
            },
            "media": {
                "url": "https://proceedings.neurips.cc/paper_files/paper/2000/file/1f1baa5b8edac74eb4eaa329f14a0361-Paper.pdf",
                "caption": "Recognizing Hand-written Digits Using Hierarchical Products of Experts",
                "credit": "Guy Mayraz, Geoffrey E. Hinton"
            }
        },
        {
            "start_date": {
                "year": 2000,
                "month": 11,
                "day": 28
            },
            "text": {
                "headline": "Neural LM",
                "text": "<p>A goal of statistical language modeling is to learn the joint probability function of sequences of words. This is intrinsically difficult because of the curse of dimensionality: we propose to fight it with its own weapons. In the proposed approach one learns simultaneously (1) a distributed representation for each word (i.e. a similarity between words) along with (2) the probability function for word sequences, expressed with these representations. Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar to words forming an already seen sentence. We report on experiments using neural networks for the probability function, showing on two text corpora that the proposed approach very significantly improves on a state-of-the-art trigram model.</p>"
            },
            "media": {
                "url": "https://papers.nips.cc/paper_files/paper/2000/file/728f206c2a01bf572b5940d7d9a8fa4c-Paper.pdf",
                "caption": "A Neural Probabilistic Language Model",
                "credit": "Yoshua Bengio, Réjean Ducharme, Pascal Vincent, Christian Janvin"
            }
        },
        {
            "start_date": {
                "year": 2000,
                "month": 9,
                "day": 1
            },
            "text": {
                "headline": "FrameNet role labeling",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://dl.acm.org/doi/10.1162/089120102760275983",
                "caption": "Automatic Labeling of Semantic Roles",
                "credit": "Daniel Gildea, Daniel Jurafsky"
            }
        },
        {
            "start_date": {
                "year": 2000,
                "month": 7,
                "day": 14
            },
            "text": {
                "headline": "SVD in recommender systems",
                "text": "<p></p>"
            },
            "media": {
                "url": "http://robotics.stanford.edu/~ronnyk/WEBKDD2000/papers/sarwar.pdf",
                "caption": "Application of Dimensionality Reduction in Recommender System -- A Case Study",
                "credit": "B Sarwar, G Karypis, J Konstan, J Riedl"
            }
        },
        {
            "start_date": {
                "year": 1999,
                "month": 12,
                "day": 1
            },
            "text": {
                "headline": "Perceptron for Large Margin Classification",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://link.springer.com/article/10.1023/A:1007662407062",
                "caption": "Large Margin Classification Using the Perceptron Algorithm",
                "credit": "Yoav Freund & Robert E. Schapire"
            }
        },
        {
            "start_date": {
                "year": 1999,
                "month": 7,
                "day": 2
            },
            "text": {
                "headline": "IBM Model 4",
                "text": "<p></p>"
            },
            "media": {
                "url": "http://www-i6.informatik.rwth-aachen.de/publications/download/266/al-onaizan--1999.pdf",
                "caption": "Statistical machine translation",
                "credit": "Yaser Al-Onaizan, Jan Curin, Michael Jahr, Kevin Knight, John Lafferty, Dan Melamed, Franz-Josef Och, David Purdy, Noah A. Smith, and David Yarowsky"
            }
        },
        {
            "start_date": {
                "year": 1999,
                "month": 1,
                "day": 2
            },
            "text": {
                "headline": "LSTM with forget gates",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://ieeexplore.ieee.org/document/818041",
                "caption": "Learning to forget: Continual prediction with LSTM",
                "credit": "F. A. Gers, J. Schmidhuber, and F. Cummins"
            }
        },
        {
            "start_date": {
                "year": 1998,
                "month": 11,
                "day": 1
            },
            "text": {
                "headline": "LeNet-5",
                "text": "<p></p>"
            },
            "media": {
                "url": "http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf",
                "caption": "Gradient-based Learning Applied to Document Recognition",
                "credit": "Yann LeCun, Léon Bottou, Yoshua Bengio, Patrick Haffner"
            }
        },
        {
            "start_date": {
                "year": 1998,
                "month": 7,
                "day": 1
            },
            "text": {
                "headline": "Social and content-based classification",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://www.aaai.org/Papers/AAAI/1998/AAAI98-101.pdf",
                "caption": "Recommendation as Classification: Using Social and Content-based Information in Recommendation",
                "credit": "C Basu, H Hirsh, W Cohen"
            }
        },
        {
            "start_date": {
                "year": 1997,
                "month": 12,
                "day": 1
            },
            "text": {
                "headline": "Sparse coding model for V1 receptive fields",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://www.sciencedirect.com/science/article/pii/S0042698997001697",
                "caption": "Sparse coding with an overcomplete basis set: A strategy employed by V1?",
                "credit": "Bruno A. Olshausen, David J. Field"
            }
        },
        {
            "start_date": {
                "year": 1997,
                "month": 11,
                "day": 15
            },
            "text": {
                "headline": "LSTM",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://direct.mit.edu/neco/article-abstract/9/8/1735/6109/Long-Short-Term-Memory?redirectedFrom=fulltext",
                "caption": "Long short-term memory",
                "credit": "Sepp Hochreiter ; Jurgen Schmidhuber"
            }
        },
        {
            "start_date": {
                "year": 1997,
                "month": 11,
                "day": 1
            },
            "text": {
                "headline": "Bidirectional RNN",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://ieeexplore.ieee.org/document/650093",
                "caption": "Bidirectional recurrent neural networks",
                "credit": "M. Schuster, KK Paliwal"
            }
        },
        {
            "start_date": {
                "year": 1997,
                "month": 6,
                "day": 17
            },
            "text": {
                "headline": "SVM for face detection",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://ieeexplore.ieee.org/document/609310",
                "caption": "Training Support Vector Machines: An Application to Face Detection",
                "credit": "E. Osuna, R. Freund, F. Girosi"
            }
        },
        {
            "start_date": {
                "year": 1997,
                "month": 5,
                "day": 1
            },
            "text": {
                "headline": "Deep Blue",
                "text": "<p>Deep Blue is the chess machine that defeated then-reigning World Chess Champion Garry Kasparov in a six-game match in 1997. There were a number of factors that contributed to this success, including:\n•a single-chip chess search engine,\n\n•a massively parallel system with multiple levels of parallelism,\n\n•a strong emphasis on search extensions,\n\n•a complex evaluation function, and\n\n•effective use of a Grandmaster game database.\n\n\nThis paper describes the Deep Blue system, and gives some of the rationale that went into the design decisions behind Deep Blue.</p>"
            },
            "media": {
                "url": "https://www.sciencedirect.com/science/article/pii/S0004370201001291",
                "caption": "Deep Blue",
                "credit": "Murray Campbell, A. Joseph Hoane Jr., Feng-hsiung Hsu"
            }
        },
        {
            "start_date": {
                "year": 1996,
                "month": 8,
                "day": 5
            },
            "text": {
                "headline": "HMM Word Alignment",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://dl.acm.org/doi/10.3115/993268.993313",
                "caption": "HMM-Based Word Alignment in Statistical Translation",
                "credit": "Stephan Vogel, Hermann Ney, Christoph Tillmann"
            }
        },
        {
            "start_date": {
                "year": 1996,
                "month": 7,
                "day": 3
            },
            "text": {
                "headline": "AdaBoost.M2 Digit Recognition",
                "text": "<p>In an earlier paper, we introduced a new “boosting” algorithm called AdaBoost which, theoretically, can be used to significantly reduce the error of any learning algorithm that con- sistently generates classifiers whose performance is a little better than random guessing. We also introduced the related notion of a “pseudo-loss” which is a method for forcing a learning algorithm of multi-label concepts to concentrate on the labels that are hardest to discriminate. In this paper, we describe experiments we carried out to assess how well AdaBoost with and without pseudo-loss, performs on real learning problems.\nWe performed two sets of experiments. The first set compared boosting to Breiman’s “bagging” method when used to aggregate various classifiers (including decision trees and single attribute- value tests). We compared the performance of the two methods on a collection of machine-learning benchmarks. In the second set of experiments, we studied in more detail the performance of boosting using a nearest-neighbor classifier on an OCR problem.</p>"
            },
            "media": {
                "url": "https://cseweb.ucsd.edu/~yfreund/papers/boostingexperiments.pdf",
                "caption": "Experiments with a New Boosting Algorithm",
                "credit": "Yoav Freund, Robert E. Schapire"
            }
        },
        {
            "start_date": {
                "year": 1996,
                "month": 6,
                "day": 18
            },
            "text": {
                "headline": "System 11",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://ieeexplore.ieee.org/document/655647",
                "caption": "Neural Network-Based Face Detection",
                "credit": "HA Rowley, S Baluja, T Kanade"
            }
        },
        {
            "start_date": {
                "year": 1996,
                "month": 6,
                "day": 3
            },
            "text": {
                "headline": "MUSIC perceptron",
                "text": "<p>The application of a multilayer perceptron for color and gray level correction in the field of photofinishing is presented. It is shown, that a neural network can improve the overall performance of a state of the art photo printer. The improved correction ability will reduce the number of unsalable pictures and thus lowers the production costs for the photo laboratory. The training experiments were carried out on a database of 30,000 photos using the MUSIC parallel supercomputer. The MUSIC system made it possible, for the first time, to process this large database in a reasonable time.</p>"
            },
            "media": {
                "url": "https://ieeexplore.ieee.org/document/549237",
                "caption": "A neural network for grey level and color correction used in photofinishing",
                "credit": "M. Kocheisen; U.A. Muller; G. Troster"
            }
        },
        {
            "start_date": {
                "year": 1995,
                "month": 11,
                "day": 27
            },
            "text": {
                "headline": "LISSOM",
                "text": "<p>An application of laterally interconnected self-organizing maps (LISSOM) to handwritten digit recognition is presented. The lateral connections learn the correlations of activity between units on the map. The resulting excitatory connections focus the activity into local patches and the inhibitory connections decorrelate redundant activity on the map. The map thus forms internal representations that are easy to recognize with e.g. a perceptron network. The recognition rate on a subset of NIST database 3 is 4.0% higher with LISSOM than with a regular Self-Organizing Map (SOM) as the front end, and 15.8% higher than recognition of raw input bitmaps directly. These results form a promising starting point for building pattern recognition systems with a LISSOM map as a front end.</p>"
            },
            "media": {
                "url": "https://www.semanticscholar.org/paper/Laterally-Interconnected-Self-Organizing-Maps-in-Choe-Sirosh/785f5facc76538ceba6f6b9e2d7b641d322e9854",
                "caption": "Laterally Interconnected Self-Organizing Maps in Hand-Written Digit Recognition",
                "credit": "Yoonsuck Choe, Joseph Sirosh, R. Miikkulainen"
            }
        },
        {
            "start_date": {
                "year": 1995,
                "month": 9,
                "day": 1
            },
            "text": {
                "headline": "Support Vector Machines",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://link.springer.com/article/10.1007/BF00994018",
                "caption": "Support-Vector Networks",
                "credit": "C Cortes, V Vapnik"
            }
        },
        {
            "start_date": {
                "year": 1995,
                "month": 8,
                "day": 14
            },
            "text": {
                "headline": "Random Decision Forests",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://ieeexplore.ieee.org/document/598994",
                "caption": "Random decision forests",
                "credit": "TK Ho"
            }
        },
        {
            "start_date": {
                "year": 1995,
                "month": 6,
                "day": 26
            },
            "text": {
                "headline": "Iterative Bootstrapping WSD",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://dl.acm.org/doi/10.3115/981658.981684",
                "caption": "Unsupervised Word Sense Disambiguation Rivaling Supervised Methods",
                "credit": "D Yarowsky"
            }
        },
        {
            "start_date": {
                "year": 1994,
                "month": 12,
                "day": 2
            },
            "text": {
                "headline": "Predictive Coding NN",
                "text": "<p>To compress text files, a neural predictor network P is used to approximate the conditional probability distribution of possible \"next characters\", given n previous characters. P's outputs are fed into standard coding algorithms that generate short codes for characters with high predicted probability and long codes for highly unpredictable characters. Tested on short German newspaper articles, our method outperforms widely used Lempel-Ziv algorithms (used in UNIX functions such as \"compress\" and \"gzip\").</p>"
            },
            "media": {
                "url": "https://proceedings.neurips.cc/paper/1994/file/5705e1164a8394aace6018e27d20d237-Paper.pdf",
                "caption": "Predictive Coding with Neural Nets: Application to Text Compression",
                "credit": "J. Schmidhuber, Stefan Heil"
            }
        },
        {
            "start_date": {
                "year": 1994,
                "month": 12,
                "day": 2
            },
            "text": {
                "headline": "NeuroChess",
                "text": "<p>This paper presents NeuroChess, a program which learns to play chess from the final outcome of games. NeuroChess learns chess board evaluation functions, represented by artificial neural networks. It integrates inductive neural network learning, temporal differencing, and a variant of explanation-based learning. Performance results illustrate some of the strengths and weaknesses of this approach.</p>"
            },
            "media": {
                "url": "https://www.semanticscholar.org/paper/Learning-to-Play-the-Game-of-Chess-Thrun/4bc7a6dcb9e0e6c7a26800532e2a00f5572eea47",
                "caption": "Learning to Play the Game of Chess",
                "credit": "S. Thrun"
            }
        },
        {
            "start_date": {
                "year": 1994,
                "month": 12,
                "day": 2
            },
            "text": {
                "headline": "Mixture of linear models",
                "text": "<p>We construct a mixture of locally linear generative models of a collection of pixel-based images of digits, and use them for recognition. Different models of a given digit are used to capture different styles of writing, and new images are classified by evaluating their log-likelihoods under each model. We use an EM-based algorithm in which the M-step is computationally straightforward principal components analysis (PCA). Incorporating tangent-plane information [12] about expected local deformations only requires adding tangent vectors into the sample covariance matrices for the PCA, and it demonstrably improves performance.</p>"
            },
            "media": {
                "url": "https://www.semanticscholar.org/paper/Recognizing-Handwritten-Digits-Using-Mixtures-of-Hinton-Revow/9dea20c1e5bbb1f543ff08113ffde5380c679f1f",
                "caption": "Recognizing Handwritten Digits Using Mixtures of Linear Models",
                "credit": "Geoffrey E. Hinton, M. Revow, P. Dayan"
            }
        },
        {
            "start_date": {
                "year": 1994,
                "month": 12,
                "day": 2
            },
            "text": {
                "headline": "JPMAX",
                "text": "<p>Unsupervised learning procedures have been successful at low-level feature extraction and preprocessing of raw sensor data. So far, however, they have had limited success in learning higher-order representations, e.g., of objects in visual images. A promising ap(cid:173) proach is to maximize some measure of agreement between the outputs of two groups of units which receive inputs physically sep(cid:173) arated in space, time or modality, as in (Becker and Hinton, 1992; Becker, 1993; de Sa, 1993). Using the same approach, a much sim(cid:173) pler learning procedure is proposed here which discovers features in a single-layer network consisting of several populations of units, and can be applied to multi-layer networks trained one layer at a time. When trained with this algorithm on image sequences of moving geometric objects a two-layer network can learn to perform accurate position-invariant object classification.</p>"
            },
            "media": {
                "url": "https://proceedings.neurips.cc/paper_files/paper/1994/hash/4b0250793549726d5c1ea3906726ebfe-Abstract.html",
                "caption": "JPMAX: Learning to Recognize Moving Objects as a Model-fitting Problem",
                "credit": "Suzanna Becker"
            }
        },
        {
            "start_date": {
                "year": 1994,
                "month": 10,
                "day": 22
            },
            "text": {
                "headline": "GroupLens",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://dl.acm.org/doi/10.1145/192844.192905",
                "caption": "GroupLens: an Open Architecture for Collaborative Filtering of Netnews",
                "credit": "Paul Resnick, Neophytos Iacovou, Mitesh Suchak, Peter Bergstrom, John Riedl"
            }
        },
        {
            "start_date": {
                "year": 1994,
                "month": 1,
                "day": 7
            },
            "text": {
                "headline": "Ceramic-MLP",
                "text": "<p>A neural network approach for pattern classification has been explored in the present paper as part of the recent resurgence of interest in this area. Our research has focused on how a multilayer feedforward structure performs in the particular problem of particle characterization. The proposed procedure, after suitable data preprocessing, consists of two distinct phases: in the former, a feedforward neural network is used to obtain an image data compression. In the latter, a neural classifier is trained on the compressed data. All the tests have been conducted on a sample constituted by two different typologies of ceramic particles, each characterized by a different microstructure. The sample image of different particles acquired and directly digitalized by scanning electron microscopy has been processed in order to achieve the best conditions to obtain the boundary profile of each particle. The boundary is thus assumed to be representative of the morphological characteristics of the ceramic products. Using the neural approach, a classification accuracy as high as 100% on a training set of 80 sub-images was achieved. These networks correctly classified up to 96.9% of 64 testing patterns not contained in the training set.</p>"
            },
            "media": {
                "url": "https://www.sciencedirect.com/science/article/abs/pii/S0921883108605506",
                "caption": "Ceramic powder characterization by multilayer perceptron (MLP) data compression and classification",
                "credit": "G. Bonifazi, P. Burrascano"
            }
        },
        {
            "start_date": {
                "year": 1993,
                "month": 11,
                "day": 29
            },
            "text": {
                "headline": "ANN Eye Tracker",
                "text": "<p>We have developed an artificial neural network based gaze tracking system which can be customized to individual users. A three layer feed forward network, trained with standard error back propagation, is used to determine the position of a user''s gaze from the appearance of the user''s eye. Unlike other gaze trackers, which normally require the user to wear cumbersome headgear, or to use a chin rest to ensure head immobility, our system is entirely non-intrusive. Currently, the best intrusive gaze tracking systems are accurate to approximately 0.75 degrees. In our experiments, we have been able to achieve an accuracy of 1.5 degrees, while allowing head mobility. In its current implementation, our system works at 15 hz. In this paper we present an empirical analysis of the performance of a large number of artificial neural network architectures for this task. Suggestions for further explorations for neurally based gaze trackers are presented, and are related to other similar artificial neural network applications such as autonomous road following.</p>"
            },
            "media": {
                "url": "https://www.semanticscholar.org/paper/Non-Intrusive-Gaze-Tracking-Using-Artificial-Neural-Baluja-Pomerleau/574c0cf98825bf09b0ab7bbfe9ba89cd6090745e",
                "caption": "Non-Intrusive Gaze Tracking Using Artificial Neural Networks",
                "credit": "S. Baluja, D. Pomerleau"
            }
        },
        {
            "start_date": {
                "year": 1993,
                "month": 8,
                "day": 1
            },
            "text": {
                "headline": "Siamese-TDNN",
                "text": "<p>This paper describes an algorithm for verification of signatures written on a pen-input tablet. The algorithm is based on a novel, artificial neural network, called a \"Siamese\" neural network. This network consists of two identical sub-networks joined at their outputs. During training the two sub-networks extract features from two signatures, while the joining neuron measures the distance between the two feature vectors. Verification consists of comparing an extracted feature vector with a stored feature vector for the signer. Signatures closer to this stored representation than a chosen threshold are accepted, all other signatures are rejected as forgeries.</p>"
            },
            "media": {
                "url": "https://www.semanticscholar.org/paper/Signature-Verification-Using-A-%22Siamese%22-Time-Delay-Bromley-Bentz/997dc5d9a058753f034422afe7bd0cc0b8ad808b",
                "caption": "Signature Verification using a \"Siamese\" Time Delay Neural Network",
                "credit": "J. Bromley, James W. Bentz, L. Bottou, Isabelle M Guyon, Yann LeCun, C. Moore, Eduard Säckinger, Roopak Shah"
            }
        },
        {
            "start_date": {
                "year": 1993,
                "month": 6,
                "day": 15
            },
            "text": {
                "headline": "IBM-5",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://dl.acm.org/doi/10.5555/972470.972474",
                "caption": "The Mathematics of Statistical Machine Translation: Parameter Estimation",
                "credit": "Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, Robert L. Mercer"
            }
        },
        {
            "start_date": {
                "year": 1992,
                "month": 11,
                "day": 30
            },
            "text": {
                "headline": "Boosting",
                "text": "<p>A boosting algorithm converts a learning machine with error rate less than 50% to one with an arbitrarily low error rate. However, the algorithm discussed here depends on having a large supply of independent training samples. We show how to circumvent this problem and generate an ensemble of learning machines whose performance in optical character recognition problems is dramatically improved over that of a single network. We report the effect of boosting on four databases (all handwritten) consisting of 12,000 digits from segmented ZIP codes from the United State Postal Service (USPS) and the following from the National Institute of Standards and Testing (NIST): 220,000 digits, 45,000 upper case alphas, and 45,000 lower case alphas. We use two performance measures: the raw error rate (no rejects) and the reject rate required to achieve a 1% error rate on the patterns not rejected. Boosting improved performance in some cases by a factor of three.</p>"
            },
            "media": {
                "url": "https://www.semanticscholar.org/paper/Improving-Performance-in-Neural-Networks-Using-a-Drucker-Schapire/77b5185dafb9e5b884a677a32713e54c253a4e0b",
                "caption": "Improving Performance in Neural Networks Using a Boosting Algorithm",
                "credit": "H. Drucker, R. Schapire, Patrice Y. Simard"
            }
        },
        {
            "start_date": {
                "year": 1992,
                "month": 10,
                "day": 16
            },
            "text": {
                "headline": "Cancer drug mechanism prediction",
                "text": "<p>Described here are neural networks capable of predicting a drug's mechanism of action from its pattern of activity against a panel of 60 malignant cell lines in the National Cancer Institute's drug screening program. Given six possible classes of mechanism, the network misses the correct category for only 12 out of 141 agents (8.5 percent), whereas linear discriminant analysis, a standard statistical technique, misses 20 out of 141 (14.2 percent). The success of the neural net indicates several things. (i) The cell line response patterns are rich in information about mechanism. (ii) Appropriately designed neural networks can make effective use of that information. (iii) Trained networks can be used to classify prospectively the more than 10,000 agents per year tested by the screening program. Related networks, in combination with classical statistical tools, will help in a variety of ways to move new anticancer agents through the pipeline from in vitro studies to clinical application.</p>"
            },
            "media": {
                "url": "https://pubmed.ncbi.nlm.nih.gov/1411538/",
                "caption": "Neural computing in cancer drug development: predicting mechanism of action",
                "credit": "John N. Weinstein, Kurt W. Kohn, Michael R. Grever, Vellarkad N.\nViswanadhan, Lawrence V. Rubinstein, Anne P. Monks, Dominic A. Scudiero, Lester Welch, Antonis D. Koutsoukos, August J. Chiausa, Kenneth D. Paull"
            }
        },
        {
            "start_date": {
                "year": 1992,
                "month": 10,
                "day": 1
            },
            "text": {
                "headline": "Golem",
                "text": "<p>Many attempts have been made to solve the problem of predicting protein secondary structure from the primary sequence but the best performance results are still disappointing. In this paper, the use of a machine learning algorithm which allows relational descriptions is shown to lead to improved performance. The Inductive Logic Programming computer program, Golem, was applied to learning secondary structure prediction rules for alpha/alpha domain type proteins. The input to the program consisted of 12 non-homologous proteins (1612 residues) of known structure, together with a background knowledge describing the chemical and physical properties of the residues. Golem learned a small set of rules that predict which residues are part of the alpha-helices--based on their positional relationships and chemical and physical properties. The rules were tested on four independent non-homologous proteins (416 residues) giving an accuracy of 81% (+/- 2%). This is an improvement, on identical data, over the previously reported result of 73% by King and Sternberg (1990, J. Mol. Biol., 216, 441-457) using the machine learning program PROMIS, and of 72% using the standard Garnier-Osguthorpe-Robson method. The best previously reported result in the literature for the alpha/alpha domain type is 76%, achieved using a neural net approach. Machine learning also has the advantage over neural network and statistical methods in producing more understandable results.</p>"
            },
            "media": {
                "url": "https://www.semanticscholar.org/paper/Protein-secondary-structure-prediction-using-Muggleton-King/9f744e48091a24b569435d070920e60db45f4fdc",
                "caption": "Protein secondary structure prediction using logic-based machine learning.",
                "credit": "S. Muggleton, Ross D. King, M. J. Sternberg"
            }
        },
        {
            "start_date": {
                "year": 1992,
                "month": 9,
                "day": 1
            },
            "text": {
                "headline": "Fuzzy NN",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://ieeexplore.ieee.org/document/159058",
                "caption": "Multilayer perceptron, fuzzy sets, and classification",
                "credit": "SK Pal, S Mitra"
            }
        },
        {
            "start_date": {
                "year": 1992,
                "month": 5,
                "day": 1
            },
            "text": {
                "headline": "TD-Gammon",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://papers.nips.cc/paper/1991/file/68ce199ec2c5517597ce0a4d89620f55-Paper.pdf",
                "caption": "Practical Issues in Temporal Difference Learning",
                "credit": "G Tesauro"
            }
        },
        {
            "start_date": {
                "year": 1991,
                "month": 12,
                "day": 2
            },
            "text": {
                "headline": "Weight Decay",
                "text": "<p>It has been observed in numerical simulations that a weight decay can improve generalization in a feed-forward neural network. This paper explains why. It is proven that a weight decay has two effects in a linear network. First, it suppresses any irrelevant components of the weight vector by choosing the smallest vector that solves the learning problem. Second, if the size is chosen right, a weight decay can suppress some of the effects of static noise on the targets, which improves generalization quite a lot. It is then shown how to extend these results to networks with hidden layers and non-linear units. Finally the theory is confirmed by some numerical simulations using the data from NetTalk.</p>"
            },
            "media": {
                "url": "https://www.semanticscholar.org/paper/A-Simple-Weight-Decay-Can-Improve-Generalization-Krogh-Hertz/48e1de7d085808004d5f0493d486669a3d2930b5",
                "caption": "A Simple Weight Decay Can Improve Generalization",
                "credit": "A. Krogh, J. Hertz"
            }
        },
        {
            "start_date": {
                "year": 1991,
                "month": 9,
                "day": 1
            },
            "text": {
                "headline": "SRN-Encoded Grammatical Structures",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://dl.acm.org/doi/10.1007/BF00114844",
                "caption": "Distributed representations, simple recurrent networks, and grammatical structure",
                "credit": "J. L. Elman"
            }
        },
        {
            "start_date": {
                "year": 1990,
                "month": 11,
                "day": 1
            },
            "text": {
                "headline": "RAAM",
                "text": "<p>A longstanding difficulty for connectionist modeling has been how to represent variable-sized recursive data structures, such as trees and lists, in fixed-width patterns. This paper presents a connectionist architecture which automatically develops compact distributed representations for such compositional structures, as well as efficient accessing mechanisms for them. Patterns which stand for the internal nodes of fixed-valence trees are devised through the recursive use of backpropagation on three-layer auto-associative encoder networks. The resulting representations are novel, in that they combine apparently immiscible aspects of features, pointers, and symbol structures. They form a bridge between the data structures necessary for high-level cognitive tasks and the associative, pattern recognition machinery provided by neural networks.</p>"
            },
            "media": {
                "url": "https://www.sciencedirect.com/science/article/abs/pii/000437029090005K?via%3Dihub",
                "caption": "Recursive Distributed Representations",
                "credit": "Jordan B. Pollack"
            }
        },
        {
            "start_date": {
                "year": 1990,
                "month": 10,
                "day": 1
            },
            "text": {
                "headline": "SexNet compression",
                "text": "<p>Sex identification in animals has biological importance. Humans are good at making this determination visually, but machines have not matched this ability. A neural network was trained to discriminate sex in human faces, and performed as well as humans on a set of 90 exemplars. Images sampled at 30×30 were compressed using a 900×40×900 fully-connected back-propagation network; activities of hidden units served as input to a back-propagation \"SexNet\" trained to produce values of 1 for male and 0 for female faces. The network's average error rate of 8.1% compared favorably to humans, who averaged 11.6%. Some SexNet errors mimicked those of humans.</p>"
            },
            "media": {
                "url": "https://www.semanticscholar.org/paper/SEXNET%3A-A-Neural-Network-Identifies-Sex-From-Human-Golomb-Lawrence/cbf90aa78fea0c8a1028705d92bc4bc7808ddeeb",
                "caption": "SEXNET: A Neural Network Identifies Sex From Human Faces",
                "credit": "B. Golomb, D. T. Lawrence, T. Sejnowski"
            }
        },
        {
            "start_date": {
                "year": 1990,
                "month": 10,
                "day": 1
            },
            "text": {
                "headline": "SexNet classification",
                "text": "<p>Sex identification in animals has biological importance. Humans are good at making this determination visually, but machines have not matched this ability. A neural network was trained to discriminate sex in human faces, and performed as well as humans on a set of 90 exemplars. Images sampled at 30×30 were compressed using a 900×40×900 fully-connected back-propagation network; activities of hidden units served as input to a back-propagation \"SexNet\" trained to produce values of 1 for male and 0 for female faces. The network's average error rate of 8.1% compared favorably to humans, who averaged 11.6%. Some SexNet errors mimicked those of humans.</p>"
            },
            "media": {
                "url": "https://www.semanticscholar.org/paper/SEXNET%3A-A-Neural-Network-Identifies-Sex-From-Human-Golomb-Lawrence/cbf90aa78fea0c8a1028705d92bc4bc7808ddeeb",
                "caption": "SEXNET: A Neural Network Identifies Sex From Human Faces",
                "credit": "B. Golomb, D. T. Lawrence, T. Sejnowski"
            }
        },
        {
            "start_date": {
                "year": 1990,
                "month": 10,
                "day": 1
            },
            "text": {
                "headline": "ISR network",
                "text": "<p>Neural network algorithms have proven useful for recognition of individ(cid:173) ual, segmented characters. However, their recognition accuracy has been limited by the accuracy of the underlying segmentation algorithm. Con(cid:173) ventional, rule-based segmentation algorithms encounter difficulty if the characters are touching, broken, or noisy. The problem in these situations is that often one cannot properly segment a character until it is recog(cid:173) nized yet one cannot properly recognize a character until it is segmented. We present here a neural network algorithm that simultaneously segments and recognizes in an integrated system. This algorithm has several novel features: it uses a supervised learning algorithm (backpropagation), but is able to take position-independent information as targets and self-organize the activities of the units in a competitive fashion to infer the positional information. We demonstrate this ability with overlapping hand-printed numerals.</p>"
            },
            "media": {
                "url": "https://papers.nips.cc/paper_files/paper/1990/hash/e46de7e1bcaaced9a54f1e9d0d2f800d-Abstract.html",
                "caption": "Integrated Segmentation and Recognition of Hand-Printed Numerals",
                "credit": "James Keeler, David Rumelhart, Wee Leow"
            }
        },
        {
            "start_date": {
                "year": 1990,
                "month": 6,
                "day": 17
            },
            "text": {
                "headline": "Bankruptcy-NN",
                "text": "<p>A neural network model is developed for prediction of bankruptcy, and it is tested using financial data from various companies. The same set of data is analyzed using a more traditional method of bankruptcy prediction, multivariate discriminant analysis. A comparison of the predictive abilities of both the neural network and the discriminant analysis method is presented. The results show that neural networks might be applicable to this problem</p>"
            },
            "media": {
                "url": "https://www.semanticscholar.org/paper/A-neural-network-model-for-bankruptcy-prediction-Odom-Sharda/ead9fa02902850a7418fb5ba720f3d9d8ab2f88b",
                "caption": "A neural network model for bankruptcy prediction",
                "credit": "M. Odom, R. Sharda"
            }
        },
        {
            "start_date": {
                "year": 1989,
                "month": 12,
                "day": 1
            },
            "text": {
                "headline": "Innervator",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://www.researchgate.net/publication/220885651_Designing_Neural_Networks_using_Genetic_Algorithms",
                "caption": "Designing neural networks using genetic algorithms",
                "credit": "Geoffrey Miller, Peter Todd, and Shailesh Hegde"
            }
        },
        {
            "start_date": {
                "year": 1989,
                "month": 12,
                "day": 1
            },
            "text": {
                "headline": "ALVINN",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://proceedings.neurips.cc/paper/1988/hash/812b4ba287f5ee0bc9d43bbf5bbe87fb-Abstract.html",
                "caption": "ALVINN: an autonomous land vehicle in a neural network",
                "credit": "DA Pomerleau"
            }
        },
        {
            "start_date": {
                "year": 1989,
                "month": 11,
                "day": 27
            },
            "text": {
                "headline": "Speaker-independent vowel classification",
                "text": "<p>Multi-layer perceptrons and trained classification trees are two very different techniques which have recently become popular. Given enough data and time, both methods are capable of performing arbitrary non-linear classification. We first consider the important differences between multi-layer perceptrons and classification trees and conclude that there is not enough theoretical basis for the clear-cut superiority of one technique over the other. For this reason, we performed a number of empirical tests on three real-world problems in power system load forecasting, power system security prediction, and speaker-independent vowel identification. In all cases, even for piecewise-linear trees, the multi-layer perceptron performed as well as or better than the trained classification trees.</p>"
            },
            "media": {
                "url": "https://www.semanticscholar.org/paper/Performance-Comparisons-Between-Backpropagation-and-Atlas-Cole/e42d2b89fcb4a1a3dfa63408f424f76975ed1e1b",
                "caption": "Performance Comparisons Between Backpropagation Networks and Classification Trees on Three Real-World Applications",
                "credit": "L. Atlas, R. Cole, J. Connor, M. El-Sharkawi, R. Marks, Y. Muthusamy, E. Barnard"
            }
        },
        {
            "start_date": {
                "year": 1989,
                "month": 11,
                "day": 27
            },
            "text": {
                "headline": "Handwritten Digit Recognition System",
                "text": "<p>We present an application of back-propagation networks to handwritten digit recognition. Minimal preprocessing of the data was required, but architecture of the network was highly constrained and specifically designed for the task. The input of the network consists of normalized images of isolated digits. The method has 1% error rate and about a 9% reject rate on zipcode digits provided by the U.S. Postal Service.</p>"
            },
            "media": {
                "url": "https://www.semanticscholar.org/paper/Handwritten-Digit-Recognition-with-a-Network-LeCun-Boser/86ab4cae682fbd49c5a5bedb630e5a40fa7529f6",
                "caption": "Handwritten Digit Recognition with a Back-Propagation Network",
                "credit": "Yann LeCun, B. Boser, J. Denker, D. Henderson, R. Howard, W. Hubbard, L. Jackel"
            }
        },
        {
            "start_date": {
                "year": 1989,
                "month": 6,
                "day": 18
            },
            "text": {
                "headline": "Truck backer-upper",
                "text": "<p>Neural networks can be used to solve highly nonlinear control problems. A two-layer neural network containing 26 adaptive neural elements has learned to back up a computer-simulated trailer truck to a loading dock, even when initially jackknifed. It is not yet known how to design a controller to perform this steering task. Nevertheless, the neural net was able to learn of its own accord to do this, regardless of initial conditions. Experience gained with the truck backer-upper should be applicable to a wide variety of nonlinear control problems.</p>"
            },
            "media": {
                "url": "https://ieeexplore.ieee.org/document/118723",
                "caption": "The truck backer-upper: an example of self-learning in neural networks\n",
                "credit": "Derrick Nguyen, Bernard Widrow"
            }
        },
        {
            "start_date": {
                "year": 1989,
                "month": 6,
                "day": 18
            },
            "text": {
                "headline": "Invariant image recognition",
                "text": "<p>A new model which permits visual patterns to be invariant to affine transforms (translations, rotations, and dimensions) is presented. A training multilayer fully connected network of ADALINE neurons is proposed as a preprocessing step for invariant image extraction. A second neural network has been trained by the popular backpropagation algorithm for recovering the real image without distortions. First, the sample invariants are obtained by the preprocessing network. In the second step, the general invariant that includes all the sample invariants is computed. Afterward, the reordered sample invariants are input to a multilayer neural network trained by the backpropagation algorithm. The original image, without distortions, is obtained in the output of this system. Several test images have been computed, and evaluation of the results shows that in the case of images with intrinsic perceptual similarity, the learning procedure leads to a global invariant extraction that requires less computational effort in comparison with an arbitrary training selection. After the training process, this system is able to extract the generalized invariant image from an arbitrary picture recovering the input image without distortions.<<ETX>></p>"
            },
            "media": {
                "url": "https://ieeexplore.ieee.org/document/118669",
                "caption": "Invariant image recognition using a multi-network neural model",
                "credit": "V. Cruz, G. Cristóbal, T. Michaux, S. Barquin"
            }
        },
        {
            "start_date": {
                "year": 1989,
                "month": 3,
                "day": 3
            },
            "text": {
                "headline": "Time-delay neural networks",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://ieeexplore.ieee.org/abstract/document/21701",
                "caption": "Phoneme recognition using time-delay neural networks",
                "credit": "A. Waibel, T. Hanazawa, G. Hinton, K. Shikano, and K. J. Lang"
            }
        },
        {
            "start_date": {
                "year": 1989,
                "month": 1,
                "day": 1
            },
            "text": {
                "headline": "Q-learning",
                "text": "<p></p>"
            },
            "media": {
                "url": "http://www.cs.rhul.ac.uk/~chrisw/thesis.html",
                "caption": "Learning from delayed rewards",
                "credit": "Christopher Watkins"
            }
        },
        {
            "start_date": {
                "year": 1989,
                "month": 1,
                "day": 1
            },
            "text": {
                "headline": "MLP baggage detector",
                "text": "<p>An artificial neural system (ANS) has been applied to the problem of discriminating between suitcases with and without explosives. The input to the ANS was data gathered during the field tests of a prototype explosive detection system. The performance of the ANS is contrasted with the standard statistical technique (discriminant analysis) used, and is shown to exceed the performance of the standard technique over a substantial range. The system that generated the data, the nature of the data, the basics of discriminant analysis, and the technique used in developing the network are described.</p>"
            },
            "media": {
                "url": "https://www.semanticscholar.org/paper/Detection-of-explosives-in-checked-airline-baggage-Shea-Lin/71da4057401f459bc079696a029aee45a0a89728",
                "caption": "Detection of explosives in checked airline baggage using an artificial neural system",
                "credit": "Patrick M Shea, Vincent Lin"
            }
        },
        {
            "start_date": {
                "year": 1988,
                "month": 8,
                "day": 1
            },
            "text": {
                "headline": "MLN-ASR",
                "text": "<p>A set of Multi-Layered Networks (MLN) for Automatic Speech Recognition (ASR) is proposed. Such a set allows the integration of information extracted with variable resolution in the time and frequency domains and to keep the number of links between nodes of the networks small in order to allow significant generalization during learning with a reasonable training set size. Subsets of networks can be executed depending on preconditions based on descriptions of the time evolution of signal energies allowing spectral properties that are significant in different acoustic situations to be learned.\nPreliminary experiments on speaker-independent recognition of the letters of the E-set are reported. Voices from 70 speakers were used for learning. Voices of 10 new speakers were used for test. An overall error rate of 9.5% was obtained in the test showing that results better than those previously reported can be achieved.</p>"
            },
            "media": {
                "url": "https://aaai.org/papers/00734-aaai88-130-data-driven-execution-of-multi-layered-networks-for-automatic-speech-recognition/",
                "caption": "Data-Driven Execution of Multi-Layered Networks for Automatic Speech Recognition",
                "credit": "Renato De Mori, Yoshua Bengio, Régis Cardin"
            }
        },
        {
            "start_date": {
                "year": 1988,
                "month": 7,
                "day": 1
            },
            "text": {
                "headline": "Motion-Driven 3D Feature Tracking",
                "text": "<p></p>"
            },
            "media": {
                "url": "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.434.4816&rep=rep1&type=pdf",
                "caption": "A Combined Corner and Edge Detector",
                "credit": "Harris & Stephens"
            }
        },
        {
            "start_date": {
                "year": 1987,
                "month": 6,
                "day": 6
            },
            "text": {
                "headline": "NetTalk (transcription)",
                "text": "<p></p>"
            },
            "media": {
                "url": "http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=03A3D3EDF0BAF35405ABCF083411B55E?doi=10.1.1.154.7012&rep=rep1&type=pdf",
                "caption": "Parallel Networks that Learn to Pronounce English Text",
                "credit": "TJ Sejnowski, CR Rosenberg"
            }
        },
        {
            "start_date": {
                "year": 1987,
                "month": 6,
                "day": 6
            },
            "text": {
                "headline": "NetTalk (dictionary)",
                "text": "<p></p>"
            },
            "media": {
                "url": "http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=03A3D3EDF0BAF35405ABCF083411B55E?doi=10.1.1.154.7012&rep=rep1&type=pdf",
                "caption": "Parallel Networks that Learn to Pronounce English Text",
                "credit": "TJ Sejnowski, CR Rosenberg"
            }
        },
        {
            "start_date": {
                "year": 1986,
                "month": 11,
                "day": 1
            },
            "text": {
                "headline": "Optimized Multi-Scale Edge Detection",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4767851",
                "caption": "A Computational Approach To Edge Detection",
                "credit": "John Canny"
            }
        },
        {
            "start_date": {
                "year": 1986,
                "month": 10,
                "day": 1
            },
            "text": {
                "headline": "Back-propagation",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://www.semanticscholar.org/paper/Learning-representations-by-back-propagating-errors-Rumelhart-Hinton/052b1d8ce63b07fec3de9dbb583772d860b7c769",
                "caption": "Learning representations by back-propagating errors",
                "credit": "Rumelhart, David E.; Hinton, Geoffrey E.; Williams, Ronald J."
            }
        },
        {
            "start_date": {
                "year": 1986,
                "month": 8,
                "day": 15
            },
            "text": {
                "headline": "Distributed representation NN",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://www.cs.toronto.edu/~hinton/absps/families.pdf",
                "caption": "Learning distributed representations of concepts.",
                "credit": "Geoffrey E. Hinton"
            }
        },
        {
            "start_date": {
                "year": 1986,
                "month": 1,
                "day": 5
            },
            "text": {
                "headline": "PDP model for serial order",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://www.osti.gov/biblio/6910294",
                "caption": "Serial order: A parallel distributed processing approach",
                "credit": "Jordan, M.I."
            }
        },
        {
            "start_date": {
                "year": 1986,
                "month": 1,
                "day": 3
            },
            "text": {
                "headline": "Error Propagation",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://stanford.edu/~jlmcc/papers/PDP/Volume%201/Chap8_PDP86.pdf",
                "caption": "Learning internal representations by error propagation",
                "credit": "D. E. Rumelhart, G. E. Hinton, and R. J. Williams"
            }
        },
        {
            "start_date": {
                "year": 1984,
                "month": 7,
                "day": 1
            },
            "text": {
                "headline": "Learnability theory of language development",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://psycnet.apa.org/record/1985-97439-000",
                "caption": "Language learnability and language development",
                "credit": "Steven Pinker"
            }
        },
        {
            "start_date": {
                "year": 1984,
                "month": 4,
                "day": 1
            },
            "text": {
                "headline": "Hierarchical Cognitron",
                "text": "<p>A hierarchical neural network model with feedback interconnections, which has the function of associative memory and the ability to recognize patterns, is proposed. The model consists of a hierarchical multi-layered network to which efferent connections are added, so as to make positive feedback loops in pairs with afferent connections. The cell-layer at the initial stage of the network is the input layer which receives the stimulus input and at the same time works as an output layer for associative recall. The deepest layer is the output layer for pattern-recognition. Pattern-recognition is performed hierarchically by integrating information by converging afferent paths in the network. For the purpose of associative recall, the integrated information is again distributed to lower-order cells by diverging efferent paths. These two operations progress simultaneously in the network. If a fragment of a training pattern is presented to the network which has completed its self-organization, the entire pattern will gradually be recalled in the initial layer. If a stimulus consisting of a number of training patterns superposed is presented, one pattern gradually becomes predominant in the recalled output after competition between the patterns, and the others disappear. At about the same time when the recalled pattern reaches a steady state in he initial layer, in the deepest layer of the network, a response is elicited from the cell corresponding to the category of the finally-recalled pattern. Once a steady state has been reached, the response of the network is automatically extinguished by inhibitory signals from a steadiness-detecting cell. If the same stimulus is still presented after inhibition, a response for another pattern, formerly suppressed, will now appear, because the cells of the network have adaptation characteristics which makes the same response unlikely to recur. Since inhibition occurs repeatedly, the superposed input patterns are recalled one by one in turn.</p>"
            },
            "media": {
                "url": "https://link.springer.com/article/10.1007/BF00337157",
                "caption": "A hierarchical neural network model for associative memory",
                "credit": "K. Fukushima"
            }
        },
        {
            "start_date": {
                "year": 1983,
                "month": 9,
                "day": 1
            },
            "text": {
                "headline": "ASE+ACE",
                "text": "<p>It is shown how a system consisting of two neuronlike adaptive elements can solve a difficult learning control problem. The task is to balance a pole that is hinged to a movable cart by applying forces to the cart's base. It is argued that the learning problems faced by adaptive elements that are components of adaptive networks are at least as difficult as this version of the pole-balancing problem. The learning system consists of a single associative search element (ASE) and a single adaptive critic element (ACE). In the course of learning to balance the pole, the ASE constructs associations between input and output by searching under the influence of reinforcement feedback, and the ACE constructs a more informative evaluation function than reinforcement feedback alone can provide. The differences between this approach and other attempts to solve problems using neurolike elements are discussed, as is the relation of this work to classical and instrumental conditioning in animal learning studies and its possible implications for research in the neurosciences.</p>"
            },
            "media": {
                "url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6313077",
                "caption": "Neuronlike adaptive elements that can solve difficult learning control problems",
                "credit": "Andrew G. Barto, Richard S. Sutton, and Charles W. Anderson"
            }
        },
        {
            "start_date": {
                "year": 1982,
                "month": 4,
                "day": 1
            },
            "text": {
                "headline": "Hopfield network",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://www.pnas.org/doi/10.1073/pnas.79.8.2554",
                "caption": "Neural networks and physical systems with emergent collective computational abilities",
                "credit": "JJ Hopfield"
            }
        },
        {
            "start_date": {
                "year": 1981,
                "month": 7,
                "day": 25
            },
            "text": {
                "headline": "Kohonen network",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://link.springer.com/article/10.1007/BF00337288",
                "caption": "Self-organized formation of topologically correct feature maps",
                "credit": "T Kohonen"
            }
        },
        {
            "start_date": {
                "year": 1980,
                "month": 4,
                "day": 1
            },
            "text": {
                "headline": "Neocognitron",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://link.springer.com/article/10.1007/BF00344251",
                "caption": "Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position",
                "credit": "K Fukushima, S Miyake"
            }
        },
        {
            "start_date": {
                "year": 1979,
                "month": 5,
                "day": 2
            },
            "text": {
                "headline": "Internal functionality of visual invariants",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://link.springer.com/article/10.1007/BF00337644",
                "caption": "The internal representation of solid shape with respect to vision",
                "credit": "Koenderink & van Doom"
            }
        },
        {
            "start_date": {
                "year": 1977,
                "month": 8,
                "day": 1
            },
            "text": {
                "headline": "TD(0)",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://www.sciencedirect.com/science/article/pii/S0019995877903540",
                "caption": "An adaptive optimal controller for discrete-time Markov environments",
                "credit": "Ian Witten"
            }
        },
        {
            "start_date": {
                "year": 1976,
                "month": 4,
                "day": 30
            },
            "text": {
                "headline": "Statistical continuous speech recognizer",
                "text": "<p>Statistical methods useful in automatic recognition of continuous speech are described. They concern modeling of a speaker and of an acoustic processor, extraction of the models' statistical parameters, and hypothesis search procedures and likelihood computations of linguistic decoding. Experimental results are presented that indicate the power of the methods.</p>"
            },
            "media": {
                "url": "https://ieeexplore.ieee.org/document/1454428",
                "caption": "Continuous speech recognition by statistical methods",
                "credit": "Frederick Jelenick"
            }
        },
        {
            "start_date": {
                "year": 1975,
                "month": 9,
                "day": 1
            },
            "text": {
                "headline": "Cognitron",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://link.springer.com/article/10.1007%2FBF00342633",
                "caption": "Cognitron: a self-organizing multilayered neural network",
                "credit": "Kunihiko Fukushima"
            }
        },
        {
            "start_date": {
                "year": 1973,
                "month": 11,
                "day": 1
            },
            "text": {
                "headline": "Piecewise linear model",
                "text": "<p>Texture is one of the important characteristics used in identifying objects or regions of interest in an image, whether the image be a photomicrograph, an aerial photograph, or a satellite image. This paper describes some easily computable textural features based on gray-tone spatial dependancies, and illustrates their application in category-identification tasks of three different kinds of image data: photomicrographs of five kinds of sandstones, 1:20 000 panchromatic aerial photographs of eight land-use categories, and Earth Resources Technology Satellite (ERTS) multispecial imagery containing seven land-use categories. We use two kinds of decision rules: one for which the decision regions are convex polyhedra (a piecewise linear decision rule), and one for which the decision regions are rectangular parallelpipeds (a min-max decision rule). In each experiment the data set was divided into two parts, a training set and a test set. Test set identification accuracy is 89 percent for the photomicrographs, 82 percent for the aerial photographic imagery, and 83 percent for the satellite imagery. These results indicate that the easily computable textural features probably have a general applicability for a wide variety of image-classification applications.</p>"
            },
            "media": {
                "url": "https://ieeexplore.ieee.org/document/4309314",
                "caption": "Textural Features for Image Classification",
                "credit": "R. Haralick, K. Shanmugam, I. Dinstein"
            }
        },
        {
            "start_date": {
                "year": 1970,
                "month": 9,
                "day": 1
            },
            "text": {
                "headline": "Graph-based structural reasoning",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://dspace.mit.edu/handle/1721.1/6884",
                "caption": "Learning Structural Definitions from Examples",
                "credit": "Patrick Winston"
            }
        },
        {
            "start_date": {
                "year": 1969,
                "month": 5,
                "day": 1
            },
            "text": {
                "headline": "Decision tree adaline",
                "text": "<p>A learning machine\" adaline neuron\" was employed for automatic diagnosis of the vectorcardiogram and the electrocardiogram. The frontal circle and the horizontal circle were divided into 480 meshes. The features were expressed by a binary digit, whether the vector loops passed through each mesh or not. In a part of the trials, 5 sets of binary digits were applied in addition to QRS duration and direction of inscription of QRS loops and T loops. In this trial a total of 490 meshes were used. Vectorcardiograms were taken by FRANK'S method in 235 cases. Several methods of adaline usage were tried. The best result was obtained so far by successive dichotomies based on the principle of the logical decision tree. First the normal patterns and the abnormal patterns were divided. The correct ratio was 96.5% when the 490 meshes were employed, cases of an output value within±10 units being regarded as undecided. Next the abnormal cases were divided into two groups depending on whether the QRS duration was more than 0.12 seconds or less. The group of cases with a QRS duration of less than 0.12 seconds was divided into right ventricular hypertrophy and others. The correct ratio was 98.6%. The remaining cases were divided into left ventricular hypertrophy and myocardial infarction, the correct ratio being 88.8%. The group of cases with a QRS duration of more than 0.12 seconds was easily divided into complete left and right bundle branch block in all cases. Here the number of meshes could be decreased to 59 meshes without changing the accuracy appreciably. This attempt showed that the application of the adaline for automatic diagnosis of the vectorcardiogram and the electrocardiogram is promising.</p>"
            },
            "media": {
                "url": "https://pubmed.ncbi.nlm.nih.gov/5820353/",
                "caption": "A use of Adaline as an automatic method for interpretation of the electrocardiogram and the vectorcardiogram",
                "credit": "T Sano, S Tsuchiya, F Suzuki"
            }
        },
        {
            "start_date": {
                "year": 1968,
                "month": 7,
                "day": 1
            },
            "text": {
                "headline": "GLEE",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.474.2430",
                "caption": "Boxes: An Experiment in Adaptive Control",
                "credit": "Michie and Chambers"
            }
        },
        {
            "start_date": {
                "year": 1968,
                "month": 7,
                "day": 1
            },
            "text": {
                "headline": "BOXES",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.474.2430",
                "caption": "Boxes: An Experiment in Adaptive Control",
                "credit": "Michie and Chambers"
            }
        },
        {
            "start_date": {
                "year": 1966,
                "month": 11,
                "day": 1
            },
            "text": {
                "headline": "LTE speaker verification system",
                "text": "<p>This paper describes an investigation of the capability of a two‐level adaptive linear threshold element (LTE) system to perform speaker discriminations. The study also includes an investigation of discriminating a speaker from an unknown population. The problem has been confined to the verification of an utterance as that of an expected informant. The environment of the experiments is discussed, and the experimental system is described. At the first level LTE, four different kinds of training have been developed for effective transformation and data reduction. At the second‐level LTE, different training conditions and different decision processes are investigated and evaluated. Over 90% accuracy is obtained in separating a known speaker from impostors.</p>"
            },
            "media": {
                "url": "https://pubs.aip.org/asa/jasa/article-abstract/40/5/966/754180/Experimental-Studies-in-Speaker-Verification-Using?redirectedFrom=fulltext",
                "caption": "Experimental Studies in Speaker Verification, Using an Adaptive System",
                "credit": "K. P. Li; J. E. Dammann; W. D. Chapman"
            }
        },
        {
            "start_date": {
                "year": 1965,
                "month": 10,
                "day": 1
            },
            "text": {
                "headline": "Heuristic Reinforcement Learning",
                "text": "<p>This paper describes a learning control system using a reinforcement technique. The controller is capable of controlling a plant that may be nonlinear and nonstationary. The only a priori information required by the controller is the order of the plant. The approach is to design a controller which partitions the control measurement space into sets called control situations and then learns the best control choice for each control situation. The control measurements are those indicating the state of the plant and environment. The learning is accomplished by reinforcement of the probability of choosing a particular control choice for a given control situation. The system was stimulated on an IBM 1710-GEDA hybrid computer facility. Experimental results obtained from the simulation are presented.</p>"
            },
            "media": {
                "url": "https://ieeexplore.ieee.org/document/1098193",
                "caption": "A heuristic approach to reinforcement learning control systems",
                "credit": "M. Waltz, K. Fu"
            }
        },
        {
            "start_date": {
                "year": 1963,
                "month": 1,
                "day": 1
            },
            "text": {
                "headline": "Print Recognition Logic",
                "text": "<p>A computer program has been written to design character recognition logic based on the processing of data samples. This program consists of two subroutines: (1) to search for logic circuits having certain constraints on hardware design, and (2) to evaluate these logics in terms of their discriminating ability over samples of the character set they are expected to recognize. An executive routine is used to apply these subroutines to select a complete logic with a given performance and complexity. This logic consists of 39 to 96 and gates connected to a shift register and a table look-up or resistance network comparison system.</p>"
            },
            "media": {
                "url": "https://ieeexplore.ieee.org/document/5392331",
                "caption": "Computer-Automated Design of Multifont Print Recognition Logic",
                "credit": "L. Kamentsky, Chao-Ning Liu"
            }
        },
        {
            "start_date": {
                "year": 1962,
                "month": 7,
                "day": 1
            },
            "text": {
                "headline": "MADALINE I",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://www.proquest.com/openview/7898314db50a218b58052ac91e3bde1e/1?",
                "caption": "An adaptive logic system with generalizing properties",
                "credit": "William Combs Ridgway"
            }
        },
        {
            "start_date": {
                "year": 1962,
                "month": 6,
                "day": 1
            },
            "text": {
                "headline": "Linear Decision Functions",
                "text": "<p>Many pattern recognition machines may be considered to consist of two principal parts, a receptor and a categorizer. The receptor makes certain measurements on the unknown pattern to be recognized; the categorizer determines from these measurements the particular allowable pattern class to which the unknown pattern belongs. This paper is concerned with the study of a particular class of categorizers, the linear decision function. The optimum linear decision function is the best linear approximation to the optimum decision function in the following sense: 1) \"Optimum\" is taken to mean minimum loss (which includes minimum error systems). 2) \"Linear\" is taken to mean that each pair of pattern classes is separated by one and only one hyperplane in the measurement space. This class of categorizers is of practical interest for two reasons: 1) It can be empirically designed without making any assumptions whatsoever about either the distribution of the receptor measurements or the a priori probabilities of occurrence of the pattern classes, providing an appropriate pattern source is available. 2) Its implementation is quite simple and inexpensive. Various properties of linear decision functions are discussed. One such property is that a linear decision function is guaranteed to perform at least as well as a minimum distance categorizer. Procedures are then developed for the estimation (or design) of the optimum linear decision function based upon an appropriate sampling from the pattern classes to be categorized.</p>"
            },
            "media": {
                "url": "https://ieeexplore.ieee.org/document/4066882?denied=",
                "caption": "Linear Decision Functions, with Application to Pattern Recognition",
                "credit": "W. Highleyman"
            }
        },
        {
            "start_date": {
                "year": 1960,
                "month": 6,
                "day": 30
            },
            "text": {
                "headline": "LMS",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://www.scirp.org/(S(351jmbntvnsjt1aadkposzje))/reference/ReferencesPapers.aspx?ReferenceID=547230",
                "caption": "Adaptive switching circuits (technical report)",
                "credit": "Widrow and Hoff"
            }
        },
        {
            "start_date": {
                "year": 1960,
                "month": 6,
                "day": 30
            },
            "text": {
                "headline": "ADALINE",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://isl.stanford.edu/~widrow/papers/c1960adaptiveswitching.pdf",
                "caption": "Adaptive switching circuits",
                "credit": "Widrow and Hoff"
            }
        },
        {
            "start_date": {
                "year": 1960,
                "month": 3,
                "day": 30
            },
            "text": {
                "headline": "Perceptron (1960)",
                "text": "<p>An experimental simulation program, which has been in progress at the Cornell Aeronautical Laboratory since 1957, is described. This program uses the IBM 704 computer to simulate perceptual learning, recognition, and spontaneous classification of visual stimuli in the perceptron, a theoretical brain model which has been described elsewhere. The paper includes a brief review of the organization of simple perceptrons, and theoretically predicted performance curves are compared with those obtained from the simulation programs, in several types of experiments, designed to study \"forced\" and \"spontaneous\" learning of pattern discriminations.</p>"
            },
            "media": {
                "url": "https://www.semanticscholar.org/paper/Perceptron-Simulation-Experiments-Rosenblatt/ae76ce1ba27ac29addce4aab93b927e9bc7f7c67",
                "caption": "Perceptron Simulation Experiments",
                "credit": "Frank Rosenblatt"
            }
        },
        {
            "start_date": {
                "year": 1959,
                "month": 12,
                "day": 1
            },
            "text": {
                "headline": "Pattern recognition and reading by machine",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://dl.acm.org/doi/10.1145/1460299.1460326",
                "caption": "Pattern recognition and reading by machine",
                "credit": "W. W. Bledsoe, I. Browning"
            }
        },
        {
            "start_date": {
                "year": 1959,
                "month": 7,
                "day": 1
            },
            "text": {
                "headline": "Samuel Neural Checkers",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://ieeexplore.ieee.org/abstract/document/5392560",
                "caption": "Some studies in machine learning using the game of checkers",
                "credit": "Arthur L. Samuel"
            }
        },
        {
            "start_date": {
                "year": 1959,
                "month": 2,
                "day": 1
            },
            "text": {
                "headline": "Pandemonium (morse)",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://aitopics.org/doc/classics:504E1BAC/",
                "caption": "Pandemonium: A Paradigm for Learning",
                "credit": "OG Selfridge"
            }
        },
        {
            "start_date": {
                "year": 1957,
                "month": 1,
                "day": 1
            },
            "text": {
                "headline": "Perceptron Mark I",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://blogs.umass.edu/brain-wars/files/2016/03/rosenblatt-1957.pdf",
                "caption": "The Perceptron—a perceiving and recognizing automaton",
                "credit": "F Rosenblatt"
            }
        },
        {
            "start_date": {
                "year": 1955,
                "month": 3,
                "day": 1
            },
            "text": {
                "headline": "Sequence-based pattern recognition",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://dl.acm.org/doi/10.1145/1455292.1455310",
                "caption": "Pattern recognition and modern computers",
                "credit": "O. G. Selfridge"
            }
        },
        {
            "start_date": {
                "year": 1955,
                "month": 3,
                "day": 1
            },
            "text": {
                "headline": "Self Organizing System",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://dl.acm.org/doi/10.1145/1455292.1455309",
                "caption": "Generalization of pattern recognition in a self-organizing system",
                "credit": "W. A. Clark and B. G. Farley"
            }
        },
        {
            "start_date": {
                "year": 1954,
                "month": 7,
                "day": 2
            },
            "text": {
                "headline": "Genetic algorithm",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://link.springer.com/article/10.1007/BF01556771",
                "caption": "Numerical testing of evolution theories",
                "credit": "NA Barricelli"
            }
        },
        {
            "start_date": {
                "year": 1952,
                "month": 1,
                "day": 8
            },
            "text": {
                "headline": "SNARC",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://en.wikipedia.org/wiki/Stochastic_neural_analog_reinforcement_calculator",
                "caption": "A Neural-Analogue Calculator Based upon a Probability Model of Reinforcement",
                "credit": "Marvin Minsky"
            }
        },
        {
            "start_date": {
                "year": 1950,
                "month": 7,
                "day": 2
            },
            "text": {
                "headline": "Theseus",
                "text": "<p></p>"
            },
            "media": {
                "url": "https://www.technologyreview.com/2018/12/19/138508/mighty-mouse/",
                "caption": "Mighty Mouse",
                "credit": "Claude Shannon"
            }
        }
    ]
}